@article{Aizawa_Fujimoto_Otsubo_Ogawa_Matsui_Tsubota_Ikuta_2020, 
	title={Building a manga dataset “Manga109” with annotations for multimedia applications}, 
	volume={27}, 
	url={https://arxiv.org/abs/2005.04425}, 
	DOI={10.1109/mmul.2020.2987895}, 
	number={2}, 
	journal={IEEE Multimedia}, 
	author={Aizawa, Kiyoharu and Fujimoto, Azuma and Otsubo, Atsushi and Ogawa, Toru and Matsui, Yusuke and Tsubota, Koki and Ikuta, Hikaru}, 
	year={2020}, 
	month=apr, 
	pages={8–18} 
}
@misc{Ikuta_Wöhler_Aizawa_2024, 
	title={MANGAUB: A Manga Understanding Benchmark for large Multimodal Models}, 
	url={https://arxiv.org/abs/2407.19034}, 
	journal={arXiv.org}, 
	author={Ikuta, Hikaru and Wöhler, Leslie and Aizawa, Kiyoharu}, 
	year={2024}, 
	month=jul, 
	language={en} 
}
@misc{Jiang_He_Zeng_Wei_Ku_Liu_Chen_2024, 
	title={MANTIS: Interleaved Multi-Image Instruction Tuning}, 
	url={https://arxiv.org/abs/2405.01483}, 
	journal={arXiv.org}, 
	author={Jiang, Dongfu and He, Xuan and Zeng, Huaye and Wei, Cong and Ku, Max and Liu, Qian and Chen, Wenhu}, 
	year={2024}, 
	month=may, 
	language={en} 
}
@misc{Li_Zhang_Guo_Zhang_Li_Zhang_Zhang_Zhang_Li_Liu_et_al_2024, 
	title={LLAVA-OneVision: Easy Visual Task Transfer}, 
	url={https://arxiv.org/abs/2408.03326}, 
	journal={arXiv.org}, 
	author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Zhang, Peiyuan and Li, Yanwei and Liu, Ziwei and Li, Chunyuan}, 
	year={2024}, 
	month=aug, 
	language={en} 
}
@misc{Radford_Kim_Hallacy_Ramesh_Goh_Agarwal_Sastry_Askell_Mishkin_Clark_et_al_2021, 
	title={Learning transferable visual models from natural language supervision}, 
	url={https://arxiv.org/abs/2103.00020}, 
	journal={arXiv.org}, 
	author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya}, 
	year={2021}, 
	month=feb, 
	language={en} 
}
@misc{Sachdeva_Zisserman_2024, 
	title={The Manga Whisperer: Automatically generating transcriptions for comics}, 
	url={https://arxiv.org/abs/2401.10224}, 
	journal={arXiv.org}, 
	author={Sachdeva, Ragav and Zisserman, Andrew}, 
	year={2024}, 
	month=jan, 
	language={en} 
}
@misc{Shen_Yao_Liu_2023, 
	title={MARU: a manga retrieval and understanding system connecting vision and language}, 
	url={https://arxiv.org/abs/2311.02083}, 
	journal={arXiv.org}, 
	author={Shen, Conghao Tom and Yao, Violet and Liu, Yixin}, 
	year={2023}, 
	month=oct, 
	language={en} 
}
@misc{Vivoli_Barsky_Souibgui_LLabres_Bertini_Karatzas_2024, title={One missing piece in Vision and Language: A Survey on Comics Understanding}, url={https://arxiv.org/abs/2409.09502}, journal={arXiv.org}, author={Vivoli, Emanuele and Barsky, Andrey and Souibgui, Mohamed Ali and LLabres, Artemis and Bertini, Marco and Karatzas, Dimosthenis}, year={2024}, month=sep, language={en} 
}
@article{Wang_Wang_Liang_Yu_2019, title={Comic-guided speech synthesis}, volume={38}, url={https://doi.org/10.1145/3355089.3356487}, DOI={10.1145/3355089.3356487}, number={6}, journal={ACM Transactions on Graphics}, author={Wang, Yujia and Wang, Wenguan and Liang, Wei and Yu, Lap-Fai}, year={2019}, month=nov, pages={1–14} 
}
@misc{Xue_Shu_Awadalla_Wang_Yan_Purushwalkam_Zhou_Prabhu_Dai_Ryoo_et_al_2024, 
    title={XGen-MM (BLIP-3): a family of open large multimodal models}, url={https://arxiv.org/abs/2408.08872}, journal={arXiv.org},
    author={Xue, Le and Shu, Manli and Awadalla, Anas and Wang, Jun and Yan, An and Purushwalkam, Senthil and Zhou, Honglu and Prabhu, Viraj and Dai, Yutong and Ryoo, Michael S and Kendre, Shrikant and Zhang, Jieyu and Qin, Can and Zhang, Shu and Chen, Chia-Chih and Yu, Ning and Tan, Juntao and Awalgaonkar, Tulika Manoj and Heinecke, Shelby and Wang, Huan and Choi, Yejin and Schmidt, Ludwig and Chen, Zeyuan and Savarese, Silvio and Niebles, Juan Carlos and Xiong, Caiming and Xu, Ran}, 
    year={2024}, 
    month=aug, 
    language={en} 
}
@misc{Yao_Yu_Zhang_Wang_Cui_Zhu_Cai_Li_Zhao_He_et_al_2024, 
    title={MiniCPM-V: a GPT-4V level MLLM on your phone}, 
    url={https://arxiv.org/abs/2408.01800}, 
    journal={arXiv.org}, 
    author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and Chen, Qianyu and Zhou, Huarong and Zou, Zhensheng and Zhang, Haoye and Hu, Shengding and Zheng, Zhi and Zhou, Jie and Cai, Jie and Han, Xu and Zeng, Guoyang and Li, Dahai and Liu, Zhiyuan and Sun, Maosong}, 
    year={2024}, 
    month=aug, 
    language={en} 
}
@misc{li2024llavanext-strong,
    title={LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild},
    url={https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/},
    author={Li, Bo and Zhang, Kaichen and Zhang, Hao and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Yuanhan and Liu, Ziwei and Li, Chunyuan},
    month={May},
    year={2024}
}
@misc{liu2024llavanext,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month={January},
    year={2024}
}
@inproceedings{li2024manga109dialog,
  title={Manga109Dialog: A Large-scale Dialogue Dataset for Comics Speaker Detection},
  author={Li, Yingxuan and Aizawa, Kiyoharu and Matsui, Yusuke},
  booktitle={Proceedings of the IEEE International Conference on Multimedia and Expo},
  year={2024}
}
@misc{sachdeva2024tailstelltaleschapterwide,
      title={Tails Tell Tales: Chapter-Wide Manga Transcriptions with Character Names}, 
      author={Ragav Sachdeva and Gyungin Shin and Andrew Zisserman},
      year={2024},
      eprint={2408.00298},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.00298}, 
}
@misc{carion2020endtoendobjectdetectiontransformers,
      title={End-to-End Object Detection with Transformers}, 
      author={Nicolas Carion and Francisco Massa and Gabriel Synnaeve and Nicolas Usunier and Alexander Kirillov and Sergey Zagoruyko},
      year={2020},
      eprint={2005.12872},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2005.12872}, 
}
@misc{liu2024groundingdinomarryingdino,
      title={Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection}, 
      author={Shilong Liu and Zhaoyang Zeng and Tianhe Ren and Feng Li and Hao Zhang and Jie Yang and Qing Jiang and Chunyuan Li and Jianwei Yang and Hang Su and Jun Zhu and Lei Zhang},
      year={2024},
      eprint={2303.05499},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2303.05499}, 
}
@inproceedings{10.1145/2647868.2654990,
author = {Pang, Xufang and Cao, Ying and Lau, Rynson W.H. and Chan, Antoni B.},
title = {A Robust Panel Extraction Method for Manga},
year = {2014},
isbn = {9781450330633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647868.2654990},
doi = {10.1145/2647868.2654990},
abstract = {Automatically extracting frames/panels from digital comic pages is crucial for techniques that facilitate comic reading on mobile devices with limited display areas. However, automatic panel extraction for manga, i.e., Japanese comics, can be especially challenging, largely because of its complex panel layout design mixed with various visual symbols throughout the page. In this paper, we propose a robust method for automatically extracting panels from digital manga pages. Our method first extracts the panel block by closing open panels and identifying a page background mask. It then performs a recursive binary splitting to partition the panel block into a set of sub-blocks, where an optimal splitting line at each recursive level is determined adaptively.},
booktitle = {Proceedings of the 22nd ACM International Conference on Multimedia},
pages = {1125–1128},
numpages = {4},
keywords = {panel extraction, computational manga, comics processing, adaptive page partitioning},
location = {Orlando, Florida, USA},
series = {MM '14}
}
@misc{soykan2022comprehensivegoldstandardbenchmark,
      title={A Comprehensive Gold Standard and Benchmark for Comics Text Detection and Recognition}, 
      author={Gürkan Soykan and Deniz Yuret and Tevfik Metin Sezgin},
      year={2022},
      eprint={2212.14674},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.14674}, 
}
@misc{iyyer2017amazingmysteriesgutterdrawing,
      title={The Amazing Mysteries of the Gutter: Drawing Inferences Between Panels in Comic Book Narratives}, 
      author={Mohit Iyyer and Varun Manjunatha and Anupam Guha and Yogarshi Vyas and Jordan Boyd-Graber and Hal Daumé III and Larry Davis},
      year={2017},
      eprint={1611.05118},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1611.05118}, 
}
@misc{liu2024mmbenchmultimodalmodelallaround,
      title={MMBench: Is Your Multi-modal Model an All-around Player?}, 
      author={Yuan Liu and Haodong Duan and Yuanhan Zhang and Bo Li and Songyang Zhang and Wangbo Zhao and Yike Yuan and Jiaqi Wang and Conghui He and Ziwei Liu and Kai Chen and Dahua Lin},
      year={2024},
      eprint={2307.06281},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2307.06281}, 
}
@misc{liu2023visualinstructiontuning,
      title={Visual Instruction Tuning}, 
      author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
      year={2023},
      eprint={2304.08485},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.08485}, 
}
@misc{touvron2023llamaopenefficientfoundation,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}touvron2023llamaopenefficientfoundation
