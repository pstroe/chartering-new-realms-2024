---
thebe: true
kernelspec:
  display_name: Python 3
  language: pyhton
  name: python3
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: `0.13`
    jupytext_version: 1.11.5
---

# Chapter 7

## Abstract
We made a comprehensive evaluation of the LLaVA-onevision model's capabilities in manga recognition and understanding by designing multiple tasks similar to those in the mangaUB framework, by Ikuta et al. (2024). LlaVa-onevision is a multimodal LLM, having strong performance in multi-image understanding and cross-scenario capabilities. These tasks included panel ordering, story reconstruction, image recognition, and character counting. Our results show that LLaVA-onevision performs well in recognizing image elements but struggles with character counting. The model also demonstrates promising abilities in story reconstruction, though there remains significant room for improvement. However, its performance in panel ordering tasks was still poor.
## 1. Introduction
In the western world, the storytelling medium of manga has seen a massive rise in popularity in recent years, bringing millions of new eyes on the Japanese art form. Once more, thousands of already established and beloved stories are now able to be enjoyed for the first time, by western audiences. Manga, with its many unique approaches to graphic narrative storytelling, from the different art styles, paneling, story structures and character dynamics, has managed to establish itself in western markets by offering something unlike anything else seen in western comics. This uniqueness, this, “je ne sais quoi” is what made us, the authors of this paper, fall in love with the art. We, like many other non-Japanese readers, have laughed, cried, been amazed and been touched by the stories told by the Japanese manga artists, creating memories we will hold dear forever.
However, many exist that are not able to enjoy the stories told through manga. The medium is, after all, extremely visually dependent. On top of this, the lack of any sort of tool or software that can automatically translate manga into a medium that can also be enjoyed by the visually impaired only worsens the accessibility issue. We see this as a great tragedy, want to help remedy this problem, even just by a little. Even if we alone can’t solve the problem, we will still rest easy, knowing that our proposal is able to be used as a brick in the road that will one day lead to a system that can make manga accessible to everyone.
In this chapter we propose a multimodal language model to transcribe manga into text. Our model will tell the story based on the text in character dialogue and narration, as well as textual descriptions of manga panels provided by a vision-model. With these elements, our model is able to tell a coherent story, translating a graphic-narrative into a textual-narrative.
There are several challenges in such a proposal. The biggest one being the fact that manga and comics as a whole is an under-researched field in the digital humanities. There are only a few resources available for us to use in our model, resources that are themselves recent and new, often developed by very small teams, or even individuals, with little to no backing from large companies. This limits both the quality and the amount of tasks that a model can cover. However, with our model, we hope to be able to add to the limited resources and improve research in the field of not just manga, but graphic narratives as a whole.
Furthermore, as manga are protected by copyright laws, there exists an issue in creating manga and comics corpora and datasets. The datasets that are available to the public either have to use images from comics and manga in the public domain, like the COMICS-dataset created by {cite}`iyyer2017amazingmysteriesgutterdrawing`, which uses American comic books from the 1950s and 60s, or the datasets have to get permission from the respective authors, like Manga109, created by {cite}`Aizawa_Fujimoto_Otsubo_Ogawa_Matsui_Tsubota_Ikuta_2020`. This low amount of publicly available training data has in turn caused the training of models to become more difficult.
Due to the nature of comics and manga being comprised both of images and text, both oftentimes being very abstract, any model that is designed to process graphic narratives will inherently be complex and multi-modal. To fully process even a single comic or manga page, a model would need to apply (at the very least) panel segmentation, panel ordering, OCR, character detection, text-to-speaker association and object recognition, before then describing the entire storyline based on this data.
Furthermore, the structural uniqueness of comics, characterized by variations in artistic style, sequential arrangement, and creative applications of nonlinear storytelling, sets them apart from other forms of visual language and poses unique challenges for artificial intelligence systems {cite}`Vivoli_Barsky_Souibgui_LLabres_Bertini_Karatzas_2024`. Notably, some comics deviate from conventional panel-to-panel sequential narration, leveraging innovative reading sequences to achieve distinctive effects. For instance, Pascal Jousselin’s Imbattable (2017, 2018, 2021) features a protagonist who can know events occurring in other panels and “travel” between them, even revisiting earlier panels. This results in two parallel reading orders: the reader’s sequential navigation and the protagonist’s linear timeline. Such unconventional storytelling techniques test the limits of AI’s interpretative capabilities, making it particularly challenging for AI tools to effectively process and understand these works.
The Large Multimodal Model (LMM) we will use is Llava-onevision, which is capable of (among other things) interpreting actions and content across multiple images and generating a coherent retelling using this data. We chose it due to its impressive performance, especially in a field so similar to manga understanding. 

## 2. Relevant literature
Vivoli et al. {cite}`Vivoli_Barsky_Souibgui_LLabres_Bertini_Karatzas_2024` provide a comprehensive overview of recent advancements in comic understanding, proposing the task-oriented AI framework LoCU. This framework aims to address the inherent complexity and diversity of visual language tasks associated with comics. Among these, multimodal visual language understanding has emerged as a critical research area. According to Vivoli et al. {cite}`Vivoli_Barsky_Souibgui_LLabres_Bertini_Karatzas_2024`, comic understanding surpasses the basic tasks of image recognition, annotation, or retrieval, requiring advanced reasoning and comprehension abilities from AI models.
There are many papers that approach the problem of transcribing manga and comics, but due to the difficulty of the task, most research only tackles “sub-problems”, such as panel-detection and extraction {cite}`10.1145/2647868.2654990` or optical character recognition (OCR) {cite}`soykan2022comprehensivegoldstandardbenchmark`. However, by combining these smaller models with large multimodal models (LMMs), with image-captioning abilities, a larger model that can process entire manga pages can theoretically be built.
While some existing LMMs have demonstrated strong performance on single-image visual language tasks, their ability to handle the more intricate multi-image tasks remains a significant challenge {cite}`Jiang_He_Zeng_Wei_Ku_Liu_Chen_2024`. Researchers have begun to address this gap by extending LMM capabilities in areas such as reasoning, OCR, in-context learning capabilities, and the integration of world knowledge {cite}`Li_Zhang_Guo_Zhang_Li_Zhang_Zhang_Zhang_Li_Liu_et_al._2024` {cite}`Yao_Yu_Zhang_Wang_Cui_Zhu_Cai_Li_Zhao_He_et_al._2024` {cite}`Xue_Shu_Awadalla_Wang_Yan_Purushwalkam_Zhou_Prabhu_Dai_Ryoo_et_al._2024` {cite}`Radford_Kim_Hallacy_Ramesh_Goh_Agarwal_Sastry_Askell_Mishkin_Clark_et_al._2021`. 
Recent advancements in LMM development, such as LLaVA-onevision {cite}`Li_Zhang_Guo_Zhang_Li_Zhang_Zhang_Zhang_Li_Liu_et_al._2024` {cite}`li2024llavanext-strong` {cite}`liu2024llavanext` and MiniCPM-V {cite}`Yao_Yu_Zhang_Wang_Cui_Zhu_Cai_Li_Zhao_He_et_al._2024`, highlight these models’ expanding potential. For example, LLaVA-NeXT, an extension of LLaVA-1.5, has outperformed Gemini Pro on certain benchmarks, demonstrating notable improvements {cite}`Li_Zhang_Guo_Zhang_Li_Zhang_Zhang_Zhang_Li_Liu_et_al._2024`. However, most multimodal language models are designed to process diverse image types rather than comics specifically. For instance, the LLaVA model's training relies on high-quality captioning datasets, such as Re-Captioned Detailed Description Data and Chinese-Language Data {cite}`Li_Zhang_Guo_Zhang_Li_Zhang_Zhang_Zhang_Li_Liu_et_al._2024` {cite}`li2024llavanext-strong` {cite}`liu2024llavanext`. The performance of these models on comic-specific data and comics understanding area remains underexplored.
Existing approaches to comics and manga understanding and transcription tend not to use LMMs, instead building specialised models, which commonly decompose comic images into their visual and textual components to take as input. We propose first using visual components as prompts to run the MLLM, followed by adding textual components as prompts, to evaluate the MLLM's varying performance in manga understanding task. Previous studies provide promising directions for this approach. For example, Wang et al. {cite}`Wang_Wang_Liang_Yu_2019` developed a method to extract dialogues, character emotions, and visual traits from comics. These elements were then synthesized into lifelike voiceovers, resulting in the creation of audio comics. Similarly, Sachdeva and Zisserman introduced Magi {cite}`Sachdeva_Zisserman_2024` (Magi), a fully automated system for transcribing character dialogues. Magi identifies text boxes and character panels, organizes the text boxes according to the intended reading order, and generates dialogue transcriptions. This approach has proven effective for both American comics and Japanese manga {cite}`Sachdeva_Zisserman_2024`. This model was then followed up with Magiv2, which included a character-bank, allowing the model to assign correct character names to the speakers {cite}`Sachdeva_et_al`. However, Magi and Magiv2 focus exclusively on dialogue transcription and character clustering, overlooking other crucial visual details within panels, such as characters' actions at specific moments. As a result, it is incapable of reconstructing a comic’s narrative in its entirety. To address this limitation, Shen et al. {cite}`Shen_Yao_Liu_2023` proposed the MaRU system (Manga Retrieval and Understanding), which identifies panel boundaries and analyzes textual content within comics. 
There are however some forays into large multimodal models in manga understanding research, as, for example, Ikuta et al. {cite}`Ikuta_Wöhler_Aizawa_2024` introduced the MangaUB, a manga understanding benchmark tailored for LMMs.  MangaUB breaks down manga content into a series of tasks that test the recognition and comprehension of individual panels or multiple panels. This approach provides a comprehensive measure of how well LMMs perform in understanding manga. Their results showed that while LMMs handle single-panel tasks more effectively than multi-panel tasks, the LLaVA-Next model demonstrated superior performance in the story comprehension capability than other multimodal models. Collectively, all these studies highlight the potential and challenges of using LMMs for comics and manga understanding. 

## 3. Methods
Our workflow is centered around the large multimodal model Llava-onevision, proposed by Li et al. {cite}`Li_Zhang_Guo_Zhang_Li_Zhang_Zhang_Zhang_Li_Liu_et_al._2024`. We chose this approach due to both the availability and popularity of Llava-onevision, leading to it having many resources on it online, but also due to its impressive performance, being able to consistently outperform other LMMs and match and sometimes even outperform GPT-4V and 4o. The model itself is based on the Llava-Model by Liu et al. {cite}`liu2023visualinstructiontuning`, which itself is based on Meta's Llama {cite}`touvron2023llamaopenefficientfoundation` and spawned a family of LMMs, of which Llava-onevision is one of the most recent members.
To test the performance of LMMs in comic understanding tasks, we will use instruction fine-tuning to adjust and test the model. Our instructions will explain rules on different tasks and explain how the model should follow and interpret the content. For example, the model will be instructed to read panels sequentially based on the order of Japanese manga (from right to left, from top to bottom); the model should learn rules about how to determine dialogue and characters. The MLLM will analyze the panels in the manga images, detect the reading order based on the sequence of the dialogue. Following this, the model will detect and describe the scene for each dialogue segment, including details about characters, backgrounds, actions, and the predominant emotions within each panel. LLaVA-onevision will also try to connect the story across panels, incorporating contextual understanding. Finally, the model will provide an overall summary of the story’s plot. Ethical considerations will also be embedded in the prompts to guide the model's responses.
As noted by Ikuta et al. {cite}`Ikuta_Wöhler_Aizawa_2024` in their MangaUB system, predicting the next panel is a key challenge in comic understanding. We consider the MangaUB benchmarks and designed two manga understanding tasks and four simple image recognition tasks for challenging the MLLM:
Understanding tasks: (1) Panels order detection task; (2) Story reconstruction task
Image recognition tasks: (3) Location; (4) Time in day; (5) Weather; (6) Number of characters. {cite}`Ikuta_Wöhler_Aizawa_2024`
Our workflow consists of loading the Llava-onevision model using python's "transformers" module, and then using instruction fine-tuning and prompt engineering to adjust and test the MLLM. As input, the model takes a single image and a prompt, It then returns a textual output. We use several different prompts in different tasks, to get outputs with more details, and make evaluations. 

## 4. Evaluation
### 4.1 Data
In the following section, the data we used to evaluate Llava-onevision will be described. This consists of 206 pages from randomly selected manga in the Manga109 and PopManga datasets.
#### Manga109
Manga109 {cite}`Aizawa_Fujimoto_Otsubo_Ogawa_Matsui_Tsubota_Ikuta_2020`, published by the University of Tokyo, consists of 21,142 images from 109 manga volumes spanning works from the 1970s to the 2010s. These images are in turn scans of the manga, with each image depicting two pages. This dataset covers a wide range of genres and styles, making it a valuable source for manga understanding research. Ikuta et al. {cite}`Ikuta_Wöhler_Aizawa_2024` categorized the Manga109 dataset into four main genres—Shonen, Shojo, Seinen, and Josei—considering these differences in panels of several artistic styles across these genres influence model learning and comprehension. In addition to high-resolution images organized sequentially by volume, Manga109 provides extensive metadata and annotation data. The metadata includes titles, authors, and publication details for each volume of manga.
The annotations in Manga109 are particularly detailed, encompassing panel boundaries, speech balloons, text boxes, character faces, and full-body character annotations. These annotations enable researchers to perform segmentation, recognition, and understanding tasks. Some comics understanding task research are based on these annotations {cite}`Sachdeva_Zisserman_2024` {cite}`li2024manga109dialog`. The combination of a large collection of manga images and comprehensive annotations makes Manga109 a foundational resource for analyzing, training, and evaluating models in manga comprehension and related tasks. For this reason, we have used data from two randomly selected books "BEMADER_P" by Yuuichi Hasegawa and "Nichijou Soup" by Uni Shindou from the Manga109 dataset to evaluate the performance of our model.

#### PopManga
PopManga is a relatively new dataset introduced by {cite}`Sachdeva_Zisserman_2024`. It is made up of publicly available manga chapters from Shueisha’s Manga Plus, totaling 57’000+ pages. Magiv2 itself is trained on a subset of PopManga, PopManga-Dev, and evaluated using two different subsets, PopManga-Test-S (Seen) and PopManga-Test-U (Unseen). There are human annotations for text-boxes, character boxes, and character-character-association, while a finetuned version of the DETR-model {cite}`Carion_et_al_2020` made the panel annotations. {cite}`Sachdeva_Zissermann_2024`

### 4.2 Experiments
Llava-onevision in our task is evaluated using a method similar to the one outlined in the MangaUB benchmark, originally proposed in {cite}`Ikuta_Wöhler_Aizawa_2024`, which itself is based on the CircularEval scheme in {cite}`liu2024mmbenchmultimodalmodelallaround`. The model is tested using four recognition tasks, and two (EDIT THIS IN CASE OF FUTURE CHANGES) understanding tasks. The recognition tasks consist of multiple choice tests, used to determine the (1) location, (2) time of day, (3) weather and (4) number of characters in a scene. Here, the model is instructed to answer with one of two choices for the first three categories ("Indoors", "Outdoors"; "Daytime", "Nighttime"; "Sunny", "Rainy") and for the character count test, the model answers in a single Arabic numeral. It is also instructed to answer with a short explanation of its choice, mainly for the user to understand the "thought process" involved in the decision-making. The idea behind using multiple-choice tests, as also mentioned by Ikuta et al., lies in limiting the output space of a Large Multimodal Model, as they can be very broad. For MangaUB there was a higher importance in evaluating a wide range of LMMs, which emphasised the use of multiple-choice tasks. For our study, we did not have this limitation, which is why our understanding task does not consist of a multiple-choice test.

#### The Understanding Tasks
The Panels order detection task is based on the Next-Panel-Inference task from MangaUB {cite}`Ikuta_Wöhler_Aizawa_2024`. However, as the cost of the manual editing of the input data was too high for the scope of this project, this task was modified, instead determining Next-Panel-Inference through prompting. A prompt is passed to Llava-onevision, asking it for the correct reading order of the panels on the page to be processed, at the same time informing it that the reading order of manga is typically right-to-left, top-to-bottom, with panels in a horizontal sequence having priority over panels vertically above each other. As output, the model returns a text, describing each panel and ordering them by what it determined to be the correct reading order using a number. This task aims to evaluate the MLLM's ability to understand narrative structures and manga comprehension. We use human manual evaluation to count the correlation and accuracy between the panels order suggested by the output of Llava-onevision and the correct reading order from human. Occasionally, the model's output may not be entirely relevant to the task; for example, it might provide detailed descriptions of the images within different panels, rather than correctly responding to the instruction to sequence the panels. Such instances are accounted for, and deductions are made in relevance scoring accordingly.
The story reconstruction task, as defined in the LoCU (Layer of Comics Understanding) framework proposed by Vivoli et al. {cite}`Vivoli_Barsky_Souibgui_LLabres_Bertini_Karatzas_2024`, involves deeply integrating visual elements with language. It goes beyond basic recognition or retrieval and represents a complex task that requires both visual understanding and logical reasoning, making it highly challenging for models. We predict that models relying on instruction tuning would struggle with this task, and it will be also hard to evaluate the outputs through other MLLMs. Therefore, human manual evaluation was also used to assess the relevance of the model's output to the original images.

#### The Image Recognition Tasks
These tasks are divided into four subtasks, primarily based on the Scene Understanding Tasks outlined in the mangaUB framework proposed by Ikuta et al {cite}`Ikuta_Wöhler_Aizawa_2024`. 
For each subtask (expect the final subtask), the model is instructed to choose between two options for three categories: "Indoors" or "Outdoors," "Daytime" or "Nighttime," and "Sunny" or "Rainy," corresponding to location, time in a day, and weather, respectively. Following this, the model is tasked with counting the number of characters present in each image. To compare the model's outputs under different instructions, multiple variations of the instructions are provided. Given the typically broad range of responses from LLMs, the instructions are explicitly constrained to require the model to respond only with the given options, optionally accompanied by a brief explanation. Human manual evaluation is used to calculate the accuracy of the model's responses. 
The final subtask involves asking the model to count the number of characters (individuals) shown in the image. However, since a single character often appears multiple times within the same image (due to the presence of multiple panels), we predict that the results for this task will deviate significantly from the actual count. During manual evaluation, we consider the calculation of the number of main characters to be the primary focus for assessing the model's performance. Consequently, the count of minor characters is treated as a deviation. Here, "minor characters" refer to anonymous characters that lack distinct features, have only a vague shape, or are obscured in the background, and whose presence has little to no impact on the storyline. If the model's count is deemed acceptable within the deviation range, the result is labelled as correct; otherwise, it is marked as incorrect.
The answers to these image recognition tasks are relatively objective and easier to evaluate. Additionally, these tasks are less complex than content-level visual reasoning, making them more accessible for MLLMs to handle effectively.

### 5. Results and Discussion
The table below describes the scores of Llava-onevision’s answers in all tasks.
For the two manga understanding tasks (1) and (2), scores were assigned manually using a 5-point scale, where 1 represents "completely unrelated" and 5 represents "highly relevant." Higher scores indicate greater relevance. The average scores were then mapped to a 1–100 scale and expressed as percentages to align with the scoring format of tasks evaluated using accuracy. For image recognition tasks, the scores were calculated as the accuracy of Llava-onevision.
|Task | (1) Panels order | (2) story reconstruction | (3) location | (4) time | (5) weather | (6) numbers of characters |
| :----: | :---: | :---: | :---: | :---: | :---: | :---: |
| prompts without annotations | 0.16|0.42|0.69|0.70|0.58|0.30|
| prompts with annotations| 0.19|0.47|0.65|0.69|0.61|0.34|

The Llava-onevision model exhibited cases in tasks (3), (4), and (5) where it did not strictly follow the choices in instructions but still provided accurate descriptions of the scene's content and atmosphere. Such instances were labelled as correct. If the model failed to follow the instruction and the description was unrelated to the image, the result was marked as incorrect. In task (3), the model was instructed to consider the features of the majority of the panels. If a page contained panels set in different locations, the location of the story in the majority of the panels was used as the standard. Any manga images in tasks (3), (4), or (5) for which the corresponding information was inherently ambiguous were excluded from the evaluation.
The performance of Llava-onevision varied across the tasks. We observed that the model performed relatively well in tasks involving location, time, and weather (tasks 3, 4, and 5), while its performance in character counting and panel order detection tasks was weaker. Adding annotations related to the content of the book helped the model better understand the story in the story reconstruction task (task 2), but it contributed little improvement to its performance in the other tasks.
In tasks (3), (4), and (5), the model demonstrated a strong ability to identify objects that aided its judgments, such as the sky, houses, windows, tables, lamps, trees, and chairs. This may be because, even in manga—where real-world objects are often highly abstracted—the appearance of such objective objects remains relatively unchanged. In contrast, the depiction of characters in manga often differs significantly from real-life appearances, posing challenges for the model in identifying characters. The model’s lower score in task (6), character counting, underscores this point, with its performance noticeably weaker than in other tasks of the image recognition tasks.
In task (2), the story reconstruction task, Llava-onevision demonstrated surprisingly promising capabilities. Upon examining the model’s outputs, we found that while its comprehension of storylines was not on par with human understanding, it could provide coherent descriptions of the recognizable elements in the scene. This suggests that Llava-onevision model has some capability for understanding coherent narratives, as each image consists of multiple panels forming a short, connected plot. However, we also noticed that the model struggled on pages with numerous onomatopoeic expressions, likely due to the difficulty of capturing the narrative information conveyed by these sound effects.
#### Future Works and Limitations
There remain many substantial works to be researched in the manga understanding field. The limitations of our study primarily fall into the following categories:
##### Challenges by the Unique Characteristics of Manga
Manga itself presents numerous distinctive features that pose challenges to MLLMs. For instance, the annotation language of the Manga109 dataset is Japanese instead of English. Although we attempted to mitigate this by supplementing with the PopManga dataset, performing the story reconstructioning task on Manga109 with the Llava-onevision model requires additional steps, such as OCR-based recognition of Japanese text, characters, and onomatopoeia, followed by their interpretation. This additional layer challenges Llava-onevision’s ability to understand manga content and increases the overall runtime of the model.
Furthermore, manga images are typically in black and white rather than color, which also impacts Llava-onevision’s reasoning capabilities. For instance, in tasks such as determining time of day or weather conditions, Llava-onevision often misinterprets predominantly black backgrounds as nighttime or rainy weather, resulting in reduced accuracy. Additionally, manga panels often contain non-dialogue elements, such as abstract or stylistic symbols, which are not confined to speech bubbles and pose additional challenges for MLLM comprehension.
##### Our Intrinsic Limitations in The Llava-onevision Model
The Llava-onevision model is not specifically designed for manga understanding tasks, unlike models such as Magi and Magiv2. Instead, it is a general-purpose model developed for multimodal tasks across various contexts, such as multi-image, image-video combinations, and cross-modal reasoning, where it performs exceptionally well. However, in our experiments, we primarily relied on prompt adjustments and evaluations of model outputs rather than fine-tuning the model’s weights. This limits our ability to comprehensively assess the potential of Llava-onevision in manga-related tasks, including manga understanding, panel inference, and specific symbols determination.
In the future, we plan to reorganize the Manga109 dataset to create a subset suitable for LoRA fine-tuning with LLAVA-onevision. Using the LoRA fine-tuning approach, we aim to train LLAVA specifically for recognizing Japanese manga images. This dataset should include single-panel images containing specific abstract elements, multi-panel images (entire manga pages), and annotations for their content (e.g., linework, onomatopoeia, etc.). Given the high cost and stringent copyright restrictions associated with independently collecting manga images and performing high-quality data annotation, this effort will rely on leveraging existing datasets while incorporating extensive manual work to ensure quality and relevance. Moreover, we could have benefited from a panel segmenter, as in that case we could have used Llava-onevision's Multi-Image functionality and evaluating the model would have been easier and more accurate.
It could have also benefited a lot from being able to use MagiV2, providing LLM-generated dialogue transcriptions instead of annotations from original dataset to Llava-onevision, but due to an unpatched bug in MagiV2's source code as of the time of writing, we were unable to use it in our research.
## Conclusion
We evaluated the LLaVA-onevision model's ability to recognize and understand single-page (multi-panel) manga images using six different tasks. Our findings indicate that LLaVA-onevision performs well in recognizing static and objective elements but struggles with character identification and panel sequence reasoning. Additionally, the model’s storytelling capabilities leave room for improvement, likely requiring targeted training on manga-specific datasets to enhance its performance.
## bibliography
{bibliography}