
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Chapter 2: How good is MT at translating Latin religious texts? &#8212; Chartering New Realms; AI as a Catalyst in Digital Humanities</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter2';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/chartering-new-realms-logo.png" class="logo__image only-light" alt="Chartering New Realms; AI as a Catalyst in Digital Humanities - Home"/>
    <img src="_static/chartering-new-realms-logo.png" class="logo__image only-dark pst-js-only" alt="Chartering New Realms; AI as a Catalyst in Digital Humanities - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to “Chartering New Realms: AI as a Catalyst in Digital Humanities”
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/pstroe/chartering-new-realms-2024/chapter_2?urlpath=lab/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/chapter2.md" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fchapter2.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/chapter2.ipynb" target="_blank"
   class="btn btn-sm btn-download-notebook-button dropdown-item"
   title="Download notebook file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li><a href="_sources/chapter2.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 2: How good is MT at translating Latin religious texts?</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#to-dos">TO DOS</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#previous-work">Previous Work</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#methods-data">Methods &amp; Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data">Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics">Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tools">Tools</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiments">Experiments</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#low-bleu-scores">Low BLEU scores</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#error-proofing">Error proofing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#retranslations">Retranslations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#results-discussion">Results &amp; Discussion</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analysing-scores">Analysing scores</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#de-legibus">De Legibus</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#history-of-the-kings-of-britain">History of the Kings of Britain</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#religious-texts">Religious texts</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#what-happened-to-job">What happened to Job</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-scores">Comparing scores</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#possible-explanations">Possible explanations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-4o">GPT-4o:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#google-translate">Google Translate:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gemini">Gemini:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#yandex">Yandex:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">Metrics:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">Bibliography</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix">Appendix</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-2-how-good-is-mt-at-translating-latin-religious-texts">
<h1>Chapter 2: How good is MT at translating Latin religious texts?<a class="headerlink" href="#chapter-2-how-good-is-mt-at-translating-latin-religious-texts" title="Link to this heading">#</a></h1>
<p>Stefano Staffa, Andrea Scheck</p>
<section id="to-dos">
<h2>TO DOS<a class="headerlink" href="#to-dos" title="Link to this heading">#</a></h2>
<p>~1. Write “Relevant Literature” (400 words)~</p>
<p>~2. Write “Methods &amp; Data” (200 words)~</p>
<p>~3. Write “Experiments” (650 words?)~</p>
<p>~4. Write “Conclusion” (200 words)~</p>
<p>~5. Revise “Introduction” and shorten (300 words)~</p>
<ol class="arabic simple" start="6">
<li><p>Revise “Results” and shorten (650 words?), answer questions:</p>
<ul class="simple">
<li><p>What tool has shown the best performance at translating Latin-English?</p></li>
<li><p>What metric has shown the best performance at scoring Latin-English translation? Or do we suggest using all 4 and averaging?</p></li>
<li><p>Is it easier for MT to translate neutral text than religious text?</p></li>
</ul>
</li>
<li><p>Write “Abstract” (max. 100 words)</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>In 1382, when the Latin Bible was first translated English by John Wycliffe, this process required enormous human effort, extensive knowledge of both Latin and the vernacular, and years of labor by many religious scholars. The result of this translation - a religious text which could be understood by the common population - had groundbreaking and far-reaching impacts on culture and religion. One can hardly imagine how history might have changed had the Bible never been translated into English—or translated less carefully.</p>
<p>Almost 650 years later, Machine Translation (MT) has reduced the effort required for translation processes from years to minutes. Even low-resource languages, like Latin, are increasingly translated with the assistance of these tools, with studies suggesting MT can achieve a reasonably good translation quality (source). However, while MT systems perform efficiently on many genres, they still face challenges when dealing with more creative works (<span id="id1">[<a class="reference internal" href="introduction.html#id22" title="Ana Isabel Cespedosa Vázquez and Ruslan Mitkov. Machine translation of literary texts: genres, times and systems. In Raquel Lázaro Gutiérrez, Antonio Pareja, and Ruslan Mitkov, editors, Proceedings of the First Workshop on NLP Tools and Resources for Translation and Interpreting Applications, 48–53. Varna, Bulgaria, September 2023. INCOMA Ltd., Shoumen, Bulgaria. URL: https://aclanthology.org/2023.nlp4tia-1.7.">Cespedosa Vázquez and Mitkov, 2023</a>]</span>), of which the Bible with its poems and psalms contains many.</p>
<p>Given that there are many Latin religious texts which remain untranslated to this day, this chapter raise the question: Does MT serve as a fitting tool for translating Latin religious text to English? To explore this, we examine the performance of four advanced MT systems (GPT-4o, Gemini, Google Translate, and Yandex) when handling Latin religious texts compared to more neutral, descriptive Latin passages. By comparing the results to each other and to the gold standard human translation, we aim to shed light on the efficacy and limitations of MT in translating Latin, identifying which tools are better suited for specific genres of texts.
By understanding these distinctions, we hope to contribute to future advancements in MT for low-resource languages and support the translation of the vast untranslated Latin texts which could provide valuable insights into the historical and intellectual evolution of the Western world.</p>
</section>
<section id="previous-work">
<h2>Previous Work<a class="headerlink" href="#previous-work" title="Link to this heading">#</a></h2>
<p>Work on MT and language modelling for Latin has progressed significantly since the advent of MT, leveraging both traditional and modern approaches. Martínez Garcia and García Tejedor (<span id="id2">Martínez Garcia and García Tejedor [<a class="reference internal" href="introduction.html#id9" title="Eva Martínez Garcia and Álvaro García Tejedor. Latin-Spanish neural machine translation: from the Bible to saint augustine. In Rachele Sprugnoli and Marco Passarotti, editors, Proceedings of LT4HALA 2020 - 1st Workshop on Language Technologies for Historical and Ancient Languages, 94–99. Marseille, France, 2020. European Language Resources Association (ELRA). URL: https://aclanthology.org/2020.lt4hala-1.14.">2020</a>]</span>) utilized the Bible as a parallel corpus to build a Transformer-based Neural Machine Translation (NMT) system for Latin-Spanish, addressing challenges associated with Latin’s complex morphology and low-resource nature. Similarly, Christodouloupoulos and Steedman (<span id="id3">Christodoulopoulos and Steedman [<a class="reference internal" href="introduction.html#id10" title="Christos Christodoulopoulos and Mark Steedman. A massively parallel corpus: the bible in 100 languages. Language Resources and Evaluation, 49:1–21, 06 2014. URL: https://doi.org/10.1007/s10579-014-9287-y, doi:10.1007/s10579-014-9287-y.">2014</a>]</span>) demonstrated the utility of the Bible as a parallel corpus in over 100 languages, including Latin, highlighting its structure and consistency as beneficial for multilingual NLP tasks. Liu et al. (<span id="id4">Liu <em>et al.</em> [<a class="reference internal" href="introduction.html#id11" title="Ling Liu, Zach Ryan, and Mans Hulden. The usefulness of bibles in low-resource machine translation. Proceedings of the Workshop on Computational Methods for Endangered Languages, 1:, 01 2021. URL: https://doi.org/10.33011/computel.v1i.957, doi:10.33011/computel.v1i.957.">2021</a>]</span>) further confirmed the effectiveness of using biblical texts for improving MT in low-resource settings, underscoring their relevance for Latin translation tasks.</p>
<p>Beyond religious texts, Fischer et al. (<span id="id5">Fischer <em>et al.</em> [<a class="reference internal" href="introduction.html#id15" title="Lukas Fischer, Patricia Scheurer, Raphael Schwitter, and Martin Volk. Machine translation of 16th century letters from latin to german. In Proceedings of the Workshop on Computational Methods for Historical Texts. 01 2022.">2022</a>]</span>) explored translating 16th-century Latin letters into German, emphasizing the importance of tailored training data for domain-specific translation. Bistafa (<span id="id6">Bistafa [<a class="reference internal" href="introduction.html#id12" title="Sylvio R. Bistafa. Translating scientific latin texts with artificial intelligence: the works of euler and contemporaries. 2024. URL: https://doi.org/10.48550/arXiv.2307.07520, arXiv:2307.07520, doi:10.48550/arXiv.2307.07520.">2024</a>]</span>) examined the challenges of translating Latin scientific texts, particularly the works of mathematician Leonhard Euler, using artificial intelligence and revealing complexities in specialized vocabulary and syntax. These studies highlight the diversity of Latin translation tasks and the potential of MT systems in addressing them.</p>
<p>Recent advancements in Large Language Models (LLMs) have further expanded the possibilities for Latin MT. Volk et al. (<span id="id7">[<a class="reference internal" href="introduction.html#id13" title="Martin Volk, Dominic Philipp Fischer, Lukas Fischer, Patricia Scheurer, and Phillip Ströbel. Llm-based machine translation and summarization for latin. In LT4HALA. 2024. URL: https://doi.org/10.5167/uzh-259369.">Volk <em>et al.</em>, 2024</a>]</span>) evaluated GPT-4’s performance on both translation and summarization of Latin texts, achieving superior results compared to traditional MT systems and showcasing LLMs’ capabilities in handling historical and low-resource languages. Riemenschneider and Frank (<span id="id8">Riemenschneider and Frank [<a class="reference internal" href="introduction.html#id16" title="Frederick Riemenschneider and Anette Frank. Exploring large language models for classical philology. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 15181–15199. Toronto, Canada, jul 2023. Association for Computational Linguistics. URL: https://doi.org/10.18653/v1/2023.acl-long.846, doi:10.18653/v1/2023.acl-long.846.">2023</a>]</span>) trained multilingual LLMs on Latin corpora, achieving state-of-the-art results for part-of-speech tagging and lemmatization, evaluated against the EvaLatin 2022 dataset. These works collectively provide a strong foundation for investigating LLM-based translation for Latin texts across religious and non-religious domains, as undertaken in this project.</p>
</section>
<section id="methods-data">
<h2>Methods &amp; Data<a class="headerlink" href="#methods-data" title="Link to this heading">#</a></h2>
<p>We constructed a corpus of Latin texts spanning both religious and non-religious genres to evaluate translation performance across diverse stylistic and thematic categories. The dataset contains 1’398 unique Latin words and 1’685 unique English words. Of these, religious texts contribute 566 unique Latin words and 937 unique English words, while non-religious texts comprise 832 unique Latin words and 748 unique English words. The dataset includes approximately 180 sentences drawn from texts written between the 1st century BCE and the 12th century CE.</p>
<section id="data">
<h3>Data<a class="headerlink" href="#data" title="Link to this heading">#</a></h3>
<p>Religious texts were sourced from the Biblia Sacra iuxta Vulgatam Clementinam and comprise 60 passages distributed across 4 songs, 3 poetic passages, and 4 neutral passages. Each Latin text is paired with three English translations from the English Standard Version (<span id="id9">[<a class="reference internal" href="introduction.html#id23" title="Crossway. ESV. Crossway, 2001. English translation.">Crossway, 2001</a>]</span>), Douay-Rheims (<span id="id10">[<a class="reference internal" href="introduction.html#id24" title="The Challoner Revision. DRB. Douay Bible, 1899. Catholic Bible translation.">Revision, 1899</a>]</span>), and King James Version (<span id="id11">[<a class="reference internal" href="introduction.html#id25" title="King James Bible. KJV. Robert Barker, 1611. Standard Protestant Bible translation.">Bible, 1611</a>]</span>). The inclusion of three Bible versions captures theological, cultural, and stylistic differences, providing a nuanced basis for comparison. For instance, the DRB was translated directly from the Latin Vulgate and, as a result, adheres more closely to Latin phrasing. The KJV and the ESV drew from Hebrew, Greek and Latin and adopt a modernized style.</p>
<p>To allow for an optimal comparison of MT performance, we aimed to chose neutral, descriptive Latin passages as sources for the non-religious excerpts. As many well-established Latin works with English translations tend to be religious, philosophical, or fictional, identifying a truly neutral text was a challenge. We selected Cicero’s De Legibus, a key legal text, and Geoffrey of Monmouth’s Historia Regum Britanniae, which includes descriptive historical narratives.</p>
<p>This combination of literal and non-literal translations was expected to highlight interpretative variations for MT systems. Poetic passages, in particular, require systems to balance semantic accuracy with stylistic complexity, while neutral texts test straightforward syntactical translations.</p>
</section>
<section id="metrics">
<h3>Metrics<a class="headerlink" href="#metrics" title="Link to this heading">#</a></h3>
<p>Translation quality was assessed using four widely recognized metrics: BLEU, ROUGE-L, METEOR, and chrF. Each metric captures a distinct dimension of translation quality, providing a comprehensive evaluation framework. All scores were calculated as percentages from 0 to 100. Thresholds were set to classify scores, with scores below 30 seen as faulty translations and scores exceeding 60 seen as high-quality translations.</p>
<p><strong>Bilingual Evaluation Understudy (BLEU)</strong></p>
<p>BLEU measures the overlap of n-grams (sequences of 1 to 4 words) between the MT and one or more reference translations (<span id="id12">[<a class="reference internal" href="introduction.html#id5" title="Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin, editors, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, 311–318. Philadelphia, Pennsylvania, USA, 2002. Association for Computational Linguistics. URL: https://doi.org/10.3115/1073083.1073135, doi:10.3115/1073083.1073135.">Papineni <em>et al.</em>, 2002</a>]</span>). It calculates precision for these n-grams and includes a brevity penalty to discourage overly short translations. The metric is particularly suited for evaluating literal translations where exact word matches are critical. However, BLEU is less effective for assessing translations with valid paraphrasing or synonym use, as it does not account for semantic similarity or contextual nuance. A BLEU score of 75 implies high fidelity, while scores below 30 suggest significant deviations from the reference.</p>
<p><strong>Recall-Oriented Understudy for Gisting Evaluation (ROUGE-l)</strong></p>
<p>ROUGE-L evaluates the longest common subsequence (LCS) between the machine-generated translation and the reference text (<span id="id13">[<a class="reference internal" href="introduction.html#id6" title="Chin-Yew Lin and Franz Josef Och. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), 605–612. Barcelona, Spain, 2004. URL: https://doi.org/10.3115/1218955.1219032, doi:10.3115/1218955.1219032.">Lin and Och, 2004</a>]</span>). This emphasizes structural similarity, focusing on recall—the proportion of the reference that appears in the generated text. Unlike BLEU, ROUGE-L is not limited to contiguous n-grams, making it useful for evaluating texts with more flexible word order, such as Latin prose and poetry. It excels in identifying translations that preserve the overall structure and flow of the reference, even if individual word choices differ.</p>
<p><strong>Metric for Evaluation of Translation with Explicit ORdering (METEOR)</strong></p>
<p>METEOR extends beyond BLEU and ROUGE-L by incorporating precision, recall, and additional linguistic features such as stemming (reducing words to their root forms) and synonym matching (<span id="id14">[<a class="reference internal" href="introduction.html#id7" title="Satanjeev Banerjee and Alon Lavie. METEOR: an automatic metric for MT evaluation with improved correlation with human judgments. In Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare Voss, editors, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, 65–72. Ann Arbor, Michigan, 2005. Association for Computational Linguistics. URL: https://aclanthology.org/W05-0909.">Banerjee and Lavie, 2005</a>]</span>). It aligns words semantically rather than strictly lexically, enabling a better assessment of idiomatic expressions, paraphrasing, and non-literal translations. METEOR also assigns higher weights to exact matches while still rewarding partial matches and correct word order. This makes it particularly effective for evaluating poetic and figurative texts where semantic equivalence outweighs literal fidelity. Higher scores reflect greater similarity to the reference, factoring in synonyms and rephrased segments.</p>
<p><strong>Character n-gram F-score (chrF)</strong></p>
<p>chrF operates at the character level, comparing sequences of character n-grams between the machine translation and the reference text (<span id="id15">[<a class="reference internal" href="introduction.html#id8" title="Maja Popović. ChrF: character n-gram F-score for automatic MT evaluation. In Ondřej Bojar, Rajan Chatterjee, Christian Federmann, Barry Haddow, Chris Hokamp, Matthias Huck, Varvara Logacheva, and Pavel Pecina, editors, Proceedings of the Tenth Workshop on Statistical Machine Translation, 392–395. Lisbon, Portugal, 2015. Association for Computational Linguistics. URL: https://doi.org/10.18653/v1/W15-3049, doi:10.18653/v1/W15-3049.">Popović, 2015</a>]</span>). This fine-grained approach is especially advantageous for highly inflectional languages like Latin, where slight morphological differences (e.g., verb conjugations or noun declensions) can significantly alter meaning. By focusing on characters rather than words, chrF provides sensitivity to subtle grammatical nuances that may not be captured by word-based metrics. It also avoids penalizing legitimate variations in word segmentation or inflection. For Latin translations, chrF is particularly valuable for detecting morphological accuracy and alignment with the reference text.</p>
</section>
<section id="tools">
<h3>Tools<a class="headerlink" href="#tools" title="Link to this heading">#</a></h3>
<p>Machine translations were generated using GPT-4, Google Translate, Gemini, and Yandex Translate. These systems leverage either pre-trained language models or statistical algorithms to translate Latin texts into English. Each system has strengths suited to specific text types: GPT-4 excels at contextual and semantic nuances, while Google Translate often delivers consistent outputs for literal translations.</p>
<p>Automated scoring scripts, written in Python, were used to evaluate the MT against gold-standard references. By combining diverse data sources, detailed metrics, and advanced translation systems, this methodology provides a comprehensive framework for evaluating machine translation performance on Latin texts of varying complexity and stylistic nuance.</p>
</section>
</section>
<section id="experiments">
<h2>Experiments<a class="headerlink" href="#experiments" title="Link to this heading">#</a></h2>
<p>The experimental setup began with selecting original Latin excerpts from the aforementioned sources and their corresponding gold-standard translations. The excerpts were translated from Latin to English individually using the web interfaces of Google Translate and Yandex. For GPT-4o and Gemini, translations were conducted in separate conversations, preceded by a standardized prompt to limit the influence of prior knowledge or external context on the outputs<sup>1</sup>. Each translation was then scored against the gold standard using the four metrics (BLEU, ROUGE, METEOR, and chrF), resulting in a matrix with four translations per excerpt and four scores per translation.</p>
<p>Table 1 allows for a look into the translation results:
(<em>here will follow an interactive code block which the reader can run to see a random example of one Latin excerpt and its translation by all MT systems</em>)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">Markdown</span>

<span class="n">csv_path</span> <span class="o">=</span> <span class="s2">&quot;data/translations.csv&quot;</span>  <span class="c1"># CSV with all translations</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csv_path</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;;&#39;</span><span class="p">)</span>

<span class="n">random_row</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Select a random row</span>

<span class="c1"># Show in a table</span>
<span class="n">table</span> <span class="o">=</span> <span class="s2">&quot;| Header | Content |</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="n">table</span> <span class="o">+=</span> <span class="s2">&quot;|--------|---------|</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="k">for</span> <span class="n">column</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">random_row</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="c1"># Only show the first 400 characters</span>
    <span class="n">truncated_value</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)[:</span><span class="mi">400</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;...&quot;</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">400</span> <span class="k">else</span> <span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="n">table</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;| </span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">truncated_value</span><span class="si">}</span><span class="s2"> |</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="n">display</span><span class="p">(</span><span class="n">Markdown</span><span class="p">(</span><span class="n">table</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">import</span> <span class="nn">random</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">Markdown</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;pandas&#39;
</pre></div>
</div>
</div>
</div>
<section id="low-bleu-scores">
<h3>Low BLEU scores<a class="headerlink" href="#low-bleu-scores" title="Link to this heading">#</a></h3>
<p>Examining the scoring results, we observed low BLEU scores across texts, with an overall BLEU average of 24.86%. Of the 49 translations, 35 received a BLEU score below the threshold of 30%, indicating notable errors in lexical or syntactic accuracy. Since the scores of all metrics were averaged, these low BLEU scores negatively influenced the overall results of almost every translation. To address this, we also considered the median of the metrics alongside the average, mitigating the impact of outliers caused by low BLEU scores.</p>
<p>While seemingly low, these scores align with prior research: Volk et al. (2024) observed a BLEU score of 25.22% for Google Translate and 34.50% for GPT-4. Our findings, with BLEU averages of 25.32% for Google Translate and 56.69% for GPT-4, indicate no major errors during the experiments but consistency in the outputs, while also highlighting a notable improvement in GPT-4’s performance compared to earlier projects.</p>
<p>Table 2 allows for a look into the scores for a random translation:
(<em>here will follow an interactive code block which the reader can run to see a random example of one Latin excerpt and its scores in all metrics</em>)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">Markdown</span>

<span class="n">csv_path_scores</span> <span class="o">=</span> <span class="s2">&quot;data/scores.csv&quot;</span>  <span class="c1"># CSV with all scores</span>
<span class="n">data_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csv_path_scores</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;;&#39;</span><span class="p">)</span>

<span class="n">random_row_scores</span> <span class="o">=</span> <span class="n">data_scores</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Select a random row</span>

<span class="c1"># Show in a table</span>
<span class="n">table_scores</span> <span class="o">=</span> <span class="s2">&quot;| Source | BLEU average | ROUGE average | chrF average | METEOR average |</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="n">table_scores</span> <span class="o">+=</span> <span class="s2">&quot;|--------|--------------|---------------|--------------|----------------|</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="k">for</span> <span class="n">column</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">random_row_scores</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="c1"># Only show the first 400 characters for the Source (although it shouldn&#39;t exceed 400 characters)</span>
    <span class="n">truncated_value</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)[:</span><span class="mi">400</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;...&quot;</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">400</span> <span class="k">else</span> <span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="n">table_scores</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;| </span><span class="si">{</span><span class="n">truncated_value</span><span class="si">}</span><span class="s2"> &quot;</span> <span class="k">if</span> <span class="n">column</span> <span class="o">==</span> <span class="s1">&#39;Source&#39;</span> <span class="k">else</span> <span class="sa">f</span><span class="s2">&quot;| </span><span class="si">{</span><span class="n">truncated_value</span><span class="si">}</span><span class="s2"> &quot;</span>
<span class="n">table_scores</span> <span class="o">+=</span> <span class="s2">&quot;|</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="n">display</span><span class="p">(</span><span class="n">Markdown</span><span class="p">(</span><span class="n">table_scores</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="error-proofing">
<h3>Error proofing<a class="headerlink" href="#error-proofing" title="Link to this heading">#</a></h3>
<p>Six translations received an overall average score below 30%, indicating significant errors in the translation. These included Psalm 88:3-7 (DRB), Psalm 23:4-6 (DRB), Book 1 Chapter 13 of The History of the Kings of Britain, Job 3:11-13 (in both the ESV and KJV), and Book 1 Section 40 of De Legibus. Upon review, we identified issues in three of the corresponding gold standard translations, where they had been either incorrectly or incompletely processed. After addressing these discrepancies, the average score for the affected excerpts increased to slightly above 30%, marking them as acceptable translations.</p>
</section>
<section id="retranslations">
<h3>Retranslations<a class="headerlink" href="#retranslations" title="Link to this heading">#</a></h3>
<p>After these corrections, three texts remained with an average score below 30%: Book 1 Chapter 13 of the History of the Kings of Britain and Job 3: 11-13 in both ESV and KJV. To address this, we adjusted the translation workflow by translating each sentence individually rather than entire paragraphs. This approach aimed to mitigate the challenges posed by long, syntactically complex structures, which are particularly difficult for MT systems to handle (He, 2023). While this method improved the score for The History of the Kings of Britain by 1.8%, it did not result in significant changes for Job 3:11-13. Ultimately, the average scores for all three excerpts remained below 30%. Possible reasons for these results are explored in detail in the following chapter.</p>
</section>
</section>
<section id="results-discussion">
<h2>Results &amp; Discussion<a class="headerlink" href="#results-discussion" title="Link to this heading">#</a></h2>
<section id="analysing-scores">
<h3>Analysing scores<a class="headerlink" href="#analysing-scores" title="Link to this heading">#</a></h3>
<p>Considering the average and median values of all chosen metrics, we compared the translation quality between the different genres of text. The highest and lowest scores per text source were examined further to be sure there were no processing errors.
<img alt="Scatter plot" src="https://github.com/user-attachments/assets/f86b18c9-e09b-4607-8be7-c755181ba1e9" /></p>
<section id="de-legibus">
<h4>De Legibus<a class="headerlink" href="#de-legibus" title="Link to this heading">#</a></h4>
<p>All translations of De Legibus lie above the quality threshold of 30 %, with five out of six sections scoring above 41% when considering the average, and above 45% when considering the median. The average and median scores for De Legibus generally fall within a narrow range, indicating consistent performance across different sections. Book 1 Section 60 had the highest scores (average of 46.82% and median of 48.73%), possibly due to its closer adherence to the original text in both structure and meaning. However, there is a notable outlier in Book 1 Section 40, which achieved an average of only 33.61%.</p>
<p>Considering <strong>Book 1 Section 40</strong> in detail, it became apparent that the gold standard translation diverged significantly from the Latin source, using fewer details and replacing long descriptive clauses with brief summaries. This abbreviation of content could cause fewer overlapping n-grams when compared to translations that more closely adhered to the original text. Additionally, the gold standard used a more modernized tone, losing certain phrases such as the justification of crimes through “naturae iure” and softening the vivid imagery of torture. This semantic difference could reduce matches in metrics like METEOR and chrF, which prioritize meaning preservation.</p>
</section>
<section id="history-of-the-kings-of-britain">
<h4>History of the Kings of Britain<a class="headerlink" href="#history-of-the-kings-of-britain" title="Link to this heading">#</a></h4>
<p>The translations for History of the Kings of Britain show a wider range of scores, with three out of four translations achieving average scores above 30%, but none exceeding 40%. The average scores ranged from 27.33% to 36.69% with median scores falling between 30.30% and 40.59%. Notably, Book 1, Chapter 13 received the lowest average score and fell below the acceptable threshold of 30%.</p>
<p>Examining <strong>Book 1, Chapter 13</strong> in detail, the Latin source used a vivid, descriptive language, explicitely depicting the violence of Corineus’s actions. For example, the original described him severing arms, decapitating enemies, and engaging in  physical combat. The gold standard translation simplified this text by consolidating actions and reducing the intensity. For example, the gold standard translation used phrases like “terrible slaughter” and omitted the specific actions, which could lead to lower BLEU scores due to discrepancies in n-gram matches.</p>
<p>As this text fell below the threshold of 30%, we also retranslated it with each MT system sentence by sentence instead of as a whole paragraph, to ensure that the length of the text was not a negative influence on the scores. This marginally improved the scores, with the average rising from 27.33 to 29.13, a percentage-wise improvement of approximately 6.57 %. Still, even the improved score from sentence-to-sentence-translation was not adequate enough to reach the 30%-threshold. (Gehört ev. mehr Richtung Methods)</p>
</section>
<section id="religious-texts">
<h4>Religious texts<a class="headerlink" href="#religious-texts" title="Link to this heading">#</a></h4>
<p>For the bible translations, we translated the Latin Vulgate as the original source and compared the outcomes to three different English versions of the bible.</p>
<p>For the Bible translations using the King James Version (KJV) as the gold standard, 12 of 13 translations achieved an average score greater than 32.75%, with the highest score reaching 64.05%. Among these high-performing texts were neutral descriptions of law and genealogy, stories as well as songs and poems. There was no clear correlation between the scores and the “neutrality” of the text. The lowest score, recorded for Job 3:11-13, was 28.44%, which deviated significantly from the other translations.</p>
<p>For the Bible translations using the English Standard Version (ESV) as the gold standard, 12 out of 13 texts received an average score above 35.85%, with the highest average score being 65.43%. Most of the texts translated with the ESV gold standard showed consistently high scores across the BLEU, ROUGE, chrF, and METEOR metrics. The average scores for these texts generally remained well above 35%, with scores consistently improving for passages like Psalm 23:4-6 (65.43%) and Exodus 7:20-24 (61.25%). Again, There was no clear correlation between the scores and the “neutrality” of the text, with a mixture of neutral texts, songs and poems across all scores. Notably, the ESV gold standard translation stands out because it was the only comparison text with a significant number of texts (3) achieving an average score above 60%. The lowest score, recorded for Job 3:11-13, was 27.54%.</p>
<p>For the Bible translations using the Douay-Rheims 1899 American Edition (DRB) as the gold standard, all texts received a score average above 45.41 %, with the highest average reaching 67.65 %, and the corresponding median at 73.06 %. Remarkably, even Job 3:11-13, which presented challenges and low scores for the other Bible versions, achieved a significantly better score of 46.52% in the DRB translation. The DRB translations consistently achieved higher scores across all the MT systems and less variation in performance across the passages compared to both the KJV and ESV.  Notably, this was the only comparison project in which no text had a translation score average under 30, suggesting that the language style and structure of the DRB translation may be more amenable to machine translation systems, particularly in the semantic equivalence captured by the evaluation metrics (e.g., ROUGE, METEOR). The lowest score was recorded for Psalm 88:4-8, but at 45.41%, it still performed significantly better than the same passage in the KJV and ESV translations.</p>
<section id="what-happened-to-job">
<h5>What happened to Job<a class="headerlink" href="#what-happened-to-job" title="Link to this heading">#</a></h5>
<p>The Latin text of Job 3:11-13 contains highly poetic language that reflects emotional distress, characterized by the repetition of rhetorical questions such as, “Why did I not die in the womb?” and “Why was I nursed at the breasts?”. This type of lyrical and repetitive structure can pose challenges any translators, as they may struggle to preserve the nuanced tone and rhetorical structure. This is amplified by the fact that the Latin word order is flexible and emphasizes certain elements for rhetorical effect, which is not always directly translatable into English. These challenges become evident when comparing the gold standard translation from KJV, ESV and DRB:</p>
<p>While both KJV as well as ESV are faithful to the original meaning and easily readable for modern eyes, they adhere to a modernized language structure that simplifies the emotional weight conveyed in the Latin source. For example, a poetic expression like “dormiens silerem” becomes the more dry “lain still/down and been quiet”, with less expressive language. The Latin differentiation between “in vulva” (approximately: in the womb) and “ex utero” (out of the womb) might have been a additional challenge for the gold standard translators, being translated not very precisely to “at birth” or “from the womb”. Additionally, the expression “give up the ghost” in the KJV translation was not replicated by MT.</p>
<p>In the DRB translation, interestingly, the same passage received an acceptable average score, likely due to the DRB structure being slightly more compatible with machine translation systems. The language and repetition in the source Latin are preserved well in the DRB, allowing for more accurate semantic mappings across the translations. Notably, however, this translation does not read as a well-formed text to a modern reader. Questions which entirely lack a subject, such as “Why received upon the knees? Why suckled at the breasts?” might be close to the Latin original and also close to the MT results, but hardly how we would form a sentence today.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Verse</p></th>
<th class="head"><p>Latin</p></th>
<th class="head"><p>ESV</p></th>
<th class="head"><p>KJV</p></th>
<th class="head"><p>DRB</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Job 3: 11-13</strong></p></td>
<td><p>Quare non in vulva mortuus sum? <br> egressus ex utero non statim perii? <br> Quare exceptus genibus? <br> cur lactatus uberibus? <br> Nunc enim dormiens silerem, <br> et somno meo requiescerem</p></td>
<td><p>Why did I not die at birth, <br> come out from the womb and expire? <br> Why did the knees receive me? <br> Or why the breasts, that I should nurse? <br> For then I would have lain down and been quiet; <br> I would have slept; then had I been at rest,</p></td>
<td><p>Why died I not from the womb? <br> Why did I not give up the ghost when I came out of the belly? <br> Why did the knees prevent me? <br> Or why the breasts that I should suck? <br> For now should I have lain still and been quiet, <br> I should have slept: then had I been at rest,</p></td>
<td><p>Why did I not die in the womb, <br> why did I not perish when I came out of the belly? <br> Why received upon the knees? <br> Why suckled at the breasts? <br> For now I should have been asleep and still, <br> and should have rest in my sleep.</p></td>
</tr>
</tbody>
</table>
</div>
<p>The same challenges which lead to deviation between the different gold standards also affected the MT systems:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Latin</strong></p></th>
<th class="head"><p><strong>GPT-4o</strong></p></th>
<th class="head"><p><strong>Google Translate</strong></p></th>
<th class="head"><p><strong>Gemini</strong></p></th>
<th class="head"><p><strong>Yandex</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Quare non in vulva mortuus sum?</strong></p></td>
<td><p>Why did I not die in the womb?</p></td>
<td><p>Why did I not die in the womb?</p></td>
<td><p>Why not in the womb did I die?</p></td>
<td><p>Why am I not dead in the womb?</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Egressus ex utero non statim perii?</strong></p></td>
<td><p>Having left the uterus, why did I not perish immediately?</p></td>
<td><p>Did I not immediately perish when I came out of the womb?</p></td>
<td><p>Having gone forth from the womb not immediately did I perish?</p></td>
<td><p>Going out of the womb was not immediately ruined?</p></td>
</tr>
<tr class="row-even"><td><p><strong>Quare exceptus genibus?</strong></p></td>
<td><p>Why was I received upon the knees?</p></td>
<td><p>Why except the knees?</p></td>
<td><p>Why caught by the knees?</p></td>
<td><p>Why knees?</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Cur lactatus uberibus?</strong></p></td>
<td><p>Why was I nursed at the breasts?</p></td>
<td><p>Why did he breastfeed?</p></td>
<td><p>Why suckled by the breasts?</p></td>
<td><p>Why breastfeed?</p></td>
</tr>
<tr class="row-even"><td><p><strong>Nunc enim dormiens silerem,</strong></p></td>
<td><p>For now, sleeping, I would be silent,</p></td>
<td><p>For now, sleeping in silence,</p></td>
<td><p>Now indeed sleeping I would be silent,</p></td>
<td><p>Silent sleeping for now,</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Et somno meo requiescerem.</strong></p></td>
<td><p>And in my sleep, I would rest.</p></td>
<td><p>And I would rest in my sleep.</p></td>
<td><p>And in my sleep I would rest.</p></td>
<td><p>I need my sleep.</p></td>
</tr>
</tbody>
</table>
</div>
<p>The repetition of rhetorical questions, expressing anguish and lamentation rather than seeking actual answers, seems to have been highly difficult for MT, which appear to interpret the words literally. The “Why” structure is abandoned in the second sentence by Google Translate, Gemini and Yandex, rather asking “if” the speaker died; in the third and fourth sentence, they even lose the reference to the subject (“I”). The emotional tone seems hard to reproduce, with very practical sounding results such as “I need my sleep”. Additionally, vocabulary ambiguities also occur due to the very limited context, with “perii” (perished) translated as “ruined” and “exceptus” (received) as “caught”.</p>
<p>A retranslation experiment, in which the text was translated sentence by sentence, showed an improvement in overall scores when compared to ESV and KJV, particularly in BLEU. This suggests that translating smaller units may help maintain structure and preserve n-gram matches, even in poetic or emotionally dense texts. The results from the sentence-by-sentence approach indicate that smaller segments may allow the MT system to more accurately replicate individual sentence structures, thus improving alignment with the original text. This underscores the difficulty of evaluating poetic texts using metrics like BLEU, which are heavily influenced by word-for-word accuracy rather than meaning preservation and stylistic nuances.</p>
</section>
</section>
</section>
<section id="comparing-scores">
<h3>Comparing scores<a class="headerlink" href="#comparing-scores" title="Link to this heading">#</a></h3>
<p>Non-religious texts, on average, did not perform better than religious texts. Religious texts showed higher overall scores and consistency across versions.</p>
<ul class="simple">
<li><p>De Legibus had average scores mostly between 33.61% and 48.54%.</p></li>
<li><p>History of the Kings of Britain scores ranged between 27.33% and 36.69%, performing worse overall.</p></li>
<li><p>Religious texts in the Douay-Rheims version (e.g., Exodus 7: 20-24, Psalm 23, Ecclesiastes 3: 7-8) performed particularly well, often exceeding 50% averages.</p></li>
<li><p>Even the lowest-performing religious texts (Job 3: 11-13) had scores comparable to or better than the poorest-performing non-religious texts.</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Source</p></th>
<th class="head"><p>Average of Averages (%)</p></th>
<th class="head"><p>Median of Averages (%)</p></th>
<th class="head"><p>General Performance Ranking</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><em>Douay-Rheims Bible (DRB)</em></p></td>
<td><p><strong>52.35</strong></p></td>
<td><p><strong>55.85</strong></p></td>
<td><p>Highest-performing source</p></td>
</tr>
<tr class="row-odd"><td><p><em>English Standard Version (ESV)</em></p></td>
<td><p>47.96</p></td>
<td><p>48.88</p></td>
<td><p>Strong, consistent</p></td>
</tr>
<tr class="row-even"><td><p><em>King James Version (KJV)</em></p></td>
<td><p>45.06</p></td>
<td><p>46.93</p></td>
<td><p>Good, slightly behind ESV</p></td>
</tr>
<tr class="row-odd"><td><p><em>De Legibus</em></p></td>
<td><p>43.42</p></td>
<td><p>47.16</p></td>
<td><p>Moderate, varying</p></td>
</tr>
<tr class="row-even"><td><p><em>History of the Kings</em></p></td>
<td><p><strong>33.08</strong></p></td>
<td><p><strong>37.11</strong></p></td>
<td><p>Lowest-performing source</p></td>
</tr>
</tbody>
</table>
</div>
<p>Additionally, whether neutral-tone texts generally performed better than poetry or songs could not be generally stated:</p>
<ul class="simple">
<li><p>The best-performing neutral texts, such as Exodus 7: 20-24 (Douay-Rheims), scored higher than most poetry, with averages exceeding 67%.</p></li>
<li><p>However, neutral texts like History of the Kings of Britain had lower scores, often around 30-40%, dragging down the overall performance.</p></li>
<li><p>Poetry showed a wide range of performance, with Ecclesiastes 3: 7-8 (63.84%-64.56%) outperforming neutral texts but Job 3: 11-13 struggling with scores around 27-29%.</p></li>
<li><p>Songs consistently scored higher than poetry. Examples include Psalm 23 and Psalm 149, which averaged 46-65%.</p></li>
</ul>
<p>(<em>Notizen, die nicht zu meinem Vergleich gehören, aber aufgefallen sind</em></p>
<ul class="simple">
<li><p>The BLEU scores for this source were notably low, especially for Book 1, Chapter 13, where it was just 6.71%. This highlights the difficulty of machine translation in retaining exact word matches when the source text is rich in descriptive detail. This also lead to a noticeable gap between average and median scores, indicating that most translations performed better than the overall average, but the BLEU scores pulled down the mean scores significantly.)</p></li>
</ul>
</section>
<section id="possible-explanations">
<h3>Possible explanations<a class="headerlink" href="#possible-explanations" title="Link to this heading">#</a></h3>
<p>Many automatic metrics like BLEU, ROUGE, chrF, and METEOR rely on matching n-grams, tokens, or semantic similarity. They are blind to readability for humans, only measuring overlap and surface-level similarity.</p>
<p>The DRB, being older, was translated with stricter adherence to the source text’s structure and meaning. Modern translations like ESV and KJV adapt to contemporary language, which introduces idiomatic changes, omissions, or reinterpretations for readability — this decreases literal similarity with the source.
Machine translations, often designed to process texts literally, are more likely to score well with the DRB as a target text, since it shares more characteristics with the source by keeping the word order, grammar and phrasing more similar to the Latin, even if the language is archaic.</p>
<p>When comparing the scores between the DRB and De Legibus and History of the Kings, the latter two often feature complex sentence structures, with frequent use of subordinate clauses, legal jargon, or historical references. De Legibus in particular uses legal and philosophical terminology, which seems challenging for MT models. On the other hand, the DRB has a very formal, literal language, with the bible in general having a highly consistent structure, which MT systems can more easily map.
Religious texts, particularly those like the DRB, often have a well-established vocabulary and phrasing that aligns with traditional theological discourse. MT systems may have encountered similar texts during training, allowing for more accurate translations. De Legibus and History of the Kings of Britain belong to genres (philosophy, law, and history) that may not be as heavily represented in training data for machine translation systems.</p>
<section id="gpt-4o">
<h4>GPT-4o:<a class="headerlink" href="#gpt-4o" title="Link to this heading">#</a></h4>
<p>GPT-4o emerges as the most versatile and reliable performer across all evaluated metrics. For biblical texts, it achieves the best results with a BLEU score of 28.74 and a ROUGE score of 58.27, reflecting a high degree of accuracy in both lexical and structural reproduction. The chrF score of 54.44 and METEOR score of 57.28 further underscore its ability to maintain semantic and stylistic alignment.
For non-biblical texts, while its performance decreases slightly, with a BLEU score of 19.10 and a ROUGE score of 47.94, GPT-4o remains robust and consistent, managing stylistic diversity better than other tools. Its particularly strong results for biblical texts suggest an aptitude for formal, repetitive, and structured content, though some challenges arise in handling more stylistically varied material.</p>
</section>
<section id="google-translate">
<h4>Google Translate:<a class="headerlink" href="#google-translate" title="Link to this heading">#</a></h4>
<p>Google Translate delivers solid translation results but consistently ranks just below GPT-4o across all metrics. Its performance with biblical texts, reflected in a BLEU score of 26.98, ROUGE score of 57.43, and chrF score of 53.35, highlights its fluency and adequacy in reproducing structured content. A METEOR score of 53.36 indicates reasonably strong semantic alignment.
However, its capabilities diminish more noticeably with non-biblical texts, where it scores a BLEU of 18.37 and a ROUGE of 46.08. The chrF score of 49.35 and METEOR score of 47.02 indicate greater difficulty in managing nuanced meanings and stylistic variability. This performance disparity suggests that Google Translate is particularly effective for formal, structured text but struggles with the complexities of informal or stylistically diverse material.</p>
</section>
<section id="gemini">
<h4>Gemini:<a class="headerlink" href="#gemini" title="Link to this heading">#</a></h4>
<p>Gemini delivers balanced results and demonstrates strong competitiveness, particularly when compared to Google Translate. For biblical texts, it performs well, achieving a BLEU score of 27.22, a ROUGE score of 56.54, and a chrF score of 52.71, indicating high fluency and coherence. Its METEOR score of 54.71 further reflects strong semantic alignment, making it a reliable option for structured content like Bible texts.
When translating non-biblical texts, Gemini faces slightly greater challenges but still performs admirably. Its BLEU score of 19.39 and ROUGE score of 45.84 indicate it handles stylistically diverse content on par with or slightly better than Google Translate in certain aspects. However, its chrF score of 48.37 and METEOR score of 43.64 highlight areas for improvement in capturing lexical richness and nuanced meaning. (Overall, Gemini is a robust tool, excelling in structured contexts and showing promise for handling more complex linguistic tasks with further refinement.)</p>
</section>
<section id="yandex">
<h4>Yandex:<a class="headerlink" href="#yandex" title="Link to this heading">#</a></h4>
<p>Yandex consistently underperforms relative to other tools, showing significant limitations in both precision and coherence. For biblical texts, it achieves a BLEU score of 18.68 and a chrF score of 46.05, suggesting weaker lexical and structural fidelity. Its METEOR score of 47.52 reflects moderate semantic alignment, though it struggles to preserve deeper meanings effectively.
The tool’s challenges become more pronounced with non-biblical texts, where it scores only 10.65 in BLEU and 45.19 in chrF, with a METEOR score of 43.13. These results highlight its difficulty in adapting to diverse stylistic and syntactic demands, making it less suitable for high-quality translations across various content types.</p>
<p>(GPT-4o stands out as the most capable translation tool across all metrics, excelling in fluency, precision and semantic preservation, particularly for highly structured content such as biblical texts. Google Translate delivers competitive results and performs closely to Gemini, especially for structured texts, though both tools show some decline with informal or stylistically diverse material. Gemini matches Google Translate in many respects, providing reliable translations with solid coherence and meaning preservation but it still encounters challenges with more nuanced and descriptive linguistic structures. Yandex, on the other hand, demonstrates significant limitations across all text types, particularly struggling with stylistic and semantic consistency. This comparison underscores the importance of evaluating translation tools comprehensively, as different metrics highlight distinct strengths and weaknesses, particularly when tackling content with varying linguistic complexity and stylistic variability.)</p>
</section>
<section id="id16">
<h4>Metrics:<a class="headerlink" href="#id16" title="Link to this heading">#</a></h4>
<p>When comparing the metrics BLEU, ROUGE, chrF, and METEOR across the models, distinct patterns emerge. For Bible texts, BLEU scores show the widest variability, ranging from 18.68 (Yandex) to 28.74 (GPT). METEOR follows closely with a range from 47.52 (Yandex) to 57.28 (GPT), capturing substantial differences in semantic alignment across models. ROUGE scores are slightly narrower, ranging from 51.92 (Yandex) to 58.27 (GPT), while chrF, which balances precision and recall at the character level, ranges from 46.05 (Yandex) to 54.44 (GPT). This indicates that while BLEU and METEOR are more sensitive to performance differences, ROUGE and chrF provide more stable evaluations, with ROUGE emphasizing recall and chrF focusing on fine-grained lexical and structural alignment. (Overall, the metrics highlight GPT’s superior performance, with Yandex trailing across all measures.)
For non-Bible texts, BLEU again shows the largest variability, with scores spanning from 10.65 (Yandex) to 19.39 (Gemini). ROUGE follows, ranging from 40.74 (Yandex) to 47.94 (GPT). chrF and METEOR, while showing less declines, still demonstrate meaningful gaps, with chrF ranging from 45.19 (Yandex) to 51.31 (GPT) and METEOR from 43.13 (Yandex) to 48.80 (GPT). Thus, while chrF and METEOR are slightly more conservative than BLEU and ROUGE, they still reflect noticeable drops in performance, particularly for more challenging text types.</p>
<p>Across all tools, the metrics reveal a consistent trend: translations of non-biblical texts score lower due to their greater stylistic variability and linguistic complexity. BLEU and ROUGE, which emphasize precision and recall for specific lexical and syntactic features, show sharper declines for such texts, highlighting the difficulty in preserving structural elements. Meanwhile, chrF, which balances character-level precision and recall and METEOR, which incorporates synonym matching and semantic alignment, show relatively smaller variations between text types. This suggests that even when lexical and stylistic accuracy falters, tools maintain a fair degree of meaning preservation. Overall, the trend underscores the challenges posed by diverse linguistic structures and the importance of metric selection for evaluating translation quality in varying contexts.</p>
</section>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>This study highlights the varied performance of MT systems in translating Latin texts, revealing distinct strengths and weaknesses across different text types and evaluation metrics. GPT-4o consistently outperformed other systems, particularly with structured and repetitive content like religious texts, achieving the highest scores across BLEU, ROUGE-L, METEOR, and chrF metrics. In contrast, systems like Yandex struggled significantly, especially with stylistically diverse and complex texts.</p>
<p>Religious texts demonstrated higher overall translation quality compared to neutral texts, likely due to the structured and consistent nature of their source material. Notably, the Douay-Rheims Bible provided the most favorable benchmark, suggesting that older, more literal translations align better with MT capabilities. Here, it is importatnt to note that the best scoring translation does not automatically equate to the most readable translation for modern readers. Additional challenges persisted with poetic and highly emotional passages, as MT systems often failed to replicate nuanced tone and rhetorical complexity.</p>
<p>The findings underscore the importance of text type and evaluation metric selection in MT research. While MT has advanced significantly, translating low-resource languages like Latin still demands refinements, particularly for creative and interpretive content. Future work could explore improved contextual understanding in MT systems and expand training datasets to include more diverse examples, aiding the broader goals of digital humanities and classical studies.</p>
</section>
<section id="bibliography">
<h2>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h2>
<p>{cite:p}``</p>
<div class="docutils container" id="id17">
<div role="list" class="citation-list">
<div class="citation" id="id21" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">1</a><span class="fn-bracket">]</span></span>
<p>Satanjeev Banerjee and Alon Lavie. METEOR: an automatic metric for MT evaluation with improved correlation with human judgments. In Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare Voss, editors, <em>Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</em>, 65–72. Ann Arbor, Michigan, 2005. Association for Computational Linguistics. URL: <a class="reference external" href="https://aclanthology.org/W05-0909">https://aclanthology.org/W05-0909</a>.</p>
</div>
<div class="citation" id="id39" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">2</a><span class="fn-bracket">]</span></span>
<p>King James Bible. <em>KJV</em>. Robert Barker, 1611. Standard Protestant Bible translation.</p>
</div>
<div class="citation" id="id26" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">3</a><span class="fn-bracket">]</span></span>
<p>Sylvio R. Bistafa. Translating scientific latin texts with artificial intelligence: the works of euler and contemporaries. 2024. URL: <a class="reference external" href="https://doi.org/10.48550/arXiv.2307.07520">https://doi.org/10.48550/arXiv.2307.07520</a>, <a class="reference external" href="https://arxiv.org/abs/2307.07520">arXiv:2307.07520</a>, <a class="reference external" href="https://doi.org/10.48550/arXiv.2307.07520">doi:10.48550/arXiv.2307.07520</a>.</p>
</div>
<div class="citation" id="id36" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">4</a><span class="fn-bracket">]</span></span>
<p>Ana Isabel Cespedosa Vázquez and Ruslan Mitkov. Machine translation of literary texts: genres, times and systems. In Raquel Lázaro Gutiérrez, Antonio Pareja, and Ruslan Mitkov, editors, <em>Proceedings of the First Workshop on NLP Tools and Resources for Translation and Interpreting Applications</em>, 48–53. Varna, Bulgaria, September 2023. INCOMA Ltd., Shoumen, Bulgaria. URL: <a class="reference external" href="https://aclanthology.org/2023.nlp4tia-1.7">https://aclanthology.org/2023.nlp4tia-1.7</a>.</p>
</div>
<div class="citation" id="id24" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">5</a><span class="fn-bracket">]</span></span>
<p>Christos Christodoulopoulos and Mark Steedman. A massively parallel corpus: the bible in 100 languages. <em>Language Resources and Evaluation</em>, 49:1–21, 06 2014. URL: <a class="reference external" href="https://doi.org/10.1007/s10579-014-9287-y">https://doi.org/10.1007/s10579-014-9287-y</a>, <a class="reference external" href="https://doi.org/10.1007/s10579-014-9287-y">doi:10.1007/s10579-014-9287-y</a>.</p>
</div>
<div class="citation" id="id37" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">6</a><span class="fn-bracket">]</span></span>
<p>Crossway. <em>ESV</em>. Crossway, 2001. English translation.</p>
</div>
<div class="citation" id="id29" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">7</a><span class="fn-bracket">]</span></span>
<p>Lukas Fischer, Patricia Scheurer, Raphael Schwitter, and Martin Volk. Machine translation of 16th century letters from latin to german. In <em>Proceedings of the Workshop on Computational Methods for Historical Texts</em>. 01 2022.</p>
</div>
<div class="citation" id="id20" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">8</a><span class="fn-bracket">]</span></span>
<p>Chin-Yew Lin and Franz Josef Och. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In <em>Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)</em>, 605–612. Barcelona, Spain, 2004. URL: <a class="reference external" href="https://doi.org/10.3115/1218955.1219032">https://doi.org/10.3115/1218955.1219032</a>, <a class="reference external" href="https://doi.org/10.3115/1218955.1219032">doi:10.3115/1218955.1219032</a>.</p>
</div>
<div class="citation" id="id25" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">9</a><span class="fn-bracket">]</span></span>
<p>Ling Liu, Zach Ryan, and Mans Hulden. The usefulness of bibles in low-resource machine translation. <em>Proceedings of the Workshop on Computational Methods for Endangered Languages</em>, 1:, 01 2021. URL: <a class="reference external" href="https://doi.org/10.33011/computel.v1i.957">https://doi.org/10.33011/computel.v1i.957</a>, <a class="reference external" href="https://doi.org/10.33011/computel.v1i.957">doi:10.33011/computel.v1i.957</a>.</p>
</div>
<div class="citation" id="id23" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">10</a><span class="fn-bracket">]</span></span>
<p>Eva Martínez Garcia and Álvaro García Tejedor. Latin-Spanish neural machine translation: from the Bible to saint augustine. In Rachele Sprugnoli and Marco Passarotti, editors, <em>Proceedings of LT4HALA 2020 - 1st Workshop on Language Technologies for Historical and Ancient Languages</em>, 94–99. Marseille, France, 2020. European Language Resources Association (ELRA). URL: <a class="reference external" href="https://aclanthology.org/2020.lt4hala-1.14">https://aclanthology.org/2020.lt4hala-1.14</a>.</p>
</div>
<div class="citation" id="id19" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">11</a><span class="fn-bracket">]</span></span>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin, editors, <em>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>, 311–318. Philadelphia, Pennsylvania, USA, 2002. Association for Computational Linguistics. URL: <a class="reference external" href="https://doi.org/10.3115/1073083.1073135">https://doi.org/10.3115/1073083.1073135</a>, <a class="reference external" href="https://doi.org/10.3115/1073083.1073135">doi:10.3115/1073083.1073135</a>.</p>
</div>
<div class="citation" id="id22" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">12</a><span class="fn-bracket">]</span></span>
<p>Maja Popović. ChrF: character n-gram F-score for automatic MT evaluation. In Ondřej Bojar, Rajan Chatterjee, Christian Federmann, Barry Haddow, Chris Hokamp, Matthias Huck, Varvara Logacheva, and Pavel Pecina, editors, <em>Proceedings of the Tenth Workshop on Statistical Machine Translation</em>, 392–395. Lisbon, Portugal, 2015. Association for Computational Linguistics. URL: <a class="reference external" href="https://doi.org/10.18653/v1/W15-3049">https://doi.org/10.18653/v1/W15-3049</a>, <a class="reference external" href="https://doi.org/10.18653/v1/W15-3049">doi:10.18653/v1/W15-3049</a>.</p>
</div>
<div class="citation" id="id38" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">13</a><span class="fn-bracket">]</span></span>
<p>The Challoner Revision. <em>DRB</em>. Douay Bible, 1899. Catholic Bible translation.</p>
</div>
<div class="citation" id="id30" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">14</a><span class="fn-bracket">]</span></span>
<p>Frederick Riemenschneider and Anette Frank. Exploring large language models for classical philology. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, <em>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 15181–15199. Toronto, Canada, jul 2023. Association for Computational Linguistics. URL: <a class="reference external" href="https://doi.org/10.18653/v1/2023.acl-long.846">https://doi.org/10.18653/v1/2023.acl-long.846</a>, <a class="reference external" href="https://doi.org/10.18653/v1/2023.acl-long.846">doi:10.18653/v1/2023.acl-long.846</a>.</p>
</div>
<div class="citation" id="id27" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">15</a><span class="fn-bracket">]</span></span>
<p>Martin Volk, Dominic Philipp Fischer, Lukas Fischer, Patricia Scheurer, and Phillip Ströbel. Llm-based machine translation and summarization for latin. In <em>LT4HALA</em>. 2024. URL: <a class="reference external" href="https://doi.org/10.5167/uzh-259369">https://doi.org/10.5167/uzh-259369</a>.</p>
</div>
</div>
</div>
</section>
<section id="appendix">
<h2>Appendix<a class="headerlink" href="#appendix" title="Link to this heading">#</a></h2>
<p><sup>1</sup>	Prompt used for GPT-4o and Gemini during the experiments: “Approach this sentence translation without drawing on any pre-existing knowledge or examples you’ve encountered. Use only the specific sentence structure and vocabulary present, rather than referencing broader linguistic context, cultural knowledge, or past translations of similar phrases.” (followed by Latin excerpt)</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "executablebooks/jupyter-book",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#to-dos">TO DOS</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#previous-work">Previous Work</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#methods-data">Methods &amp; Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data">Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics">Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tools">Tools</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiments">Experiments</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#low-bleu-scores">Low BLEU scores</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#error-proofing">Error proofing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#retranslations">Retranslations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#results-discussion">Results &amp; Discussion</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analysing-scores">Analysing scores</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#de-legibus">De Legibus</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#history-of-the-kings-of-britain">History of the Kings of Britain</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#religious-texts">Religious texts</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#what-happened-to-job">What happened to Job</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-scores">Comparing scores</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#possible-explanations">Possible explanations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-4o">GPT-4o:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#google-translate">Google Translate:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gemini">Gemini:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#yandex">Yandex:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">Metrics:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">Bibliography</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix">Appendix</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Phillip B. Ströbel
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>