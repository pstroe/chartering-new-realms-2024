{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908cb262",
   "metadata": {},
   "source": [
    "# Chapter 1: From Data to Corpus\n",
    "Anouk Menzi, Elizabeth Wagner\n",
    "\n",
    "## Abstract \n",
    "\n",
    "#We will write this in the end\n",
    "\n",
    "## Introduction\n",
    "Scholars spend many hours on preprocessing raw data into structured collections that suit their needs {cite:p}`chen_2023`. This process is time and resource intensive, especially when dealing with natural language. Considering the current trend in the digital humanities away from big data towards massive data, the questions of making data Findable, Accessible, Interoperable, and Reusable {cite:p}`wilkinson_2016` have become a focus area of constructing data collections {cite:p}`ide_2003, könig_2021`. An attempt at making data accessible, interoprable, and reusable can be found in the Text Encoding Initiative {cite:p}`burnard_2013`. In this chapter, we propose an approach to the construction of structured data collections with the assistance of Large Language Models, LLMs, to reduce the amount of human labour invested in preprocessing, using the ParlaMint schema, an interoperable TEI XML schema for transcripts of parliamentary proceedings. This approach could be promising in cases where there is much raw data but it is not accessible in any coherent structure, as is the case for the variations of South African English {cite:p}`barnard_2014, jeffery_2003, pienaar_2011`. One such source of raw data are the South African parliamentary proceedings. The South African Parliament supplies transcripts of its parliamentary proceedings online, and whilst attempts have been made to structure this data into the ParliMint schema, the attempt has been labour-intensive and done only on a small scale {cite:p}`ogrodniczuk_2024`. This chapter shall thus attempt to format the parliamentary proceedings into an interoperable XML schema, by employing the aid of different LLMs without the use of industrial strength hardware. \n",
    "\n",
    "## Relevant Literature \n",
    ">Parliamentary proceedings (PP) are a rich source of data used by e.g. scholars in historiography, sociology, political science, linguistics, economics and economic history. As opposed to sources of most other language corpora, PP are not subject to copyright or personal privacy protections, and are typically available online thus making them ideal for compilation into corpora and open distribution. For these reasons many countries have already produced PP corpora, but each typically in their own encoding, thus limiting their comparability and utilisation in a multilingual setting.\n",
    ">{cite:p}`erjavec_2019`\n",
    "\n",
    "ParlaMint was suggested as an interoperable, adaptive framework of formatting non-standardised parliamentary proceedings {cite:p}`erjavec_2023`. A first attempt at including the South African Parliamentary Proceedings has been made by Ogrodniczuk {cite:t}`ogrodniczuk_2024`, highlighting again the difficulties of formatting an unstructed document into the ParlaMint scheme {cite:p}`erjavec_2023`. The experiment was conducted on a singular session with a rule based approach {cite:p}`ogrodniczuk_2024`. This proved difficult as all quotations had to be marked manually, thus making it a labour intensive process when considering the 25 years of proceedings available for download on [the website of the South African Parliament](https://www.parliament.gov.za/hansard) {cite:p}`ogrodniczuk_2024, truan_2021`. \n",
    "\n",
    "The primary attractivity of harnessing LLMs lies in their capability to process Natural Language inputs and their generalized applicability to unknown tasks {cite:p}`zhang_jellyfish_2024, narayan_2022`. In this they are more flexible than specialized tools. Their flexibility is especially appreciated when it comes to the robustness of processing as, because they are not rule based, they are able to adapt to unforseen circumstances {cite:p}`zhang_jellyfish_2024`. In this ability they have found a wide application ground within the field of linguistics such as in Zappavigna where ChatGPT was tasked with evaluating noisy social media data {cite:p}`zappavigna_2023` or in the use of generative LLMs for corpus analysis {cite:p}`curry_2024`. As to the knowledge of the authors of this chapter, no attempts at harnessing LLMs for parliamentary corpus building specifically have been attempted. However, in the wider field of data curation and formatting, the capabilities of LLMs are utilized, for example in summarising healthcare data from semi-structured forms into possible schematas of illnesses {cite:p}`letinier_2021` or in processing natural language for the biomedical field into a reusable format {cite:p}`beck_2022`. These adaptations of LLMs are highly specialized to their respective tasks and have thus lost much of their generality which is so desired by data scientists in their quest for a one-stop-shop solution for data wrangling {cite:p}`chen_2023`. A further issue of these specialized tasks lie in the idea that LLMs also mark faulty data, or rather correct these errors, such as in customer databases {cite:p}`pookandy_2022`. A behaviour which, in the field of linguistics, is at its best irrelevant but rather more likely renders the data worthless as it would alter the transcripts. \n",
    "\n",
    "In light of these added difficulties when it comes to language data, where the language itself is of importance, research has largely been based on developping a tool that, in its foundations, is based on LLMs but that also includes code with a rule-based approach to wrangle data {cite:p}`chen_2023, arora_2023`. Evaporate is capable of transforming various, semistructured inputs into a table output, its LLM components are based on using cloud solutions to run LLMs {cite:p}`arora_2023`. SEED works on a similar basis, though its output format can be customised [^footnote1]. A model specifically fine-tuned for dataprocessing is the jellyfish family [^footnote4], as proposed by Zhang et al. which is based on the smaller models of the Llama 3 family {cite:t}`zhang_jellyfish_2024`. These tools display a remarkable adaptivity to new tasks, especially with few-shot prompting, where a model or tool is supplied with several examples before it is set to the task {cite:p}`chen_2023`. However, this comes at a greater computational cost than when employing non-LLM or rule-based tools {cite:p}`arora_2023`. Considering this trade-off between robustness, human-costliness and energy-efficiency, any approach must be carefully calculated and balanced.\n",
    "\n",
    "[^footnote4]: The Jellyfish model requires a GPU with more than 15 GB of memory, we neither have a device available with such a GPU, nor does Google Colab support such memory use on their free plan, thus we are unable to test it. \n",
    "\n",
    "[^footnote1]: SEED is currently undergoing restructuring and thus cannot be used. It seems to be a promising project for further investigation into processing raw data via LLM, see [SEED repository](https://anonymous.4open.science/r/SEED/paper.pdf). \n",
    "\n",
    "\n",
    "## Data and Methods\n",
    "\n",
    "### Data\n",
    "The data was extracted from the official website for the Hansard of the Parliament of the Republic of South Africa. It constitutes the transcripts of the mini plenary sessions of the National Assembly, the National Council of Provinces, the National Assembly, and any joint sessions. The National Assembly is formed by 400 members from the various South African political parties. The National Assembly is elected by the voting population of South Africa. The National Council of Provinces, NCoP, is formed with 90 provincial delegates which translates to 10 delegates for each province. It is thus composed regardless of population distribution. The NCoP is chosen by the provincial governments/legislatures. \n",
    "\n",
    "The reports are majoritatively held in English, though when a speaker chooses to use another official language, it is transcribed, but no English translation is given. \"Hansard is a substantially verbatim report - with repetitions and redundancies omitted and obvious mistakes corrected - of parliamentary proceedings\" (see [Website of the South African parliament](https://www.parliament.gov.za/hansard?sorts[language]=-1&page=5&offset=40)). {cite:t}`kotze_2020` remark that it is and remains unclear what substantially verbatim conotes in the sense of corrections towards an overstandardisation {cite:p}`kotze_2020, hibbert_2016, hibbert_2003`.[^footnote2] \n",
    "\n",
    "[^footnote2]: For a more detailed discussion of editing practices in the South African Hansard view {cite:p}`hibbert_2016, hibbert_2003`.\n",
    "\n",
    "The decision was made to process all reports of 2020 from the National Assembly, henceforth abbreviated as NA. The data thus excludes any other years, all joint sittings, all meetings of the NCoP and all mini plenary sessions. The decision to look at the NA was made to maximise the possible speaker count and because it holds the most sessions of all parliamentray chambers. In total 51 sessions were held in 2020.\n",
    "\n",
    "#### ParlaMint \n",
    "\n",
    "To ensure that the data in this corpus remains both human- and machine-readable while adhering to widely accepted standards, it was decided to encode the transcriptions in XML using the ParlaMint schema {cite:p}`erjavec_2022`, a customisation of the Parla-Clarina schema, which itself is based on the Text Encoding Initiative (TEI) guidelines {cite:p}`tei_consortium_guidelines`. This approach allows the corpus to maintain a consistent structure while also providing a way to encode the specific nuances of parliamentary discourse.\n",
    "\n",
    "The decision to adopt TEI, and specifically the ParlaMint schema, was guided by several considerations, specifically the goal of adhering to the FAIR principles. TEI's flexibility allows for the encoding of a diverse range of textual features, including but not limited to metadata about speakers. The ParlaMint schema, as a specialized extension of TEI, was designed to standardise the encoding of parliamentary data across various languages and regions {cite:p}`ogrodniczuk_2024` and allows for the encoding of a wide variety of metadata while following a strict structure to enable maximal interoperability {cite:p}`erjavec_2022`. In addition to the strict encoding guidelines for data and metadata provided by the ParlaMint schema, it also allows for meticulous documentation of the process to enable reusability for future research using this data. Overall, the ParlaMint was designed to adhere to the FAIR principles as closely as possible {cite:p}`erjavec_2022`. \n",
    "\n",
    "The concluded ParlaMint I project entailed the encoding of corpora containing transcriptions of the sessions of 17 European national parliaments, resulting in a collection of half a billion words {cite:p}`erjavec_2022_TEI`. Each corpus was prepared in two versions, one being the filly marked-up corpus with speeches in plain text, the other being identical to the first but with added linguistic annotation {cite:p}`erjavec_2022`. Adhering to the ParlaMint schema while encoding the South African Hansard papers would allow this corpus to seamlessly integrate with the ParlaMint I project.\n",
    "\n",
    "A ParlaMint corpus is contained within a teiCorpus element, which includes a teiHeader for overarching metadata and multiple TEI elements, each representing a distinct component of the corpus, typically corresponding to a single day's transcripts. This corpus root encodes information such as the title and language of the corresponding transcripts, the number of speakers and speeches contained within them, and the time the transcriptions span. The corpus root file also contains information about the license the transcripts are published under and the place online where they can be downloaded. \n",
    "To manage large corpora more easily, ParlaMint uses the XInclude mechanism. In this setup, the main corpus file, called the corpus root, references individual files, the corpus component files. Thus, each day's transcripts are stored in a separate file, with the overarching structure being represented in the corpus root. This approach facilitates scalability and makes the corpus more easy to maintain. {cite:p}`ParlaMint_2024`\n",
    "\n",
    "\n",
    "#### Training Data\n",
    "The preprocessing of the transcriptions involved several steps to ensure consistency and compliance with the ParlaMint schema. This included turning the PDF-documents downloaded from the South African parliament's website {cite:p}`hansardSA_2020` into text files. The content of these txt-files was not edited at all, save for occasional spelling errors within headers and subtitles. These txt files were then converted into xml files following the ParlaMint schema.\n",
    "\n",
    "A ParlaMint corpus is contained within a teiCorpus element, which includes a teiHeader for overarching metadata and multiple TEI elements, each representing a distinct component of the corpus, typically corresponding to a single day's transcripts. To manage large corpora more easily, ParlaMint uses the XInclude mechanism. In this setup, the main corpus file, called the corpus root, references individual files, the corpus component files. Thus, each day's transcripts are stored in a separate file, with the overarching structure being represented in the corpus root. This approach facilitates scalability and makes the corpus easier to maintain. {cite:p}`ParlaMint_2024`\n",
    "\n",
    "Example structure of the corpus root file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed35318",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1976756135.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    <teiCorpus xmlns=\"http://www.tei-c.org/ns/1.0\">\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<teiCorpus xmlns=\"http://www.tei-c.org/ns/1.0\">\n",
    " <teiHeader>...</teiHeader>\n",
    " <TEI>...</TEI>\n",
    " <TEI>...</TEI>\n",
    " ...\n",
    "</teiCorpus>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6885f4ba",
   "metadata": {},
   "source": [
    "A corpus component file consists of one TEI element. The teiHeader element within the TEI element contains metadata specific to the component, such as details about the parliamentary session, date, and participants. The text element holds the actual transcription of the parliamentary proceedings. This transcription is organized into divisions, which may represent different sessions or segments, and further into individual utterances. Each utterance is typically associated with a speaker, who is identified through attributes that link to the metadata in the teiHeader. {cite:p}`ParlaMint_2024`\n",
    "\n",
    "Example structure of a corpus component file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1899c576",
   "metadata": {},
   "outputs": [],
   "source": [
    "<TEI xmlns=\"http://www.tei-c.org/ns/1.0\">\n",
    " <teiHeader>...</teiHeader>\n",
    " <text>\n",
    "    <body>\n",
    "      <div type=\"commentSection\">...</div>\n",
    "      <div type=\"debateSection\">...</div>\n",
    "      <div type=\"debateSection\">...</div>\n",
    "    ...\n",
    "    </body>\n",
    " </text>\n",
    "</TEI>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256adba1",
   "metadata": {},
   "source": [
    "The ParlaMint schema also allows for the encoding of extensive metadata around speakers and organisations, all of which are stored in separate files, which are referenced when necessary.[^footnote3]\n",
    "\n",
    "[^footnote3]: For more information about the structure of the ParlaMint schema visit their [GitHub repository](https://github.com/clarin-eric/ParlaMint).\n",
    "\n",
    "#### Pre-Processing\n",
    "The preprocessing of the transcriptions involved several steps to ensure consistency and compliance with the ParlaMint schema. This included turning the PDF-documents downloaded from the South African parliament's website {cite:p}`hansardSA_2020` into text files. The content of these txt-files was not edited at all, save for occassional spelling errors within headers and subtitles. These txt files were then converted into xml files following the ParlaMint schema.\n",
    "\n",
    "The first step was to prepare the corpus root file which contains the metadata about the South African Hansard papers. In a next step, a sample xml file was prepared. For this purpose, the txt file containing the transcripts of the session of the National Assembly held on 25.02.2020 was selected. A shortened version of around 17 pages was created, containing around three speeches and the introductory conversation of that session. This short txt was then converted into an xml file, adhering to the ParlaMint schema. It was judged that these 17 pages contained enough variation in speakers and discourse as to provide a wide array of different xml elements and attributes within the xml file to serve as example for the prompts served to the LLMs. \n",
    "\n",
    "Example snippet from the converted xml file, showing part of the teiHeader element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fc5b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "<TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"HansardSA_NA_2020\" xml:lang=\"en\">\n",
    "    <teiHeader>\n",
    "     <fileDesc>\n",
    "       <titleStmt>\n",
    "            <title type=\"main\" xml:lang=\"en\">South African Hansard papers</title>\n",
    "            <title type=\"sub\">Minutes of the National Assembly of South Africa</title>\n",
    "            <meeting n=\"1\" corresp=\"#DZ\" </meeting> \n",
    "       </titleStmt>\n",
    "       ...\n",
    "      <profileDesc>\n",
    "        <settingDesc>\n",
    "            <setting> \n",
    "                <name type=\"place\">Houses of Parliament</name>\n",
    "                <name type=\"city\">Cape Town</name>\n",
    "                <name type=\"country\" key=\"ZA\">South Africa</name>\n",
    "                <date when=\"2020-02-25\">25.02.2020</>\n",
    "            </setting>\n",
    "        </settingDesc>\n",
    "     </profileDesc>\n",
    "</TEI>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcca2a8f",
   "metadata": {},
   "source": [
    "As described above, speaker metadata is stored in a separate file, which is referenced as necessary. Specifically, a unique ID is defined for each speaker within this speaker metadata file, which is used in the component file to identify the speaker and link them to the metadata file. To facilitate the prompt engineering at first, the decision was made to forgo the speaker metadata context to ensure a clean run of the LLM. In a next step, the speaker element could be called to insert the relevant ID.\n",
    "\n",
    "Example snippet from the converted xml file, showing part of text element containing the speeches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac5f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "<text>\n",
    "  <body>\n",
    "    <div type=\"debateSection\">\n",
    "      <note type=\"time\">The House met at <time when=\"2020-02-25T014:00:00\">14:00</time>.</note>\n",
    "      <note type=\"narrative\">House Chairperson Ms M G Boroto took the Chair and requested members to observe a moment of silence for prayer or meditation.</note>\n",
    "      <note type=\"speaker\">The HOUSE CHAIRPERSON (Ms M G Boroto):</note>\n",
    "      <u xml:id=\"25-02-2020_u1\" who=\"#houseChairperson\">\n",
    "        <seg xml:lang=\"en\">Hon members, I would like to remind you that on 4 December 2019 the House adopted the Rules Committee report which introduced a number of\n",
    "            amendments to our rules. Some of the amendments pertain to thesequence of proceedings and Members’ Statements. To facilitate sufficient opportunity for Ministers’ Responses to Members’ Statements, the sequence of proceedings has been amended so that Members’ Statements are now at the start of the proceedings on days that they are scheduled by the programming committee.\n",
    "            </seg>\n",
    "        <seg xml:lang=\"en\">The Rules Committee further agreed that the number of Ministers’ Responses be increased from six to seven and that time allowed for ministers’ Responses be increased from two minutes to three minutes. With that background, I will now take the first item on the Order Paper which is Members’ Statements. Does any member of the ANC wish to make a statement?\n",
    "        </seg>\n",
    "      </u>\n",
    "      <note type=\"speaker\">The CHIEF WHIP OF THE OPPOSITION:</note>\n",
    "      <u xml:id=\"25-02-2020_u2\" who=\"#ChiefWhipOfOpposition\"> \n",
    "        <seg xml:lang=\"en\">\n",
    "            Sorry Chair, on a point of order.\n",
    "        </seg>\n",
    "      </u>\n",
    "      <note type=\"speaker\">The HOUSE CHAIRPERSON (Ms M G Boroto):</note>\n",
    "      <u xml:id=\"25-02-2020_u3\" who=\"#houseChairperson\">\n",
    "        <seg xml:lang=\"en\">\n",
    "            Please take your seat. Yes, what’s your point of order?\n",
    "        </seg>\n",
    "      </u>\n",
    "      ...\n",
    "    </div>\n",
    "  </body>\n",
    "</text>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec49107",
   "metadata": {},
   "source": [
    "### Method \n",
    "This chapter uses the newest releases of the Llama 3 model family, Gemini 1.5 Flash and GPT-4o. \n",
    "\n",
    "Llama makes multiple sets of pretrained models with different quantities of parameters available, thus, offering the possibility of maximising minimal parameter count to maximum quality output. A further issue in harnessing LLMs for data formatting lies in the costliness of the training and running of such models. Whilst there is an effort to optimize models, it is still not possible to train a LLM locally on a standard laptop {cite:p}`zhang_jellyfish_2024`. However, it is possible to run some pretrained models locally, provided that their parameter count is relatively small, and adapt them to a specific task via few-shot prompting. In this context Llama offers small-scale options with their development of the general models Llama 3.2 1B, 3B and 70B, where especially the 1B and the 3B parameter models are runnable on mobile or edge devices {cite:p}`dubey_2024`. The smaller models are \"best-in-class, outperforming alternative models with similar numbers of parameters\" {cite:p}`dubey_2024`. The model family was pretrained on 15T tokens which marks a large increase from Llama 2 with 1.8T tokens {cite:p}`dubey_2024`. \n",
    "\n",
    "```{figure} chapter1_ZA-content/images/llama_3.jpg\n",
    "---from IPython.display import display, Javascript\n",
    "import pathlib\n",
    "\n",
    "# Function to create the pop-up window using JavaScript\n",
    "def show_xml_popup(file_path):\n",
    "    # Ensure the file exists\n",
    "    if not pathlib.Path(file_path).exists():\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist!\")\n",
    "    \n",
    "    # Read the XML file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        xml_content = file.read()\n",
    "\n",
    "    # Escape the XML content for safe JavaScript embedding\n",
    "    escaped_content = xml_content.replace('<', '&lt;').replace('>', '&gt;')\n",
    "\n",
    "    # JavaScript to open a new window and display the XML\n",
    "    js_code = f\"\"\"\n",
    "    var newWindow = window.open('', '_blank', 'width=800,height=600,scrollbars=yes');\n",
    "    newWindow.document.open();\n",
    "    newWindow.document.write('<pre>{escaped_content}</pre>');\n",
    "    newWindow.document.close();\n",
    "    \"\"\"\n",
    "    # Display the JavaScript\n",
    "    display(Javascript(js_code))\n",
    "\n",
    "# Example usage: call this function with the path to your XML file\n",
    "# Replace 'example.xml' with the path to your actual XML file\n",
    "show_xml_popup('example.xml')\n",
    "\n",
    "width: 650px\n",
    "align: center\n",
    "name: fig-llama_3\n",
    "---\n",
    "Llama 3 herd with parameters {cite:p}`dubey_2024`\n",
    "```\n",
    "\n",
    "Gemini 1.5 Flash constitutes the attempt at constructing a lightweight model with GPT-4 capabilities but a longer context window {cite:p}`gemini_2024`. It promises accuracy across a context window of 10 million token, whilst being relatively efficient and more efficient to serve then the Gemini 1.0 models {cite:p}`gemini_2024`. \n",
    "\n",
    "ADD GPT-4O TO THIS.\n",
    "\n",
    "**Models Used**\n",
    "- Llama 3.2 1B\n",
    "- Llama 3.2 3B\n",
    "- Llama 3 8B\n",
    "- Gemini 1.5 Flash\n",
    "- GPT-4o\n",
    "\n",
    "Because of the various implementations of the LLMs, with the Llama herd being locally run, and Gemini and GPT-4o being run through their online interface, different\n",
    "approaches had to be taken. Overall, the ParlaMint schema was simplified as to compartementalize the different elements of the structure. Erjavec's most time consuming task when annotating a file by hand, was the marking of metalinguistic commentary, respectively, speaker and metalinguistic commentary differenciation {cite:t}`erjavec_2023`. The input prompt was always structured by giving an example of the raw data, an example of the structured, corresponding xml section and instructions {cite:p}`sahoo_2024`. Depending on whether the LLM was called via API or it's online interface, it was either guided onwards through the repetition of the instruction, or through a guiding conversation. For a detailed description of the approaches, please view the [Experiments and Results](#Experiments and Results) section.\n",
    "\n",
    "#### Evaluation\n",
    "To evaluate the work of the LLMs automatically, a twofold approach was selected, where both the structure, [Evaluation XML Schema](##### Evaluation XML Schema) and the content, [Evaluation Content](##### Evaluation Content) of the processed file is assessed. \n",
    "\n",
    "##### Evaluation XML Schema\n",
    "To validate the XML schema of the files output by the LLMs, the RelaxNG {cite:p}`clark_2001` file format was chosen. A RelaxNG file is itself an XML file, which can be used to check and validate the structure of an XML file {cite:p}`van-der-vlist_2003`. This format was selected as there already exists an official RelaxNG file created by the ParlaMint team [^footnote9]. Due to the simplified nature of the XML schema followed within this paper, the ParlaMint RelaxNG file was adapted and simplified to better suit this project's needs.\n",
    "\n",
    "Using a short Python script, the adapted RelaxNG file was used to evaluate and validate all XML files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a8177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from collections import Counter\n",
    "\n",
    "def validate_xml(relaxng_file, xml_file):\n",
    "    \"\"\"\n",
    "    Validates an XML file against a RelaxNG schema and prints detailed error messages,\n",
    "    along with a total count of errors and a count of each error type.\n",
    "\n",
    "    :param relaxng_file: Path to the RelaxNG schema file.\n",
    "    :param xml_file: Path to the XML file to be validated.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(relaxng_file, 'r', encoding='utf-8') as rng_file:\n",
    "            relaxng_doc = etree.parse(rng_file)\n",
    "            relaxng = etree.RelaxNG(relaxng_doc)\n",
    "        \n",
    "        with open(xml_file, 'r', encoding='utf-8') as xml_file_obj:\n",
    "            xml_doc = etree.parse(xml_file_obj)\n",
    "        \n",
    "        if relaxng.validate(xml_doc):\n",
    "            print(f\"The XML file '{xml_file}' is valid according to the RelaxNG schema.\")\n",
    "        else:\n",
    "            print(f\"The XML file '{xml_file}' is NOT valid according to the RelaxNG schema.\\n\")\n",
    "            print(\"Validation errors:\")\n",
    "\n",
    "            error_count = 0\n",
    "            error_type_counter = Counter()\n",
    "\n",
    "            # Process and print each error\n",
    "            for error in relaxng.error_log:\n",
    "                error_count += 1\n",
    "                error_type_counter[error.type_name] += 1\n",
    "                print(f\"Line {error.line}, Column {error.column}: {error.message}\")\n",
    "                print(f\"  Domain: {error.domain_name}, Type: {error.type_name}\\n\")\n",
    "\n",
    "            # Print total error summary\n",
    "            print(\"Summary of Validation Errors:\")\n",
    "            print(f\"Total Errors: {error_count}\")\n",
    "            for error_type, count in error_type_counter.items():\n",
    "                print(f\"  {error_type}: {count} occurrences\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    relaxng_file = \"Adapted_ParlaMint.rng\" \n",
    "    xml_file = \"uh_25.02_short.xml\"    \n",
    "\n",
    "    validate_xml(relaxng_file, xml_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1258ec72",
   "metadata": {},
   "source": [
    "If the XML file is valid, the output consists of a single line: \"The XML file '{xml_file}' is valid according to the RelaxNG schema.\". If the XML file is not valid, the script outputs a list of all errors with their corresponding line numbers and error types. Additionally, it outputs a total sum of errors and a sum of each type of error, which facilitates the comparison across different evaluations.\n",
    "\n",
    "[^footnote9]: This RelaxNG file can be accessed on the ParlaMint project's GitHub repository, in the [Schema](https://github.com/clarin-eric/ParlaMint/tree/main/Schema) folder.\n",
    "\n",
    "##### Evaluation Content\n",
    "To evaluate the content of the output of the LLMs tested, a percentage scale was chosen. To avoid looping through each file, the decision was made to base the validation script on a random sampler of sentences. It samples a specified number of sentences from the processed XML file and compares them to the original txt file on a token basis. \n",
    "\n",
    "```{attention} This code needs to be configured for the xml tag that denotes where the text content of the file is stored. The ParlaMint scheme specifies this with the *seg* tag, though it is customisbale, to allow for output from LLMs which configure this tag wrongly, to allow for a consistent check of content. The code below is configured for the gold standard.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9313a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Path to the XML and txt file\n",
    "xml_file_path = 'test_objects/gold_standard.xml'  \n",
    "txt_file_path = 'test_objects/gold_standard.txt'  \n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse(xml_file_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Define the namespace for TEI XML\n",
    "namespace = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "\"\"\"initiates the segment that the llm assigns to the text elements. Can be variable. \n",
    "Correct tag: seg \n",
    "Common other tags: note, \n",
    "\"\"\"\n",
    "text_seg = \"seg\"\n",
    "\n",
    "#configures the sampler size. \n",
    "sampler = 10\n",
    "\n",
    "# Find all the text elements within the XML\n",
    "segments = root.findall(f'.//tei:{text_seg}', namespace)\n",
    "\n",
    "# List to store all sentences\n",
    "all_sentences = []\n",
    "\n",
    "# Function to split text into sentences\n",
    "def split_into_sentences(text):\n",
    "    # sentence splitting \n",
    "    return re.split(r'(?<=\\w[.!?]) +', text)\n",
    "\n",
    "# Loop through each text element, split text into sentences, and add to the list\n",
    "for seg in segments:\n",
    "    if seg.text:\n",
    "        sentences = split_into_sentences(seg.text.strip())\n",
    "        all_sentences.extend(sentences)\n",
    "\n",
    "# Randomly pick sentences\n",
    "random_sentences = random.sample(all_sentences, sampler) if len(all_sentences) >= 10 else all_sentences\n",
    "\n",
    "# Remove newline characters and extra spaces from the random sentences\n",
    "random_sentences = [re.sub(r'\\s+', ' ', sentence.replace('\\n', ' ').strip()) for sentence in random_sentences]\n",
    "\n",
    "\n",
    "\n",
    "# Open the text file and read its content\n",
    "with open(txt_file_path, 'r', encoding='utf-8') as file:\n",
    "    txt_content = file.read()\n",
    "\n",
    "# Remove newline characters and extra spaces from the txt file content\n",
    "txt_content = re.sub(r'\\s+', ' ', txt_content.replace('\\n', ' ').strip())\n",
    "\n",
    "# calculates how much of the sentence is found\n",
    "def calculate_match_percentage(sentence, txt_content):\n",
    "    # Find the longest substring match in the text content\n",
    "    match = re.search(re.escape(sentence), txt_content)\n",
    "    if match:\n",
    "        match_len = len(match.group(0))  # Length of the match\n",
    "        sentence_len = len(sentence)  # Length of the original sentence\n",
    "        return (match_len / sentence_len) * 100  # Percentage of the sentence found\n",
    "    return 0  # No match found\n",
    "\n",
    "# Check how many of the sentences are present in the TXT file\n",
    "print(\"\\nChecking how much of the sentences are present in the TXT file:\")\n",
    "for sentence in random_sentences:\n",
    "    match_percentage = calculate_match_percentage(sentence, txt_content)\n",
    "    if match_percentage > 0:\n",
    "        print(f\"Found: {match_percentage:.2f}% of the sentence: {sentence}\")\n",
    "    else:\n",
    "        print(f\"Not found: {sentence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775b8298",
   "metadata": {},
   "source": [
    "## Experiments and Results\n",
    "In a primary approach, the attempt was made to guide a locally run, smaller, LLM, Llama, via prompt engineering with a standard prompting approach but enriched with an example {cite:p}`vijayan_2023, zhang_2023, naveed_2023`. This approach was chosen to assess whether a smaller, and thus less costly, LLM could fulfill the task requirements. Furthermore, two larger LLMs, Gemini and GPT-4o, were tested through their online chat interface, to assess whether they produce a different, possibly a more stable output. \n",
    "\n",
    "### LLama Herd \n",
    "The prompt for the Llama herd is comprised of a shortened version of the input txt file and the corresponding xml file in the ParlaMint schema. This decision to utilize a standard prompting approach was made to accomodate the context windows of the models tested. To work with the context window given, the files had to be chunked. The decision was made not to enlargen the context windows as larger context windows generally amplify hallucinations, which in the case of data formatting would be detrimental.\n",
    "\n",
    "Ollama was chosen as basesoftware as it offers the smaller Llama 3.2 models in downloadable form. Furthermore, Ollama is linked to langchain to customise its prompting abilities, as Ollama offers limited customization options, though this is subject to changes [^footnote]. Langchain offers flexibility with regards to customisation {cite:p}`martra_2024`. Thus, the temperature of the model was arranged between 0-0.3 to minimize creativity within the responses. The setting of the model was varied to test whether different base settings would alter the responses given by the model. \n",
    "\n",
    "[^footnote]: For the newest updates and developments concerning Ollama consult their [blog](https://ollama.com/blog).\n",
    "\n",
    "In the first attempt the model was given a prompt of the structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9863415",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_xml = f'<note type=\"speaker\">The CHIEF WHIP OF THE MAJORITY PARTY:</note> <u xml:id=\"25-02-2020_u16\" who=\"#ChiefWhipOfMajorityParty\"> <seg xml:lang=\"en\">Thank you very much, House Chair. As indicated on the Order Paper we shall proceed.</seg>'\n",
    "example_txt = f'The CHIEF WHIP OF THE MAJORITY PARTY: Thank you very much, House Chair. As indicated on the Order Paper we shall proceed.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bd0623",
   "metadata": {},
   "source": [
    "A code example is given below with Llama 3.2 configured: \n",
    "\n",
    "```{tip}\n",
    "Make sure that you close Ollama before serving it on the command line, otherwise it will not work.\n",
    "\n",
    "To exit Ollama in the command line press ctrl + c.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13b06f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "folder_path = r'test_objects'\n",
    "\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "#specifies the model\n",
    "model = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "#chunks the files into manageable sizes for the LLMs context windows.\n",
    "def chunk_text(text, chunk_size=5000):\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        yield ' '.join(words[i:i + chunk_size])\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    #checks the input file's format\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                print(f'Processing file: {filename}')\n",
    "                content = file.read()\n",
    "                document_list = []\n",
    "                i = 0 #counters for debugging\n",
    "                for chunk in chunk_text(content, chunk_size=1000):\n",
    "                    try:\n",
    "                        #actual prompt given to LLM\n",
    "                        question = f'If given raw text: {example_txt} with the end goal: {example_xml}, can you adapt this: {chunk} into the same xml format?'\n",
    "                        response = model(prompt.format(question=question))\n",
    "                        document_list.append(response)\n",
    "                        i += 1\n",
    "                        if i == 4:  # Limit iterations for testing\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing chunk: {e}\")\n",
    "                output_file = os.path.join(folder_path, f\"{os.path.splitext(filename)[0]}.xml\")\n",
    "                with open(output_file, 'w', encoding='utf-8') as output:\n",
    "                    output.write('\\n'.join(document_list))\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea7a287",
   "metadata": {},
   "source": [
    "```{attention} This code will fail unless langchain and Ollama are installed!\n",
    "```\n",
    "The input prompt was varied, and the global setting for the Llama family adapted. View the appendix for the specific settings and their corresponding results. \n",
    "\n",
    "ADAPT THE STYLE APPROPRIATELY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583dba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "excel_file = \"ZA-content/comments.xlsx\"\n",
    "sn = \"13.12\"\n",
    "\n",
    "comments = pd.read_excel(excel_file, sheet_name=sn)\n",
    "\n",
    "print(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29008de",
   "metadata": {},
   "source": [
    "### Gemini 1.5 Flash\n",
    "To assess whether a larger LLM gives a better output, Gemini 1.5 Flash was tested in its online chat interface. Gemini's primary attractiveness for this task lies in its long context windows of up to 10 million token and its superior efficiency over the GPT models {cite:p}`gemini_2024`. As the online chat interface does not allow file input, the prompt was structured to contain both an example xml and an example txt, as well as a chunk of a file to be processed. See below for an example of the structure. The file was chunked into 4000 word segments to respect the input maximum of 5108 tokens of each call for Gemini 1.5 Flash. Every conversation was held thrice to assess the answer scheme of the LLM and whether it's answers are similar in content. \n",
    "\n",
    "```{example} PROMPT:[Given: [The CHIEF WHIP OF THE MAJORITY PARTY: Thank you very much, House Chair. As indicated on the Order Paper we shall proceed.] with the goal [<note type=\"speaker\">The CHIEF WHIP OF THE MAJORITY PARTY:</note> <who=\"#ChiefWhipOfMajorityParty\"> <seg xml:lang=\"en\">Thank you very much, House Chair. As indicated on the Order Paper we shall proceed.</seg>] format the following text into the same xml format. Format all of the text.\n",
    "[UNREVISED HANSARD\n",
    "NATIONAL ASSEMBLY\n",
    "TUESDAY, 25 FEBRUARY 2020\n",
    "Page: 1\n",
    "TUESDAY, 25 FEBRUARY 2020\n",
    "____\n",
    "PROCEEDINGS OF THE NATIONAL ASSEMBLY\n",
    "____\n",
    "The House met at 14:00.\n",
    "House Chairperson Ms M G Boroto took the Chair and requested\n",
    "members to observe a moment of silence for prayer or\n",
    "meditation.\n",
    "The HOUSE CHAIRPERSON (Ms M G Boroto): Hon members, I would\n",
    "like to remind you that on 4 December 2019 the House adopted\n",
    "the Rules Committee report which introduced a number of\n",
    "amendments to our rules. Some of the amendments pertain to the\n",
    "sequence of proceedings and Members’ Statements. To facilitate\n",
    "sufficient opportunity for Ministers’ Responses to Members’\n",
    "Statements, the sequence of proceedings has been amended so\n",
    "that Members’ Statements are now at the start of the\n",
    "proceedings on days that they are scheduled by the programming\n",
    "committee.\n",
    "UNREVISED HANSARD\n",
    "NATIONAL ASSEMBLY\n",
    "TUESDAY, 25 FEBRUARY 2020\n",
    "Page: 2\n",
    "```\n",
    "\n",
    "It's output however, was unusable, as it refused to attempt the task and gave answers such as: \n",
    "\n",
    "- Sorry, I can't help you with that. (test_8, 27.12.2024)\n",
    "- I can't help with responses on elections and political figures right now. While I would never deliberately share something that's inaccurate, I can make mistakes. So, while I work on improving, you can try Google Search. (test_3, 27.12.2024)\n",
    "\n",
    "The output can thus not be evaluated with the prepared scripts. \n",
    "\n",
    "### GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba63a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Javascript\n",
    "import pathlib\n",
    "\n",
    "# Function to create the pop-up window using JavaScript\n",
    "def show_xml_popup(file_path):\n",
    "    # Ensure the file exists\n",
    "    if not pathlib.Path(file_path).exists():\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist!\")\n",
    "    \n",
    "    # Read the XML file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        xml_content = file.read()\n",
    "\n",
    "    # Escape the XML content for safe JavaScript embedding\n",
    "    escaped_content = xml_content.replace('<', '&lt;').replace('>', '&gt;')\n",
    "\n",
    "    # JavaScript to open a new window and display the XML\n",
    "    js_code = f\"\"\"\n",
    "    var newWindow = window.open('', '_blank', 'width=800,height=600,scrollbars=yes');\n",
    "    newWindow.document.open();\n",
    "    newWindow.document.write('<pre>{escaped_content}</pre>');\n",
    "    newWindow.document.close();\n",
    "    \"\"\"\n",
    "    # Display the JavaScript\n",
    "    display(Javascript(js_code))\n",
    "\n",
    "show_xml_popup('chapter1_ZA-content/gpt-results/converted_hansard.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ff088e",
   "metadata": {},
   "source": [
    "## Discussion \n",
    "STRUCTURE IT AS AN OVERALL DISCUSSION? DO YOU WANNA COMPARE THE SCORES TO EACH OTHER HERE? \n",
    "### Llama Herd \n",
    "\n",
    "### Gemini 1.5 Flash \n",
    "\n",
    "### GPT-4o \n",
    "\n",
    "### Limitations\n",
    "Problems: specific world knowledge that is needed to fill in the metadata, size of context window, computational power/resources. \n",
    "Prompt Engineering on local LLMs (Why it doesn't work for this specific case, why it didn't work for us.) -> the limited context window paired with the large input, the inability to work with unaltered text, computational issues/hardware issues. Batching didn't work.\n",
    "\n",
    "Note for limitations: we do not populate the metadata files, because very specific real world knowledge would be needed, and it is easier and computationally more efficient to populate this metadate with a rule-based approach once the base xml of the speeches themselves are parsed/created by the LLM. \n",
    "Many members of the SA parliament do not have their birth date published online. \n",
    "\n",
    "\n",
    "## Conclusion \n",
    "\n",
    "As pretrained LLMs show difficulties in formatting a large amount of documents into a highly specific format such as the ParlaMint, further research is necessary on whether tools such as Evaporate or SEED, whene again available, can be adapted better to the task. A different approach could lie in accessing stronger hardware through cloud computing platforms such as google colab to run models such as Jellyfish which are specialized on the task of formatting data {cite:p}`zhang_jellyfish_2024`.\n",
    "\n",
    "## Bibliography\n",
    "```{bibliography}\n",
    ":style: plain\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "format": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": "0.13",
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "source_map": [
   14,
   71,
   78,
   84,
   96,
   109,
   130,
   136,
   166,
   234,
   286,
   297,
   373,
   387,
   390,
   400,
   449,
   456,
   466,
   509,
   538
  ],
  "thebe": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}