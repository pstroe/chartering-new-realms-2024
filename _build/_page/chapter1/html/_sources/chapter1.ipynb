{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b36f74af",
   "metadata": {},
   "source": [
    "# Chapter 1: From Data to Corpus\n",
    "Anouk Menzi, Elizabeth Wagner\n",
    "\n",
    "## Abstract \n",
    "\n",
    "This chapter explores the potential of utilizing Large Language Models (LLMs) to preprocess raw textual data into structured collections, specifically focusing on formatting parliamentary proceedings from the South African Hansard into the interoperable ParlaMint TEI XML schema. The investigation evaluates several LLMs, including the Llama 3 models, Gemini 1.5 Flash, GPT-4o, and custom GPT configurations, to determine their efficacy in automating this labor-intensive task. While the Llama models failed to produce valid XML outputs and Gemini struggled with consistency and adherence to the schema, GPT-4o and custom GPTs demonstrated the ability to generate syntactically well-formed XML files, albeit with significant content and structural inaccuracies. The chapter highlights the promise and limitations of current LLMs in adapting to complex schema-specific requirements, underscoring the challenges of integrating such models into the workflow of corpus creation for multilingual parliamentary proceedings.\n",
    "\n",
    "## Introduction\n",
    "Scholars spend many hours on preprocessing raw data into structured collections that suit their needs {cite:p}`chen_2023`. This process is time- and resource-intensive, especially when dealing with natural language data. Considering the current trend in the digital humanities away from big data towards massive data, the questions of making data Findable, Accessible, Interoperable, and Reusable {cite:p}`wilkinson_2016` have become a focus area of constructing data collections {cite:p}`ide_2003, könig_2021`. An attempt at making data accessible, interoprable, and reusable can be found in the Text Encoding Initiative, TEI {cite:p}`burnard_2013`. In this chapter, we propose an approach to the construction of structured data collections with the assistance of Large Language Models (LLMs) to reduce the amount of human labour invested in preprocessing. Using the ParlaMint schema, an interoperable TEI XML schema for transcripts of parliamentary proceedings, to structure a collection of parliamentary transcriptions. This approach could be promising in cases where there exists much raw data without it being accessible in any coherent structure, as is the case, for example, for the variations of South African English {cite:p}`barnard_2014, jeffery_2003, pienaar_2011`. One such source of raw data are the South African parliamentary proceedings. The South African Parliament supplies transcripts of its parliamentary proceedings online, and whilst attempts have been made to structure this data into the ParliMint schema, the attempt has been labour-intensive and performed on a small scale {cite:p}`ogrodniczuk_2024`. This chapter shall thus attempt an approach of formatting these parliamentary proceedings into the ParlaMint XML schema by employing the aid of different LLMs without the use of industrial strength hardware. \n",
    "\n",
    "## Relevant Literature \n",
    ">Parliamentary proceedings (PP) are a rich source of data used by e.g. scholars in historiography, sociology, political science, linguistics, economics and economic history. As opposed to sources of most other language corpora, PP are not subject to copyright or personal privacy protections, and are typically available online thus making them ideal for compilation into corpora and open distribution. For these reasons many countries have already produced PP corpora, but each typically in their own encoding, thus limiting their comparability and utilisation in a multilingual setting.\n",
    ">{cite:p}`erjavec_2019`\n",
    "\n",
    "ParlaMint was invented as an interoperable, adaptive framework of formatting non-standardised parliamentary proceedings {cite:p}`erjavec_2023`. A first attempt at including the South African Parliamentary Proceedings has been made by Ogrodniczuk {cite:t}`ogrodniczuk_2024`, again highlighting the difficulties of formatting an unstructed document into the ParlaMint scheme {cite:p}`erjavec_2023`. The experiment was conducted on a singular session with a rule-based approach {cite:p}`ogrodniczuk_2024`. This proved difficult as, for example, all kinesic categories, which denote descriptions of behaviour such as applause mentioned in the transcripts, had to be annotated manually, thus making it a labour-intensive process when considering the 25 years of proceedings available for download on [the website of the South African Parliament](https://www.parliament.gov.za/hansard) {cite:p}`ogrodniczuk_2024, truan_2021`. \n",
    "\n",
    "This difficulty of the rule-based approach raised the question of whether LLMs could be harnessed to format the data. The primary attractivity of harnessing LLMs lies in their capability to process natural language inputs and their generalized applicability to unknown tasks {cite:p}`zhang_jellyfish_2024, narayan_2022`. In this they are more flexible than specialized tools. Their flexibility is especially appreciated when it comes to the robustness of processing because, given that they are not rule based, they are able to adapt to unforseen circumstances {cite:p}`zhang_jellyfish_2024`. With this ability, they have found a wide application ground within the field of linguistics such as in Zappavigna, where ChatGPT was tasked with evaluating noisy social media data {cite:t}`zappavigna_2023`, extracting data points from unstructured documents {cite:p}`vijayan_2023`, or in the use of generative LLMs for corpus analysis {cite:p}`curry_2024`. As to the knowledge of the authors of this chapter, no attempts at harnessing LLMs for parliamentary corpus building specifically have been attempted. However, in the wider field of data curation and formatting, the capabilities of LLMs are utilized, for example, to summarise healthcare data from semi-structured forms into possible schematas of illnesses {cite:p}`letinier_2021` or in processing natural language for the biomedical field into a reusable format {cite:p}`beck_2022`. These adaptations of LLMs are highly specialized to their respective tasks and have thus lost much of their generality which is so desired by data scientists in their quest for a one-stop-shop solution for data wrangling {cite:p}`chen_2023`. A further issue of these specialized tasks lie in the idea that LLMs also mark faulty data, or rather correct these errors, such as in customer databases {cite:p}`pookandy_2022`. A behaviour which, in the field of linguistics, is at its best irrelevant but rather more likely renders the data worthless as it would alter the transcripts. \n",
    "\n",
    "In light of these added difficulties when it comes to language data, where the language itself is of importance rather than data points, research has largely been based on developing a tool that, in its foundations, is based on LLMs but that also includes code with a rule-based approach to wrangle data {cite:p}`chen_2023, arora_2023`. Evaporate is capable of transforming various, semistructured inputs into a table output, its LLM components are based on using cloud solutions to run LLMs {cite:p}`arora_2023`. SEED works on a similar basis, though its output format can be customised [^footnote1]. A model specifically fine-tuned for dataprocessing is the jellyfish family [^footnote4], as proposed by Zhang et al. which is based on the smaller models of the Llama 3 family {cite:t}`zhang_jellyfish_2024`. These tools display a remarkable adaptivity to new tasks, especially when paired with few-shot prompting, where a model or tool is supplied with several examples before it is set to the task {cite:p}`chen_2023`. However, this comes at a greater computational cost than when employing rule-based tools {cite:p}`arora_2023`. Considering this trade-off between robustness, human-costliness and energy-efficiency, any approach must be carefully calculated and balanced.\n",
    "\n",
    "[^footnote4]: The Jellyfish model requires a GPU with more than 15 GB of memory, we neither have a device available with such a GPU, nor does Google Colab support such memory use on their free plan, thus we are unable to include it in our tests. \n",
    "\n",
    "[^footnote1]: SEED is currently undergoing restructuring and thus cannot be used. It seems to be a promising project for further investigation into processing raw data via LLM, see [SEED repository](https://anonymous.4open.science/r/SEED/paper.pdf). \n",
    "\n",
    "\n",
    "## Data and Methods\n",
    "\n",
    "```{warning} Due to jupyterlite not supporting many of the modules used in this method, many of the code-blocks in this paper are not executable. Please export the code and the corresponding files to run it locally if required. See the [Thebe documentation](https://jupyterbook.org/en/stable/interactive/thebe.html) for more details.\n",
    "```\n",
    "\n",
    "### Data\n",
    "The data was extracted from the official website for the Hansard Papers of the Parliament of the Republic of South Africa. The Hansard Papers constitute the transcripts of the mini plenary sessions of the National Assembly, the National Council of Provinces, the National Assembly, and any joint sessions. The National Assembly is formed by 400 members from the various South African political parties. The National Assembly is elected by the voting population of South Africa. The National Council of Provinces, NCoP, is formed with 90 provincial delegates which translates to 10 delegates for each province. It is thus composed regardless of population distribution. The NCoP is chosen by the provincial legislatures {cite:t}`piombo_2005`. \n",
    "\n",
    "The transcripts are majoritatively held in English, though when a speaker chooses to use another official language, it is transcribed and no English translation is given. \"Hansard is a substantially verbatim report - with repetitions and redundancies omitted and obvious mistakes corrected - of parliamentary proceedings\" (see [Website of the South African parliament](https://www.parliament.gov.za/hansard?sorts[language]=-1&page=5&offset=40)). Kotze and Van Rooy remark that it remains unclear what \"substantially verbatim\" connotes, particularly in terms of corrections leading to overstandardization. {cite:p}`kotze_2020, hibbert_2016, hibbert_2003`.[^footnote2] \n",
    "\n",
    "[^footnote2]: For a more detailed discussion of editing practices in the South African Hansard view {cite:p}`hibbert_2016, hibbert_2003`.\n",
    "\n",
    "The decision was made to process a randomly selected transcript from the National Assembly, henceforth abbreviated NA, to assess whether the proposed approach is feasible, thus following the methodology of Ogrodniczuk {cite:t}`ogrodniczuk_2024`. The data thus excludes any other years, all joint sittings, all meetings of the NCoP and all mini plenary sessions, however the formatting for all of these transcripts follows the approximate same standards as those of the NA. The session selected was from the 25th of February 2020. \n",
    "\n",
    "#### ParlaMint \n",
    "\n",
    "To ensure that the data in this corpus remains both human- and machine-readable while adhering to widely accepted standards, it was decided to encode the transcriptions into XML using the ParlaMint schema {cite:p}`erjavec_2023`, a customisation of the Parla-Clarina schema, which itself is based on the Text Encoding Initiative (TEI) guidelines {cite:p}`tei_consortium_guidelines`. This approach allows the corpus to maintain a consistent structure while also providing a way to encode the specific nuances of parliamentary discourse.\n",
    "\n",
    "The decision to adopt TEI, and specifically the ParlaMint schema, was guided by several considerations, specifically the goal of adhering to the FAIR principles. TEI's flexibility allows for the encoding of a diverse range of textual features, including data about speakers, organisations, places and times and other transcriber's notes about activity within the parliamentary session. The ParlaMint schema, as a specialized extension of TEI, was designed to standardise the encoding of parliamentary data across various languages and regions {cite:p}`ogrodniczuk_2024` and allows for the encoding of a wide variety of metadata while following a strict structure to enable maximal interoperability {cite:p}`erjavec_2023`. In addition to the strict encoding guidelines for data and metadata provided by the ParlaMint schema, it also allows for meticulous documentation of the process to enable reusability for future research. Overall, the ParlaMint schema was designed to adhere to the FAIR principles as closely as possible {cite:p}`erjavec_2023`. \n",
    "\n",
    "The concluded ParlaMint I project entailed the encoding of corpora containing transcriptions of the sessions of 17 European national parliaments, resulting in a collection of half a billion words {cite:p}`erjavec_2022_TEI`. Each corpus was prepared in two versions, one being the fully marked-up corpus with speeches in plain text, the other being identical to the first but with added linguistic annotation {cite:p}`erjavec_2023`. Adhering to the ParlaMint schema while encoding the South African Hansard papers would allow this corpus to seamlessly integrate with the ParlaMint I project.\n",
    "\n",
    "##### Gold Standard\n",
    "The preprocessing of the transcriptions involved several steps to ensure consistency and compliance with the ParlaMint schema. This included turning the PDF-document downloaded from the South African parliament's website {cite:p}`hansardSA_2020` into a TXT file. The content of this TXT file was not edited, save for occasional spelling errors within headers and subtitles. This TXT file was then converted to an XML file manually.\n",
    "\n",
    "A ParlaMint corpus is contained within a teiCorpus element, which includes a teiHeader for overarching metadata and multiple TEI elements, each representing a distinct component of the corpus, typically corresponding to a single day's transcripts. To manage large corpora more easily, ParlaMint uses the XInclude mechanism. In this setup, the main corpus file, called the corpus root, references individual files, the corpus component files. Thus, each day's transcripts are stored in a separate file, with the overarching structure being represented in the corpus root. This approach facilitates scalability and makes the corpus easier to maintain. This approach facilitates scalability and makes the corpus more easy to maintain. {cite:p}`ParlaMint_2024`.\n",
    "\n",
    "Example structure of the corpus root file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "274a6cd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1976756135.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    <teiCorpus xmlns=\"http://www.tei-c.org/ns/1.0\">\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<teiCorpus xmlns=\"http://www.tei-c.org/ns/1.0\">\n",
    " <teiHeader>...</teiHeader>\n",
    " <TEI>...</TEI>\n",
    " <TEI>...</TEI>\n",
    " ...\n",
    "</teiCorpus>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4294cc35",
   "metadata": {},
   "source": [
    "A corpus component file consists of one TEI element. The teiHeader element within the TEI element contains metadata specific to the component, such as details about the parliamentary session, date, and participants. The text element holds the actual transcription of the parliamentary proceedings. This transcription is organized into divisions, which may represent different sessions or segments, and further into individual utterances. Each utterance is typically associated with a speaker, who is identified through attributes that link to the metadata in the teiHeader. {cite:p}`ParlaMint_2024`\n",
    "\n",
    "Example structure of a corpus component file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33812f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "<TEI xmlns=\"http://www.tei-c.org/ns/1.0\">\n",
    " <teiHeader>...</teiHeader>\n",
    " <text>\n",
    "    <body>\n",
    "      <div type=\"commentSection\">...</div>\n",
    "      <div type=\"debateSection\">...</div>\n",
    "      <div type=\"debateSection\">...</div>\n",
    "    ...\n",
    "    </body>\n",
    " </text>\n",
    "</TEI>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ea99f7",
   "metadata": {},
   "source": [
    "The ParlaMint schema also allows for the encoding of extensive metadata around speakers and organisations, all of which are stored in separate files, which are referenced when necessary.[^footnote3]\n",
    "\n",
    "[^footnote3]: For more information about the structure of the ParlaMint schema visit their [GitHub repository](https://github.com/clarin-eric/ParlaMint).\n",
    "\n",
    "The first step was to prepare the corpus root file which contains the metadata about the South African Hansard papers. In a next step, a sample XML file was prepared. For this purpose, the TXT file containing the transcripts of the session of the National Assembly held on 25.02.2020 was selected. A shortened version of around 17 pages was created, containing around three speeches and the introductory conversation of that session. This short TXT was then converted into an XML file, adhering to the ParlaMint schema. It was judged that these 17 pages contained enough variation in speakers and discourse as to provide a wide array of different XML elements and attributes within the XML file to serve as example for the prompts served to the LLMs. \n",
    "\n",
    "Example snippet from the converted XML file, showing part of the teiHeader element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1f0132",
   "metadata": {},
   "outputs": [],
   "source": [
    "<TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"HansardSA_NA_2020\" xml:lang=\"en\">\n",
    "    <teiHeader>\n",
    "     <fileDesc>\n",
    "       <titleStmt>\n",
    "            <title type=\"main\" xml:lang=\"en\">South African Hansard papers</title>\n",
    "            <title type=\"sub\">Minutes of the National Assembly of South Africa</title>\n",
    "            <meeting n=\"1\" corresp=\"#DZ\" </meeting> \n",
    "       </titleStmt>\n",
    "       ...\n",
    "      <profileDesc>\n",
    "        <settingDesc>\n",
    "            <setting> \n",
    "                <name type=\"place\">Houses of Parliament</name>\n",
    "                <name type=\"city\">Cape Town</name>\n",
    "                <name type=\"country\" key=\"ZA\">South Africa</name>\n",
    "                <date when=\"2020-02-25\">25.02.2020</>\n",
    "            </setting>\n",
    "        </settingDesc>\n",
    "     </profileDesc>\n",
    "</TEI>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6122b3bf",
   "metadata": {},
   "source": [
    "As described above, speaker metadata is stored in a separate file, which is referenced as necessary. Specifically, a unique ID is defined for each speaker within this speaker metadata file, which is used in the component file to identify the speaker and link them to the metadata file. To facilitate the prompt engineering at first, the decision was made to forgo the speaker metadata context to ensure a clean run of the LLM. In a next step, the speaker element could be called upon to insert the relevant ID.\n",
    "\n",
    "Example snippet from the converted XML file, showing part of text element containing the speeches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08657c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "<text>\n",
    "  <body>\n",
    "    <div type=\"debateSection\">\n",
    "      <note type=\"time\">The House met at <time when=\"2020-02-25T014:00:00\">14:00</time>.</note>\n",
    "      <note type=\"narrative\">House Chairperson Ms M G Boroto took the Chair and requested members to observe a moment of silence for prayer or meditation.</note>\n",
    "      <note type=\"speaker\">The HOUSE CHAIRPERSON (Ms M G Boroto):</note>\n",
    "      <u xml:id=\"25-02-2020_u1\" who=\"#houseChairperson\">\n",
    "        <seg xml:lang=\"en\">Hon members, I would like to remind you that on 4 December 2019 the House adopted the Rules Committee report which introduced a number of\n",
    "            amendments to our rules. Some of the amendments pertain to thesequence of proceedings and Members’ Statements. To facilitate sufficient opportunity for Ministers’ Responses to Members’ Statements, the sequence of proceedings has been amended so that Members’ Statements are now at the start of the proceedings on days that they are scheduled by the programming committee.\n",
    "            </seg>\n",
    "        <seg xml:lang=\"en\">The Rules Committee further agreed that the number of Ministers’ Responses be increased from six to seven and that time allowed for ministers’ Responses be increased from two minutes to three minutes. With that background, I will now take the first item on the Order Paper which is Members’ Statements. Does any member of the ANC wish to make a statement?\n",
    "        </seg>\n",
    "      </u>\n",
    "      <note type=\"speaker\">The CHIEF WHIP OF THE OPPOSITION:</note>\n",
    "      <u xml:id=\"25-02-2020_u2\" who=\"#ChiefWhipOfOpposition\"> \n",
    "        <seg xml:lang=\"en\">\n",
    "            Sorry Chair, on a point of order.\n",
    "        </seg>\n",
    "      </u>\n",
    "      <note type=\"speaker\">The HOUSE CHAIRPERSON (Ms M G Boroto):</note>\n",
    "      <u xml:id=\"25-02-2020_u3\" who=\"#houseChairperson\">\n",
    "        <seg xml:lang=\"en\">\n",
    "            Please take your seat. Yes, what’s your point of order?\n",
    "        </seg>\n",
    "      </u>\n",
    "      ...\n",
    "    </div>\n",
    "  </body>\n",
    "</text>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46e81a6",
   "metadata": {},
   "source": [
    "### Method \n",
    "This chapter uses the newest releases of the Llama 3 model family, Gemini 1.5 Flash, GPT-4o, and the custom GPTs. \n",
    "\n",
    "Llama makes multiple sets of pretrained models with different quantities of parameters available, thus, offering the possibility of maximising minimal parameter count to maximum quality output thus possibly minimizing the costliness of the running of such models. Whilst there is an effort to optimize models, it is still not possible to train a LLM locally on a standard laptop {cite:p}`zhang_jellyfish_2024`. However, it is possible to run some pretrained models locally, provided that their parameter count is relatively small, and adapt them to a specific task via prompting. In this context Llama offers small-scale options with their development of the general models Llama 3.2 1B, 3B, 8B and 70B, where especially the 1B and the 3B parameter models are runnable on mobile or edge devices {cite:p}`dubey_2024`. The smaller models are \"best-in-class, outperforming alternative models with similar numbers of parameters\" {cite:p}`dubey_2024`. However, these smaller models come at a reduction of processing power, which may cause difficulties in processing the complex ParlaMint structure. \n",
    "\n",
    "```{figure} chapter1_ZA-content/images/llama_3.jpg\n",
    "---\n",
    "width: 650px\n",
    "align: center\n",
    "name: fig-llama_3\n",
    "---\n",
    "Llama 3 herd with parameters {cite:p}`dubey_2024`\n",
    "```\n",
    "\n",
    "Another model utilised was Gemini 1.5 Flash. Gemini 1.5 Flash constitutes the attempt at constructing a lightweight model with GPT-4 capabilities but a longer context window {cite:p}`gemini_2024`. It promises accuracy across a context window of 10 million token, whilst being relatively efficient and more efficient to serve then the Gemini 1.0 models {cite:p}`gemini_2024`, thus making it a promising candidate for formatting data as, especially in the case of ParlaMint, the complete context of the correct XML structure is relatively long.  \n",
    "\n",
    "GPT-4o was chosen for its accessibility, computational efficiency, and ease of use. Unlike other, resource-intensive models, GPT-4o offers advanced capabilities while remaining efficient to run on standard hardware. Additionally, OpenAI announced in a press conference that GPT-4o demonstrates significant advancements in linguistic, textual, and visual reasoning task {cite:p}`liu_2024`.\n",
    "\n",
    "With the subscription to the GPT-4o model comes the option of configuring user-specific GPTs. Custom GPTs are specialized models tailored to perform specific tasks. Users can configure these models without coding by providing clear instructions and uploading relevant documents. Once configured, custom GPTs operate by leveraging the provided instructions and data to generate responses aligned with the user's requirements. {cite:p}`zhao_2024, garrido_2023, openai_2025` \n",
    "\n",
    "Custom GPTs have been shown to outperform basic models like GPT-3.5 and GPT-4 in specialised, domain-specific tasks in fields such as medicine {cite:p}`liu_2024, muti_2024` and education {cite:p}`garrido_2023`, especially when the tasks involve information-retrieval. \n",
    "\n",
    "**Models Used**\n",
    "- Llama 3.2 1B\n",
    "- Llama 3.2 3B\n",
    "- Llama 3 8B\n",
    "- Gemini 1.5 Flash\n",
    "- GPT-4o\n",
    "- Custom GPT\n",
    "\n",
    "Because of the various implementations of the LLMs, with the Llama herd being run locally, and Gemini, GPT-4o and the custom GPTs being run through their online interface, different approaches had to be taken. \n",
    "\n",
    "Overall, the ParlaMint schema was simplified as to compartementalize the different elements of the structure. The input prompt was always structured by giving an example of the raw data, an example of the structured, corresponding XML section and a set of instructions {cite:p}`sahoo_2024`. Depending on whether the LLM was called via API or its online interface, it was either guided onwards through repeating the instruction, or iterative refinement {cite:p}`vijayan_2023`. Furthermore, persona prompting and chain of thought prompting was attempted where appropriate. For a detailed description of the approaches, please view the [Experiments and Results](experiments) section.\n",
    "\n",
    "#### Evaluation\n",
    "To evaluate the work of the LLMs automatically, a twofold approach was selected, where both the structure, [Evaluation XML Schema](evaluation_xml) and the content, [Evaluation Content](evaluation_content) of the processed file was assessed. \n",
    "\n",
    "(evaluation_xml)=\n",
    "##### Evaluation XML Schema\n",
    "To validate the XML schema of the files output by the LLMs, the RelaxNG {cite:p}`clark_2001` file format was chosen. A RelaxNG file is itself an XML file, which can be used to check and validate the structure of an XML file {cite:p}`van-der-vlist_2003`. This format was selected as there already exists an official RelaxNG file created by the ParlaMint team [^footnote9]. Due to the simplified nature of the XML schema followed within this paper, the ParlaMint RelaxNG file was adapted and simplified to better suit this project's needs.\n",
    "\n",
    "Using a short Python script, the adapted RelaxNG file was used to evaluate and validate all XML files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797739cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lxml\n",
    "\n",
    "from lxml import etree\n",
    "from collections import Counter\n",
    "\n",
    "def validate_xml(relaxng_file, xml_file):\n",
    "    \"\"\"\n",
    "    Validates an XML file against a RelaxNG schema and prints detailed error messages,\n",
    "    along with a total count of errors and a count of each error type.\n",
    "\n",
    "    :param relaxng_file: Path to the RelaxNG schema file.\n",
    "    :param xml_file: Path to the XML file to be validated.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(relaxng_file, 'r', encoding='utf-8') as rng_file:\n",
    "            relaxng_doc = etree.parse(rng_file)\n",
    "            relaxng = etree.RelaxNG(relaxng_doc)\n",
    "        \n",
    "        with open(xml_file, 'r', encoding='utf-8') as xml_file_obj:\n",
    "            xml_doc = etree.parse(xml_file_obj)\n",
    "        \n",
    "        if relaxng.validate(xml_doc):\n",
    "            print(f\"The XML file '{xml_file}' is valid according to the RelaxNG schema\")\n",
    "        else:\n",
    "            print(f\"The XML file '{xml_file}' is NOT valid according to the RelaxNG schema\\n\")\n",
    "            print(\"Validation errors:\")\n",
    "\n",
    "            error_count = 0\n",
    "            error_type_counter = Counter()\n",
    "\n",
    "            # Process and print each error\n",
    "            for error in relaxng.error_log:\n",
    "                error_count += 1\n",
    "                error_type_counter[error.type_name] += 1\n",
    "                print(f\"Line {error.line}, Column {error.column}: {error.message}\")\n",
    "                print(f\"  Domain: {error.domain_name}, Type: {error.type_name}\\n\")\n",
    "\n",
    "            # Print total error summary\n",
    "            print(\"Summary of Validation Errors:\")\n",
    "            print(f\"Total Errors: {error_count}\")\n",
    "            for error_type, count in error_type_counter.items():\n",
    "                print(f\"  {error_type}: {count} occurrences\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    relaxng_file = \"chapter1_ZA-content/xml_validation/Adapted_ParlaMint.rng\" \n",
    "    xml_file = \"chapter1_ZA-content/xml_validation/uh_25.02_short.xml\"    \n",
    "\n",
    "    validate_xml(relaxng_file, xml_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4361872",
   "metadata": {},
   "source": [
    "If the XML file is valid, the output consists of a single line: \"The XML file '{xml_file}' is valid according to the RelaxNG schema\". If the XML file is not valid, the script outputs a list of all errors with their corresponding line numbers and error types. Additionally, it outputs a total sum of errors and a sum of each type of error, which facilitates the comparison across different evaluations.\n",
    "\n",
    "Where the validation using a RelaxNG file falls short is when the tested XML file is not well-formed, when the XML is syntax is not adhered to. For instance, the script will fail immediately if the tested file is missing an XML tag (< or >) or when an element is not properly closed. It also cannot validate the correctness of the content of the XML elements, an issue that was manually evaluated. \n",
    "\n",
    "[^footnote9]: This RelaxNG file can be accessed on the ParlaMint project's GitHub repository, in the [Schema](https://github.com/clarin-eric/ParlaMint/tree/main/Schema) folder.\n",
    "\n",
    "(evaluation_content)=\n",
    "##### Evaluation Content\n",
    "To evaluate the content of the output of the LLMs tested an approach based on the Levenshtein distance was adapted. To avoid looping through each file, the decision was made to base the validation script on a random sampler of sentences. It samples a specified number of sentences from the processed XML file and chunks the sentence into 5-grams. 5-grams are generally more flexible than an entire sentence whilst simultaneously being of a size large enough to not be confused with common expressions {cite:p}`fischer-starcke_2009`. This approach was chosen to match sentences even when a word or sequence of the selected sentence does not match exactly, and to make the script more robust for  typeset errors. These 5-grams are then matched up to a sentence from the TXT file with the highest match score. Then the sentence extracted from the XML is compared to the sentence from the  and compares them to the original TXT file employing the Levenshtein distance {cite:p}`beijering_2008`.\n",
    "\n",
    "```{attention} This code needs to be configured for the XML tag that denotes where the text content of the file is stored. The ParlaMint scheme specifies this with the *seg* tag. In this script it is customisable, to allow for output from LLMs which configure this tag wrongly, to allow for a consistent check of content. Furthermore, a regular expression was configured to check whether the speaker segmentation was successful. The code below is configured for the gold standard.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2ca998",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install levenshtein\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "\"\"\"Script to evaluate all XML output files in a folder against its original. \n",
    "Includes error handling for incomplete/incorrect XML schema where it parses the erroneous file as TXT format.\n",
    "\"\"\"\n",
    "\n",
    "# Path to the folder containing XML files\n",
    "xml_folder_path = 'chapter1_ZA-content/gold_standard'  \n",
    "txt_file_path = 'chapter1_ZA-content/gold_standard/gold_standard.txt'  \n",
    "\n",
    "# Open the original file and reads its content, this serves as comparator for the processed XML files. \n",
    "with open(txt_file_path, 'r', encoding='utf-8') as file:\n",
    "    txt_content = file.read()\n",
    "\n",
    "# Removes newline characters and extra spaces from the txt file content, ensures that the script is not bothered by the original format.\n",
    "txt_content = re.sub(r'\\s+', ' ', txt_content.replace('\\n', ' ').strip())\n",
    "\n",
    "# Define the namespace for the TEI XML, given for ParlaMint, \n",
    "namespace = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "# Option to configure the tag where the text is contained. 'seg' is the official ParlaMint tag.\n",
    "tag = \"text\"\n",
    "\n",
    "# Option to configure the sampler size from each XML file. Can be set at random, or calculated for a desired probability of reliability. \n",
    "sampler = 50\n",
    "\n",
    "# Function to split text into sentences\n",
    "def split_into_sentences(text):\n",
    "    # Basic sentence splitting based on a simple regex. Not perfectly robust but when tested on the gold standard, output is at 100%\n",
    "    \n",
    "    #r'(?<=\\w[.!?]) +'\n",
    "    return re.split(r'(?<=\\w[.!?])\\s*(?=\\w)', text)\n",
    "\n",
    "# Function to generate 5-grams from a sentence\n",
    "def generate_5grams(sentence):\n",
    "    words = sentence.split()\n",
    "    return [tuple(words[i:i+5]) for i in range(len(words) - 4)]\n",
    "\n",
    "# Function to calculate how much of the sentence is found in the TXT content using 5-grams and Levenshtein distance\n",
    "def calculate_match_percentage_using_5grams_and_levenshtein(sentence, txt_content):\n",
    "    # Generate 5-grams for the sentence\n",
    "    sentence_5grams = generate_5grams(sentence)\n",
    "    \n",
    "    # Split txt_content into sentences and generate 5-grams for each\n",
    "    txt_sentences = split_into_sentences(txt_content)\n",
    "    txt_5grams = [generate_5grams(txt_sentence) for txt_sentence in txt_sentences]\n",
    "    \n",
    "    # Find the sentence from txt_content with the most matching 5-grams\n",
    "    max_match_count = 0\n",
    "    best_matching_sentence = \"\"\n",
    "    for i, txt_grams in enumerate(txt_5grams):\n",
    "        match_count = len(set(sentence_5grams) & set(txt_grams))  # Count matching 5-grams\n",
    "        if match_count > max_match_count:\n",
    "            max_match_count = match_count\n",
    "            best_matching_sentence = txt_sentences[i]\n",
    "    \n",
    "    # Compute Levenshtein distance between the sentence and the best matching sentence\n",
    "    levenshtein_dist = levenshtein_distance(sentence, best_matching_sentence)\n",
    "    \n",
    "    return max_match_count, best_matching_sentence, levenshtein_dist\n",
    "\n",
    "# Function to check for speaker names in the sentence\n",
    "def check_names(sentences): \n",
    "    for sentence in sentences: \n",
    "        pattern = r\"(?<!>)([A-Z]+(?: [A-Z]+)*)(?!>):\"\n",
    "        matches = re.findall(pattern, sentence)\n",
    "        count = len(matches)\n",
    "        if count != 0: \n",
    "            return(count)\n",
    "\n",
    "# Function to remove content between angle brackets\n",
    "def remove_angle_brackets_content(input_string):\n",
    "    return re.sub(r'<.*?>', '', input_string)\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# List to store all sampled sentences\n",
    "all_sentences = []\n",
    "\n",
    "# Dictionary to track speaker errors\n",
    "speaker_error = {}\n",
    "\n",
    "# Loop through all XML files in the folder\n",
    "for xml_file_name in os.listdir(xml_folder_path):\n",
    "    print(xml_file_name)\n",
    "    if xml_file_name.endswith('.xml'):  # Only process XML files\n",
    "        xml_file_path = os.path.join(xml_folder_path, xml_file_name)\n",
    "\n",
    "        try:\n",
    "            # Try parsing the XML file\n",
    "            tree = ET.parse(xml_file_path)\n",
    "            root = tree.getroot()\n",
    "            segments = root.findall(f'.//tei:{tag}', namespace)\n",
    "\n",
    "            # Loop through each segment and extract sentences\n",
    "            for seg in segments:\n",
    "                sentences = split_into_sentences(seg.strip())  # Split text into sentences\n",
    "                speaker_error[xml_file_name] = check_names(sentences)  # Check for speaker names\n",
    "                sentences_tagless = [remove_angle_brackets_content(item) for item in sentences]  # Remove XML tags\n",
    "                sam_sentences = random.sample(sentences_tagless, sampler) if len(sentences_tagless) >= sampler else sentences_tagless\n",
    "                sampled_sentences = [re.sub(r'\\s+', ' ', sentence.replace('\\n', ' ').strip()) for sentence in sam_sentences]\n",
    "                all_sentences.extend(sampled_sentences)  # Add to the global list\n",
    "        except:\n",
    "            # If XML parsing fails, read the file as plain text\n",
    "            print(f\"XML parsing failed for {xml_file_name}, treating as plain text...\")\n",
    "            with open(xml_file_path, 'r', encoding='utf-8') as file:\n",
    "                raw_content = file.read()\n",
    "                sentences = split_into_sentences(raw_content.strip())  # Split sentences from raw content\n",
    "                sentences_tagless = [remove_angle_brackets_content(item) for item in sentences]  # Remove tags\n",
    "                speaker_error[xml_file_name] = check_names(sentences)  # Check for speaker names\n",
    "                sam_sentences = random.sample(sentences_tagless, sampler) if len(sentences_tagless) >= sampler else sentences_tagless\n",
    "                sampled_sentences = [re.sub(r'\\s+', ' ', sentence.replace('\\n', ' ').strip()) for sentence in sam_sentences]\n",
    "                all_sentences.extend(sampled_sentences)  # Add to the global list\n",
    "        # List to store match percentages for the current file\n",
    "        match_percentages = []\n",
    "        counts_token = 0\n",
    "\n",
    "        # Check for sentence match percentage with the TXT content\n",
    "        for sentence in sampled_sentences:\n",
    "            match_count, matching_sentence, levenshtein_dist = calculate_match_percentage_using_5grams_and_levenshtein(sentence, txt_content)\n",
    "            match_percentages.append((match_count, matching_sentence, levenshtein_dist))\n",
    "\n",
    "            counts_token += len(sentence.split())\n",
    "        \n",
    "        \n",
    "\n",
    "        # Store results\n",
    "        for i, (match_count, matching_sentence, levenshtein_dist) in enumerate(match_percentages, start=1):\n",
    "            file_error = speaker_error.get(xml_file_name, None)\n",
    "            results.append({\n",
    "                'XML File': xml_file_name,\n",
    "                'Sentence #': i,\n",
    "                'Sentence': sampled_sentences[i-1],\n",
    "                'Best Matching Sentence': matching_sentence,\n",
    "                'Matching 5-grams Count': match_count,\n",
    "                'Levenshtein Distance': levenshtein_dist,\n",
    "                'Speaker Error': file_error\n",
    "            })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Write the results to an Excel file\n",
    "excel_file_path = 'chapter1_ZA-content/gold_standard/gold_standard.xlsx'\n",
    "df.to_excel(excel_file_path, index=False)\n",
    "\n",
    "print(f\"Results have been written to {excel_file_path}\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc834b4c",
   "metadata": {},
   "source": [
    "(experiments)=\n",
    "## Experiments and Results\n",
    "In a primary approach, the attempt was made to guide locally run, smaller LLMs, from the Llama herd, via prompt engineering with a standard prompting approach but enriched with an example {cite:p}`vijayan_2023, zhang_2023, naveed_2023`. This approach was chosen to assess whether a smaller, and thus less costly, LLM could fulfill the task requirements. Furthermore, two larger LLMs, Gemini and GPT-4o, were tested through their online chat interface, to assess whether they produce a different, possibly a more stable output. \n",
    "\n",
    "### Llama Herd \n",
    "The prompt for the Llama herd is comprised of a shortened version of the TXT original file and its corresponding XML gold standard in the ParlaMint schema. The decision to utilize a standard prompting approach for the Llama herd was made to accomodate the context windows of the models tested. To work with the context window given, the files had to be chunked. The decision was made not to enlargen the context windows as larger context windows generally amplify hallucinations, which in the case of data formatting would be detrimental.\n",
    "\n",
    "Ollama was chosen as basesoftware as it offers the smaller Llama 3.2 models in downloadable form. Furthermore, Ollama is linked to langchain to customise its prompting abilities, as Ollama offers limited customization options, though this is subject to changes [^footnote]. Langchain offers flexibility with regards to customisation {cite:p}`martra_2024`. Thus, the temperature of the model was arranged between 0-0.3 to minimize creativity within the responses. The langchain Ollama link offers the option to employ persona prompting which was varied to test whether it influenced the models. h\n",
    "\n",
    "[^footnote]: For the newest updates and developments concerning Ollama, consult their [blog](https://ollama.com/blog).\n",
    "\n",
    "In the first attempt the model was given an example of the structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6ce6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_xml = f'<note type=\"speaker\">The CHIEF WHIP OF THE MAJORITY PARTY:</note> <u xml:id=\"25-02-2020_u16\" who=\"#ChiefWhipOfMajorityParty\"> <seg xml:lang=\"en\">Thank you very much, House Chair. As indicated on the Order Paper we shall proceed.</seg>'\n",
    "example_txt = f'The CHIEF WHIP OF THE MAJORITY PARTY: Thank you very much, House Chair. As indicated on the Order Paper we shall proceed.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4fb70a",
   "metadata": {},
   "source": [
    "A code example is given below with Llama 3.2 configured: \n",
    "\n",
    "```{tip}\n",
    "Make sure that you close Ollama before serving it on the command line, otherwise it will not work.\n",
    "\n",
    "To exit Ollama in the command line press ctrl + c.\n",
    "```\n",
    "\n",
    "```{attention} This code will fail because Ollama is a standalone piece of software!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af0f340",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain\n",
    "\n",
    "import os\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "#implements the access to the folder with the original file from the 25.02\n",
    "folder_path = r'test_objects'\n",
    "\n",
    "#sets the mood of the model.\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "#persona prompting text\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "#calls the type of prompt\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "#specifies the model\n",
    "model = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "#chunks the files into manageable sizes for the LLMs context windows. Chunk size can be customised, depending on the input frame and the context window of the model. \n",
    "def chunk_text(text, chunk_size=1000):\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        yield ' '.join(words[i:i + chunk_size])\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    #checks the input file's format\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                #gives the file currently being processed.\n",
    "                print(f'Processing file: {filename}')\n",
    "                content = file.read()\n",
    "\n",
    "                #implements the output of the model into a list so that it can later be added to an XML file. \n",
    "                document_list = []\n",
    "                i = 0 #counters for debugging\n",
    "                for chunk in chunk_text(content, chunk_size=1000):\n",
    "                    try:\n",
    "                        #actual prompt given to LLM\n",
    "                        question = f'If given raw text: {example_txt} with the end goal: {example_xml}, can you adapt this: {chunk} into the same XML format?'\n",
    "                        #stores the response of the model in a variable\n",
    "                        response = model(prompt.format(question=question))\n",
    "                        document_list.append(response)\n",
    "                         # Limit iterations for testing\n",
    "                        i += 1\n",
    "                        if i == 4: \n",
    "                            break\n",
    "                    #error handling\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing chunk: {e}\")\n",
    "                output_file = os.path.join(folder_path, f\"{os.path.splitext(filename)[0]}.xml\")\n",
    "                #write results to output file\n",
    "                with open(output_file, 'w', encoding='utf-8') as output:\n",
    "                    output.write('\\n'.join(document_list))\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c152ea3f",
   "metadata": {},
   "source": [
    "The results were mixed in quality. Llama 3 1B parameters mainly ignored the prompts and simply rephrased, summarized or corrected without attempting to format it into an XML structure. \n",
    "\n",
    "|| **Prompts**                                                                                    | **Persona** | **Temperature** | **Additional Files/Information** | **Results**                       |\n",
    "|----|--------------------------------------------------------------------------------------------------|-----|----|------------------------------------|------------------------------------|\n",
    "|5| Given raw text: {example_txt} with the end goal: {example_xml}, adapt this: {chunk} into the same XML format. | Answer: Let's think step by step | - |Snippets from TXT and XML file | View {Download}`llama 3 1B result 5<./chapter1_ZA-content/results/llama_herd/13.12/result_5.xml>` |\n",
    "|7| Given raw text: {example_txt} with the end goal: {example_xml}, adapt this: {chunk} into the same XML format. | Answer: Be a helpful assistant. | - |Snippets from TXT and XML file | View {Download}`llama 3 1B result 7<./chapter1_ZA-content/results/llama_herd/13.12/result_7.xml>` |\n",
    "\n",
    "Llama 3 3B parameters showed similar problems, though it did not try to correct the data, rather it only sporadically, if at all, formated one or two utterances by a speaker. \n",
    "\n",
    "|| **Prompts**                                                                                   | **Persona** | **Temperature** | **Additional Files/Information** | **Results**                       |\n",
    "|----|--------------------------------------------------------------------------------------------------|-------|-------|------------------------------------|------------------------------------|\n",
    "|2| Given raw text: {example_txt} with the end goal: {example_xml}, adapt this: {chunk} into the same XMLformat. | Answer: Let's think step by step | - |Snippets from TXT and XML file | View {Download}`llama 3 3B result 2<./chapter1_ZA-content/results/llama_herd/13.12/result_2.xml>` |\n",
    "|10| Given raw text: {example_txt} with the end goal: {example_xml}, adapt this: {chunk} into the same XML format. | Answer: Be a helpful assistant.  | - |Snippets from TXT and XML file | View {Download}`llama 3 3B result 10<./chapter1_ZA-content/results/llama_herd/13.12/result_10.xml>` |\n",
    "|17| Given: {example_txt} with the goal: {example_xml}, format this: {chunk} into the same XML format. Format all of the text. Give only the formated text. | Answer: Be a helpful assistant.  | 0 |Snippets from TXT and XML file | View {Download}`llama 3 3B result 17<./chapter1_ZA-content/results/llama_herd/22.12/result_10.xml>` |\n",
    "\n",
    "Llama 3 8B parameters was very inconsistent in its replies. It ranged from simply formatting all text into bold as in result 27, to formatting only the speakers but not the text as in result 7, or formatting only the text but not the speaker as in result 12. \n",
    "\n",
    "|| **Prompts**                                                                                   | **Persona** | **Temperature** | **Additional Files/Information** | **Results**                       |\n",
    "|----|--------------------------------------------------------------------------------------------------|-------|-------|------------------------------------|------------------------------------|\n",
    "|27| Given: {example_txt} with the goal: {example_xml}, format this: {chunk} into the same XML format. Format all of the text. | Answer: Be a helpful assistant | - |Snippets from TXT and XML file | View {Download}`llama 3 8B result 27<./chapter1_ZA-content/results/llama_herd/13.12/result_27.xml>` |\n",
    "|1| Given: {example_txt} with the goal: {example_xml}, format this: {chunk} into the same XML format. Format all of the text. | Answer: Be a helpful assistant | 0.1 |Snippets from TXT and XML file | View {Download}`llama 3 8B result 1<./chapter1_ZA-content/results/llama_herd/22.12/result_1.xml>` |\n",
    "|7| Given: {example_txt} with the goal: {example_xml}, format this: {chunk} into the same XML format. Format all of the text. Give only the formated text. | Answer: Be a helpful assistant.  | 0.05 |Snippets from TXT and XML file | View {Download}`llama 3 8B result 7<./chapter1_ZA-content/results/llama_herd/13.12/result_7.xml>` |\n",
    "|12| Given: {example_txt} with the goal: {example_xml}, format this: {chunk} into the same XML format. Format all of the text. Give only the formated text. |-|-|-| View {Download}`llama 3 8B result 12<./chapter1_ZA-content/results/llama_herd/22.12/result_12.xml>` |\n",
    "\n",
    "```{note}\n",
    "The conclusive results for the Llama herd and its corresponding commentary can be found in the {doc}`results<../chapter1_ZA-content/results/llama_herd>` folder of this chapter.\n",
    "```\n",
    "\n",
    "### Gemini 1.5 Flash\n",
    "To assess whether a larger LLM gives a better output, Gemini 1.5 Flash was tested in its online chat interface. Gemini's primary attractiveness for this task lies in its long context windows of up to 10 million token and its superior efficiency over the GPT models {cite:p}`gemini_2024`. As the online chat interface does not allow file input, the prompt was structured to contain both an example XML and an example TXT, as well as a chunk of a file to be processed. See below for an example of the structure. The file was chunked into 4000 word segments to respect the input maximum of 5108 tokens of each call for Gemini 1.5 Flash. Every conversation was held thrice to assess the answer scheme of the LLM and whether its answers are similar in content. \n",
    "\n",
    "```{example} PROMPT:[Given: [The CHIEF WHIP OF THE MAJORITY PARTY: Thank you very much, House Chair. As indicated on the Order Paper we shall proceed.] with the goal [<note type=\"speaker\">The CHIEF WHIP OF THE MAJORITY PARTY:</note> <who=\"#ChiefWhipOfMajorityParty\"> <seg xml:lang=\"en\">Thank you very much, House Chair. As indicated on the Order Paper we shall proceed.</seg>] format the following text into the same XML format. Format all of the text.\n",
    "[UNREVISED HANSARD\n",
    "NATIONAL ASSEMBLY\n",
    "TUESDAY, 25 FEBRUARY 2020\n",
    "Page: 1\n",
    "TUESDAY, 25 FEBRUARY 2020\n",
    "____\n",
    "PROCEEDINGS OF THE NATIONAL ASSEMBLY\n",
    "____\n",
    "The House met at 14:00.\n",
    "House Chairperson Ms M G Boroto took the Chair and requested\n",
    "members to observe a moment of silence for prayer or\n",
    "meditation.\n",
    "The HOUSE CHAIRPERSON (Ms M G Boroto): Hon members, I would\n",
    "like to remind you that on 4 December 2019 the House adopted\n",
    "the Rules Committee report which introduced a number of\n",
    "amendments to our rules. \n",
    "```\n",
    "\n",
    "Its output, however, was mostly unusable, as it refused to attempt the task and gave answers such as: \n",
    "\n",
    "|| **Prompts**                                                                                    |**Additional Files/Information** | **Output**                       |\n",
    "|----|--------------------------------------------------------------------------------------------------|------------------------------------|------------------------------------|\n",
    "|1| help me format some text | None | I can help you format text in a few different ways. I can: Change the font and size of the text. Make the text bold, italic, or underlined. Add bullet points or numbered lists to the text. Indent the text. Align the text to the left, right, or center. To help me format your text, please provide me with the following information: The text you want to format. The specific formatting you want to apply. For example, you could say: \"Please format the following text: This is an example of some text. I want to make the text bold and italic.\" Once you provide me with this information, I will be able to format your text for you. |\n",
    "|2| Your task: Given: [The CHIEF WHIP OF THE MAJORITY PARTY: Thank you very much, House Chair. As indicated on the Order Paper we shall proceed.] with the goal [<note type=\"speaker\">The CHIEF WHIP OF THE MAJORITY PARTY:</note> <who=\"#ChiefWhipOfMajorityParty\"> <seg xml:lang=\"en\">Thank you very much, House Chair. As indicated on the Order Paper we shall proceed.</seg>] format the following text  into the same XML format. Format all of the text.| 4000 token chunk of 25.02 transcript | View {Download}`second XML file<./chapter1_ZA-content/gemini-results/31.12/attempt_6-1.xml>`|\n",
    "|3|format all of the text | None | I'm not programmed to assist with that. |\n",
    "\n",
    "The experiment was successful insofar as that when specifically asking for help, Gemini attempted to format the input. In a first attempt it did format the text into an XML structure, though when re-running the prompt, it neglected all speaker turns and created one large segment containing the entire input. \n",
    "\n",
    "|| **Prompts**                                                                                    |**Additional Files/Information** | **Output**                       |\n",
    "|----|--------------------------------------------------------------------------------------------------|------------------------------------|------------------------------------|\n",
    "|1| I need help with some XML formatting. I have this example text: \"The CHIEF WHIP OF THE MAJORITY PARTY: Thank you very much, House Chair. As indicated on the Order Paper we shall proceed.\" And I need it formatted into this shape: \"<note type=\"speaker\">The CHIEF WHIP OF THE MAJORITY PARTY:</note> <who=\"#ChiefWhipOfMajorityParty\"> <seg xml:lang=\"en\">Thank you very much, House Chair. As indicated on the Order Paper we shall proceed.</seg>\". Help me format the following text: | 4000 token chunk of 25.02 transcript | View {Download}`output attempt 11 31.12 <./chapter1_ZA-content/gemini-results/31.12/attempt_11-1.xml>` |\n",
    "|2| Try again, pay attention to the speakers| None | View {Download}`second XML file<./chapter1_ZA-content/gemini-results/31.12/attempt_11-2.xml>`|\n",
    "|3|Assign a separate speaker tag to each speaker please | None | View {Download}`third XML file<./chapter1_ZA-content/gemini-results/31.12/attempt_11-3.xml>` |\n",
    "|4| Find all speakers in the text | None | View {Download}`fourth XML file<./chapter1_ZA-content/gemini-results/31.12/attempt_11-4.xml>`  |\n",
    "\n",
    "```{note}\n",
    "The conclusive results for Gemini Flash and its corresponding commentary can be found in the {doc}`results<../chapter1_ZA-content/results/gemini_flash>` folder of this chapter.\n",
    "```\n",
    "\n",
    "### Custom GPT\n",
    "When configuring a custom GPT, the user can set several different paramaters. For this paper, the paramaters Additional Instructions, Knowledge and New Capabilities were of particular interest.\n",
    "In the Additional Instructions section, the user may provide detailed instructions or guidelines on how the GPT should behave, its functionalities, and any particular behaviors to avoid. {cite:p}`openai_2025`\n",
    "\n",
    "The Knowledge is provided by the user in the form of uploaded files, which provides addtional context for the GPT to reference. The New Capabilities consist of Web Browsing, DALL·E Image Generation, Canvas and Advanced Data Analysis, which allow the GPT to perform additional functionality. {cite:p}`openai_2025, openai_knowledge`\n",
    "\n",
    "In order to have the custom GPT perform the XML-formatting task, the approach of interactive or multi-shot reasoning was chosen, as it has been suggested that this improves GPT's performance compared to a single-shot reasoning approach {cite:p}`truhn_2023`.\n",
    "\n",
    "Three custom GPTs were trained with each GPT being configured with slightly different parameters. However, each GPT was at some point in the interaction, given two matching snippets from the input TXT file and its corresponding XML file.\n",
    "\n",
    "Snippet from the TXT file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8c8e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNREVISED HANSARD\n",
    "NATIONAL ASSEMBLY\n",
    "TUESDAY, 25 FEBRUARY 2020\n",
    "Page: 1\n",
    "TUESDAY, 25 FEBRUARY 2020\n",
    "____\n",
    "PROCEEDINGS OF THE NATIONAL ASSEMBLY\n",
    "____\n",
    "The House met at 14:00.\n",
    "House Chairperson Ms M G Boroto took the Chair and requested\n",
    "members to observe a moment of silence for prayer or\n",
    "meditation.\n",
    "The HOUSE CHAIRPERSON (Ms M G Boroto): Hon members, I would\n",
    "like to remind you that on 4 December 2019 the House adopted\n",
    "the Rules Committee report which introduced a number of\n",
    "amendments to our rules. Some of the amendments pertain to the\n",
    "sequence of proceedings and Members’ Statements. To facilitate\n",
    "sufficient opportunity for Ministers’ Responses to Members’\n",
    "Statements, the sequence of proceedings has been amended so\n",
    "that Members’ Statements are now at the start of the\n",
    "proceedings on days that they are scheduled by the programming\n",
    "committee.\n",
    "UNREVISED HANSARD\n",
    "NATIONAL ASSEMBLY\n",
    "TUESDAY, 25 FEBRUARY 2020\n",
    "Page: 2\n",
    "The Rules Committee further agreed that the number of\n",
    "Ministers’ Responses be increased from six to seven and that\n",
    "time allowed for Ministers’ Responses be increased from two\n",
    "minutes to three minutes.\n",
    "With that background, I will now take the first item on the\n",
    "Order Paper which is Members’ Statements. Does any member of\n",
    "the ANC wish to make a statement?\n",
    "The CHIEF WHIP OF THE OPPOSITION: Sorry Chair, on a point of\n",
    "order.\n",
    "The HOUSE CHAIRPERSON (Ms M G Boroto): Please take your seat.\n",
    "Yes, what’s your point of order?\n",
    "The CHIEF WHIP OF THE OPPOSITION: I’m terribly sorry to the\n",
    "hon member for disrupting him, but House Chair, I have to just\n",
    "look around this House and I think we need to note that there\n",
    "is one Minister in the House. So, we are about to do\n",
    "ministerial statements but there are not any Ministers in the\n",
    "House to hear them.\n",
    "The HOUSE CHAIRPERSON (Ms M G Boroto): I’m looking around the\n",
    "House and I can say that I see a Minister and two Deputy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e30b89b",
   "metadata": {},
   "source": [
    "Snippet from the XML file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607bd09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "<TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"HansardSA_NA_2020\" xml:lang=\"en\">\n",
    "    <teiHeader>\n",
    "     <fileDesc>\n",
    "       <titleStmt>\n",
    "            <title type=\"main\" xml:lang=\"en\">South African parliamentary Hansard papers</title>\n",
    "            <title type=\"sub\">Minutes of the National Assembly of South Africa</title>\n",
    "            <meeting n=\"uh_1\"</meeting> \n",
    "       </titleStmt>\n",
    "\n",
    "\n",
    "       <editionStmt>\n",
    "            <edition>1.0</edition>\n",
    "       </editionStmt>\n",
    "\n",
    "\n",
    "       <extent> \n",
    "            <measure unit=\"speeches\" quantity=\"3\"\n",
    "            xml:lang=\"en\">3 speeches</measure>\n",
    "            <measure unit=\"words\" quantity=\"2624\"\n",
    "            xml:lang=\"en\">2624 words</measure>\n",
    "       </extent>\n",
    "\n",
    "       <sourceDesc> \n",
    "            <bibl>\n",
    "            <title type=\"main\" xml:lang=\"en\">Proceedings of the National Assembly</title>\n",
    "            <idno type=\"URI\">https://www.parliament.gov.za/storage/app/media/Docs/hansard/ff55b51e-536f-4214-85c2-5f6259579504.pdf</idno>\n",
    "            <date when=\"2020-02-25\">25.02.2020</date>\n",
    "            </bibl>\n",
    "       </sourceDesc>\n",
    "\n",
    "     </fileDesc>\n",
    "\n",
    "     <encodingDesc> \n",
    "        <projectDesc> \n",
    "            <p xml:lang=\"en\">The South African Hansard Papers corpus</ref>\n",
    "              Here we could add a description of our project, if we wanted to.</p>\n",
    "        </projectDesc>\n",
    "\n",
    "     </encodingDesc>\n",
    "\n",
    "     <profileDesc>\n",
    "        <settingDesc>\n",
    "            <setting> \n",
    "                <name type=\"place\">Houses of Parliament</name>\n",
    "                <name type=\"city\">Cape Town</name>\n",
    "                <name type=\"country\" key=\"ZA\">South Africa</name>\n",
    "                <date when=\"2020-02-25\">25.02.2020</>\n",
    "            </setting>\n",
    "        </settingDesc>\n",
    "     </profileDesc>\n",
    "\n",
    "    </teiHeader>\n",
    "\n",
    "    <text>\n",
    "        <body>\n",
    "            <div type=\"debateSection\">\n",
    "             <note type=\"time\">The House met at <time when=\"2020-02-25T014:00:00\">14:00</time>.</note>\n",
    "             <note type=\"narrative\">House Chairperson Ms M G Boroto took the Chair and requested members to observe a moment of silence for prayer or meditation.</note>\n",
    "             <note type=\"speaker\">The HOUSE CHAIRPERSON (Ms M G Boroto):</note>\n",
    "             <u xml:id=\"u1_25-02-2020\" who=\"#houseChairperson\">\n",
    "                <seg xml:lang=\"en\">Hon members, I would\n",
    "                    like to remind you that on 4 December 2019 the House adopted\n",
    "                    the Rules Committee report which introduced a number of\n",
    "                    amendments to our rules. Some of the amendments pertain to the\n",
    "                    sequence of proceedings and Members’ Statements. To facilitate\n",
    "                    sufficient opportunity for Ministers’ Responses to Members’\n",
    "                    Statements, the sequence of proceedings has been amended so\n",
    "                    that Members’ Statements are now at the start of the\n",
    "                    proceedings on days that they are scheduled by the programming\n",
    "                    committee.\n",
    "                    </seg>\n",
    "                <seg xml:lang=\"en\">The Rules Committee further agreed that the number of\n",
    "                    Ministers’ Responses be increased from six to seven and that\n",
    "                    time allowed for Ministers’ Responses be increased from two\n",
    "                    minutes to three minutes.\n",
    "                    With that background, I will now take the first item on the\n",
    "                    Order Paper which is Members’ Statements. Does any member of\n",
    "                    the ANC wish to make a statement?\n",
    "                </seg>\n",
    "             </u>\n",
    "             <note type=\"speaker\">The CHIEF WHIP OF THE OPPOSITION:</note>\n",
    "             <u xml:id=\"u2_25-02-2020\" who=\"#ChiefWhipOfOpposition\"> \n",
    "                <seg xml:lang=\"en\">\n",
    "                    Sorry Chair, on a point of order.\n",
    "                </seg>\n",
    "             </u>\n",
    "             <note type=\"speaker\">The HOUSE CHAIRPERSON (Ms M G Boroto):</note>\n",
    "             <u xml:id=\"u3_25-02-2020\" who=\"#houseChairperson\">\n",
    "                <seg xml:lang=\"en\">\n",
    "                    Please take your seat. Yes, what’s your point of order?\n",
    "                </seg>\n",
    "             </u>\n",
    "             <note type=\"speaker\">The CHIEF WHIP OF THE OPPOSITION:</note>\n",
    "             <u xml:id=\"u4_25-02-2020\" who=\"#ChiefWhipOfOpposition\"> \n",
    "                <seg xml:lang=\"en\">\n",
    "                    I’m terribly sorry to the\n",
    "                    hon member for disrupting him, but House Chair, I have to just\n",
    "                    look around this House and I think we need to note that there\n",
    "                    is one Minister in the House. So, we are about to do\n",
    "                    ministerial statements but there are not any Ministers in the\n",
    "                    House to hear them.\n",
    "                </seg>\n",
    "             </u>\n",
    "             <note type=\"speaker\">The HOUSE CHAIRPERSON (Ms M G Boroto):</note>\n",
    "             <u xml:id=\"u5_25-02-2020\" who=\"#houseChairperson\">\n",
    "                <seg xml:lang=\"en\">\n",
    "                    I’m looking around the\n",
    "                    House and I can say that I see a Minister and two Deputy\n",
    "                <\\seg>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898af954",
   "metadata": {},
   "source": [
    "#### Try 1\n",
    "\n",
    "Additional Instructions: You are a TXT to XML converter. You are given sample corresponding TXT and XML files, with the XML file being the converted version of the TXT file. These files contain the same information and illustrate the xml schema that you will adhere to when converting other TXT files. \n",
    "\n",
    "Knowledge: Snippet from a raw TXT file, the corresponding XML file following the ParlaMint schema.\n",
    "\n",
    "The options Web Borwsing, DALL·E Image Generation and Canvas were disabled. \n",
    "\n",
    "|| **Prompts**                                                                                    |**Additional Files/Information** | **Output**                       |\n",
    "|----|--------------------------------------------------------------------------------------------------|------------------------------------|------------------------------------|\n",
    "|1| Please convert the TXT file I have given you into an XML file following the same schema |Snippets from TXT and XML file, uploaded part 1 of raw TXT data | View {Download}`first XML file<./chapter1_ZA-content/results/custom_gpt/Try1/converted_hansard_25_02_2020.xml>` |\n",
    "|2| Please note that your final result should include the entire content of the TXT file. You have omitted a large part of the original data I gave you.| None | View {Download}`second XML file<./chapter1_ZA-content/results/custom_gpt/Try1/complete_converted_hansard_25_02_2020.xml>`|\n",
    "|3|This is what the XML schema is supposed to look like. Please note that a new \"u\" element is used every time the speaker changes. The speeches are contained within the \"seg\" element. The \"note\" element is used for transcriber's notes.| Correct snippet from XML file | View {Download}`third XML file<./chapter1_ZA-content/results/custom_gpt/Try1/updated_converted_hansard_25_02_2020.xml>` |\n",
    "|4| It is looking much better. Please contain entire speeches in one <seg> element instead of starting a new one for each line break.| None | View {Download}`fourth XML file<./chapter1_ZA-content/results/custom_gpt/Try1/final_updated_converted_hansard_25_02_2020.xml>`  |\n",
    "\n",
    "#### Try 2\n",
    "\n",
    "Additional Instructions: You are a TXT to XML converter. You are given a TXT file which you will convert into a downloadable XML file following the RelaxNG file which you were given.\n",
    "\n",
    "Knowledge: the RNG file used to validate the ParlaMint schema.\n",
    "\n",
    "The options Web Borwsing, DALL·E Image Generation and Canvas were disabled. \n",
    "\n",
    "|| **Prompts**                                                                                    |**Additional Files/Information** | **Output**                       |\n",
    "|----|--------------------------------------------------------------------------------------------------|------------------------------------|------------------------------------|\n",
    "|1| Please convert the TXT file I have given you into an XML file following the same schema |Snippets from TXT and XML file, uploaded part 1 of raw TXT data | View {Download}`first XML file<./chapter1_ZA-content/results/custom_gpt/Try2/converted_hansard.xml>` |\n",
    "|2| Please note that your final result should include the entire content of the TXT file. You have omitted a large part of the original data I gave you.| None | View {Download}`second XML file<./chapter1_ZA-content/results/custom_gpt//Try2/converted_hansard_full.xml>`| \n",
    "|3|Please note that this is what the XML schema is supposed to look like. Please omit the page headers and please include entire utterances within the <seg> element. There is no need to split them up by line breaks.| Correct snippet XML file | View {Download}`third XML file<./chapter1_ZA-content/results/custom_gpt/Try2/converted_hansard_updated.xml>` |\n",
    "|4| You have now stored the entirety of the speeches in the TXT file in one \"u\" element. Please take care to start a new \"u\" element every time the speaker changes and to store their speeches in the \"seg\" element.| None | View {Download}`fourth XML file<./chapter1_ZA-content/results/custom_gpt//Try2/converted_hansard_final.xml>`  |\n",
    "\n",
    "#### Try 3\n",
    "\n",
    "Additional Instructions: You are a TXT to XML converter. You are given sample corresponding TXT and XML files, with the XML file being the converted version of the TXT file. These files contain the same information and illustrate the xml schema that you will adhere to when converting other TXT files. Additionally, you were also given a RelaxNG file which can be used to validate the XML files and illustrates the XML schema you will be following.\n",
    "\n",
    "Knowledge: Snippet from a raw TXT file, the corresponding XML file following the ParlaMint schema, the RNG file used to validate the ParlaMint schema.\n",
    "\n",
    "The options Web Borwsing, DALL·E Image Generation and Canvas were disabled. \n",
    "\n",
    "|| **Prompts**                                                                                    |**Additional Files/Information** | **Output**                       |\n",
    "|----|--------------------------------------------------------------------------------------------------|------------------------------------|------------------------------------|\n",
    "|1| Please convert the TXT file I have given you into an XML file following the same schema |Snippets from TXT and XML file, uploaded part 1 of raw TXT data | View {Download}`first XML file<./chapter1_ZA-content/results/custom_gpt/Try3/converted_hansard.xml>` |\n",
    "|2| Please note that your final result should include the entire content of the TXT file. You have omitted a large part of the raw data. | None | View {Download}`second XML file<./chapter1_ZA-content/results/custom_gpt/Try3/converted_hansard_full.xml>`      |\n",
    "|3|This is an extract of the XML you provided. It is correct. Please keep the same schema when adapting the rest of the TXT file into the XML schema. Note also that there is no need to repeat the page header every time - please omit it. | Correct snippet from previously output XML file | View {Download}`third XML file<./chapter1_ZA-content/results/custom_gpt/Try3/converted_hansard_adapted.xml>` |\n",
    "|4| You have now omitted the entirety of the textual data. Please make sure to include all speeches in your XML. | None | View {Download}`fourth XML file<./chapter1_ZA-content/results/custom_gpt/Try3/converted_hansard_complete.xml>`  |\n",
    "\n",
    "### GPT-4o\n",
    "\n",
    "|| **Prompts**                                                                                    |**Additional Files/Information** | **Output**                       |\n",
    "|----|--------------------------------------------------------------------------------------------------|------------------------------------|------------------------------------|\n",
    "|1| I have a TXT file which needs to be converted into an XML file following a specific XML format, the ParlaMint schema.Here is a snippet of the TXT file: [TXT snippet]. The content of this snippet was converted into an XML schema, which looks like this: [XML snippet]. Please convert the TXT file I have given you into a downloadable XML file following the same schema. I have also given you an .rng file illustrating the schema.|uploaded RelaxNG file, part 1 of raw TXT data | View {Download}`first XML file<./chapter1_ZA-content/results/gpt4o/HansardSA_25_Feb_2020.xml>` |\n",
    "|2|Please note that utterances are stored in the \"seg\" element, with metadata about the utterance stored in the parent element \"u\". A new \"u\" element ist started every time the speaker changes. Please remember the snippet of the XML file I have given you and try again.| None | View {Download}`second XML file<./chapter1_ZA-content/results/gpt4o/HansardSA_25_Feb_2020_refined.xml>`|\n",
    "|3|You started off well, but stopped implementing the schema after a while. Please make sure to carry the schema through the entire document.| None | View {Download}`third XML file<./chapter1_ZA-content/results/gpt4o/HansardSA_25_Feb_2020_consistent.xml>` |\n",
    "|4| Here is a reminder as to how the schema should look: [XML snippet] Please follow this schema through the entire document, which you have failed to do before.| Snippet from gold standard XML file | View {Download}`fourth XML file<./chapter1_ZA-content/results/custom_gpt/Try1/final_updated_converted_hansard_25_02_2020_fully_structured.xml>`  |\n",
    "\n",
    "\n",
    "## Discussion \n",
    "STRUCTURE IT AS AN OVERALL DISCUSSION? DO YOU WANNA COMPARE THE SCORES TO EACH OTHER HERE? \n",
    "\n",
    "An overview of the XML validation results for each LLM can be found {doc}`here <../chapter1_ZA-content/results/xml_validation.xlsx>`\n",
    "\n",
    "### Llama Herd \n",
    "The Llama models failed entirely in producing well-formed XMLs that adhered to the ParlaMint schema. Their outputs were not valid XML files at all, with the structure lacking the basic syntax required for XML validation. As a result, the validation script failed completely, rendering the outputs unusable for any meaningful processing or schema compliance.\n",
    "\n",
    "### Gemini 1.5 Flash \n",
    "Gemini largely failed to produce any output at all in most attempts. On the rare occasions it did generate XML output, it was incorrect both structurally and content-wise, failing to adhere to the ParlaMint schema or accurately represent the input data. However, the XML syntax was mostly correct, with proper tag closures and formatting, demonstrating a basic understanding of XML structure despite the overall inadequacy of the results.\n",
    "\n",
    "### Custom GPT\n",
    "The custom GPTs' attempts at generating XML output exhibited recurring issues. Across all three cutsom GPTs the first outputs were incomplete, omitting much of the content. Later attempts included the entire input but failed to adhere to the ParlaMint XML schema. Speech content was incorrectly nested in \"note\" elements, parts of speeches were contained within the \"speaker\" attribute, metadata such as page numbers and headers were retained as part of the speeches, treanscriber's notes were handled incorrectly, and languages were universally mislabeled as English. Despite prompting, structural issues persisted, such as missing attributes in \"u\" elements, inconsistent line counts, and a failure to follow the example XML's formatting. Across multiple attempts, only minimal improvements were achieved, the most promising of which was the first GPT's final attempt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3146433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "<note type=\"speaker\">The HOUSE CHAIRPERSON (Ms M G Boroto): I’m looking around the</note>\n",
    "        <u xml_id=\"u21_25-02-2020\" who=\"#TheHOUSECHAIRPERSON(MsMGBoroto)\">\n",
    "          <seg xml_lang=\"en\">House and I can say that I see a Minister and two Deputy</seg>\n",
    "          <seg xml_lang=\"en\">UNREVISED HANSARD</seg>\n",
    "          <seg xml_lang=\"en\">NATIONAL ASSEMBLY</seg>\n",
    "          <seg xml_lang=\"en\">TUESDAY, 25 FEBRUARY 2020</seg>\n",
    "          <seg xml_lang=\"en\">Page: 3</seg>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa6258",
   "metadata": {},
   "source": [
    "This seems to indicate that giving the custom GPT the gold standard TXT and XML files was the most successful, though more prompting and more attempts did nothing to improve this final output. \n",
    "\n",
    "Despite the issues with content accuracy and schema adherence, the custom GPTs consistently produced well-formed XMLs. The syntax was valid, with correctly closed tags and mostly proper nesting of elements, even when the content within the elements or their attributes was incorrect. This demonstrates the GPTs' capability to handle the structural requirements of XML, albeit with challenges in adhering to the specificity of the ParlaMint schema.\n",
    "\n",
    "\n",
    "### GPT-4o \n",
    "\n",
    "In its first attempt, GPT-4o produced an XML file that initially adhered to the ParlaMint structure but  became disorganized after 40 lines. Below is an excerpt from the section containing the speeches in this initial attempt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd5ee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "<u who=\"#House met at 14\"><seg xml:lang=\"en\">House Chairperson Ms M G Boroto took the Chair and requested</seg></u>\n",
    "<u who=\"#House met at 14\"><seg xml:lang=\"en\">members to observe a moment of silence for prayer or</seg></u>\n",
    "<u who=\"#House met at 14\"><seg xml:lang=\"en\">meditation.</seg></u>\n",
    "<note type=\"speaker\">HOUSE CHAIRPERSON (Ms M G Boroto)</note>\n",
    "<u who=\"#HOUSE CHAIRPERSON (Ms M G Boroto)\"><seg xml:lang=\"en\">like to remind you that on 4 December 2019 the House adopted</seg></u>\n",
    "<u who=\"#HOUSE CHAIRPERSON (Ms M G Boroto)\"><seg xml:lang=\"en\">the Rules Committee report which introduced a number of</seg></u>\n",
    "<u who=\"#HOUSE CHAIRPERSON (Ms M G Boroto)\"><seg xml:lang=\"en\">amendments to our rules. Some of the amendments pertain to the</seg></u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b1bb6",
   "metadata": {},
   "source": [
    "Although having a \"seg\" element contain a single line of transcription is acceptable, a new \"u\" element should only begin when there is a change in speaker. However, after the initial exchange between the House Chairperson and the Chief Whip of the Majority Party, GPT-4o incorrectly attributed all subsequent utterances to the same speaker, the Minister of Higher Education—an obvious error. The subsequent three outputs showed no improvement. While the initial formatting was acceptable—likely because it copied the provided input snippet—GPT-4o processed the rest of the input into a single \"seg\" element. This approach completely ignored speaker changes and failed to represent them correctly. More broadly, GPT-4o struggled with key tasks: it failed to identify languages, labeling all utterances as English, and neglected to annotate transcriber's notes, such as descriptions of applause or interruptions. \n",
    "\n",
    "Despite these issues, GPT-4o did succeed in producing valid XML files, even if they failed to follow the specific schema required for encoding the input.\n",
    "\n",
    "### Limitations\n",
    "Problems: specific world knowledge that is needed to fill in the metadata, size of context window, computational power/resources. \n",
    "Prompt Engineering on local LLMs (Why it doesn't work for this specific case, why it didn't work for us.) -> the limited context window paired with the large input, the inability to work with unaltered text, computational issues/hardware issues. Batching didn't work.\n",
    "\n",
    "Note for limitations: we do not populate the metadata files, because very specific real world knowledge would be needed, and it is easier and computationally more efficient to populate this metadate with a rule-based approach once the base XML of the speeches themselves are parsed/created by the LLM. \n",
    "Many members of the SA parliament do not have their birth date published online. \n",
    "\n",
    "\n",
    "## Conclusion \n",
    "\n",
    "As pretrained LLMs show difficulties when modelling data into complex formats such as the ParlaMint, further research is necessary on whether tools such as Evaporate or SEED, which are partially rulebased, can be adapted better to the task {cite:p}`aorora_2023, chen_2023`. A different approach could lie in accessing stronger hardware through cloud computing platforms such as google colab to run models such as Jellyfish which are specialized on the task of formatting data {cite:p}`zhang_jellyfish_2024`. It might also be possible to fine-tune a model with a currated dataset from the ParlaMint corpus to create a custom model specifically atuned to formatting data into the ParlaMint schema. Furthermore, different prompting approaches might show more fruitful (see {cite:t}`sahoo_2024` for an overview of prompt engineering techniques).\n",
    "\n",
    "## Bibliography\n",
    "```{bibliography}\n",
    ":style: plain\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "format": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": "0.13",
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "source_map": [
   14,
   70,
   77,
   83,
   95,
   105,
   126,
   132,
   162,
   207,
   261,
   275,
   432,
   447,
   450,
   464,
   525,
   609,
   655,
   659,
   770,
   841,
   849,
   860,
   868
  ],
  "thebe": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}