{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OEB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# File paths\n",
    "input_file = 'OEB.txt'\n",
    "output_tsv_file = 'OEB_combined.tsv'\n",
    "\n",
    "# Book names extracted from the table of contents\n",
    "book_names = [\n",
    "    \"Ruth\", \"Esther\", \"Psalms\", \"Hosea\", \"Joel\", \"Amos\", \"Obadiah\", \"Jonah\", \n",
    "    \"Micah\", \"Nahum\", \"Habakkuk\", \"Zephaniah\", \"Haggai\", \"Zechariah\", \"Malachi\",\n",
    "    \"Matthew\", \"Mark\", \"Luke\", \"John\", \"Acts\", \"Romans\", \"1 Corinthians\", \n",
    "    \"2 Corinthians\", \"Galatians\", \"Ephesians\", \"Philippians\", \"Colossians\", \n",
    "    \"1 Thessalonians\", \"2 Thessalonians\", \"1 Timothy\", \"2 Timothy\", \"Titus\", \n",
    "    \"Philemon\", \"Hebrews\", \"James\", \"1 Peter\", \"2 Peter\", \"1 John\", \"2 John\", \n",
    "    \"3 John\", \"Jude\", \"Revelation\"\n",
    "]\n",
    "\n",
    "# Regex pattern to detect verses\n",
    "verse_pattern = re.compile(r\"\\[(\\d+:\\d+)\\]\\s+(.+?)(?=\\[\\d+:\\d+\\]|$)\")\n",
    "\n",
    "# Data collection\n",
    "combined_data = []\n",
    "current_book = None\n",
    "\n",
    "# Use the order of books from the table of contents to infer context\n",
    "book_index = 0\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Detect verses\n",
    "        for verse_match in verse_pattern.finditer(line):\n",
    "            verse_id, verse_text = verse_match.groups()\n",
    "            chapter_num, verse_num = verse_id.split(\":\")\n",
    "\n",
    "            # Assign current book based on the detected order in the text\n",
    "            if current_book is None or len(combined_data) > 0 and int(chapter_num) == 1 and int(verse_num) == 1:\n",
    "                current_book = book_names[book_index]\n",
    "                book_index += 1\n",
    "\n",
    "            # Collect data\n",
    "            combined_data.append((current_book, chapter_num, verse_num, verse_text.strip()))\n",
    "\n",
    "# Write the data to the TSV file\n",
    "with open(output_tsv_file, 'w', encoding='utf-8') as out_file:\n",
    "    out_file.write(\"Book\\tChapter\\tVerse\\tText\\n\")\n",
    "    for entry in combined_data:\n",
    "        out_file.write(\"\\t\".join(entry) + \"\\n\")\n",
    "\n",
    "print(f\"Extraction complete! {len(combined_data)} verses written to {output_tsv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 31102 entries. Cleaned WEB file saved as WEB_combined.tsv.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# File paths for input and output\n",
    "input_web_file = \"WEB.txt\"\n",
    "output_web_tsv_file = \"WEB_combined.tsv\"\n",
    "\n",
    "# Regex patterns for books, chapters, and verses\n",
    "book_pattern = re.compile(r\"^Book \\d+\\s+(.+)$\")  # Matches lines like \"Book 01 Genesis\"\n",
    "verse_pattern = re.compile(r\"^(\\d{3}):(\\d{3})\\s+(.+)$\")  # Matches lines like \"001:001 Text\"\n",
    "\n",
    "# Function to normalize chapter and verse numbers\n",
    "def normalize_number(value):\n",
    "    return str(int(value))  # Remove leading zeros by converting to integer and back to string\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[\\\"“”]', '', text).strip()  # Remove quotation marks and clean whitespace\n",
    "\n",
    "# Data collection\n",
    "web_data = []\n",
    "current_book = None\n",
    "current_verse = None\n",
    "current_text = []\n",
    "\n",
    "with open(input_web_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Detect book titles\n",
    "        book_match = book_pattern.match(line)\n",
    "        if book_match:\n",
    "            current_book = book_match.group(1).strip()\n",
    "            continue\n",
    "\n",
    "        # Detect verses\n",
    "        verse_match = verse_pattern.match(line)\n",
    "        if verse_match and current_book:\n",
    "            # Save the previous verse if it exists\n",
    "            if current_verse and current_text:\n",
    "                full_text = \" \".join(current_text).strip()\n",
    "                full_text = clean_text(full_text)  # Clean the text\n",
    "                web_data.append((current_book, *current_verse, full_text))\n",
    "\n",
    "            # Start a new verse\n",
    "            chapter_num, verse_num, text = verse_match.groups()\n",
    "            chapter_num = normalize_number(chapter_num)  # Normalize chapter number\n",
    "            verse_num = normalize_number(verse_num)      # Normalize verse number\n",
    "            current_verse = (chapter_num, verse_num)\n",
    "            current_text = [text]\n",
    "        else:\n",
    "            # Accumulate lines for the current verse\n",
    "            current_text.append(line)\n",
    "\n",
    "    # Save the last verse\n",
    "    if current_verse and current_text:\n",
    "        full_text = \" \".join(current_text).strip()\n",
    "        full_text = clean_text(full_text)  # Clean the text\n",
    "        web_data.append((current_book, *current_verse, full_text))\n",
    "\n",
    "# Write data to a single TSV file\n",
    "with open(output_web_tsv_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    out_file.write(\"Book\\tChapter\\tVerse\\tText\\n\")\n",
    "    for entry in web_data:\n",
    "        out_file.write(\"\\t\".join(entry) + \"\\n\")\n",
    "\n",
    "print(f\"Processed {len(web_data)} entries. Cleaned WEB file saved as {output_web_tsv_file}.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KJV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# File paths\n",
    "input_kjv_file = 'KJV.txt'\n",
    "reformatted_kjv_file = 'KJV_reformatted.txt'\n",
    "\n",
    "# Regex to match Chapter:Verse markers\n",
    "verse_marker = re.compile(r\"(\\d+:\\d+)\")\n",
    "\n",
    "# Step 1: Reformat the file to ensure each verse starts on a new line\n",
    "with open(input_kjv_file, 'r', encoding='utf-8') as infile, open(reformatted_kjv_file, 'w', encoding='utf-8') as outfile:\n",
    "    for line in infile:\n",
    "        # Replace inline Chapter:Verse markers with new-line-prefixed markers\n",
    "        reformatted_line = verse_marker.sub(r\"\\n\\1\", line.strip())\n",
    "        outfile.write(reformatted_line + \"\\n\")\n",
    "\n",
    "print(f\"Reformatted file saved as: {reformatted_kjv_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# File paths for input and output\n",
    "input_kjv_file = 'KJV_reformatted.txt'\n",
    "output_ot_file = 'KJV_OT.tsv'\n",
    "output_nt_file = 'KJV_NT.tsv'\n",
    "\n",
    "# Markers for Old and New Testaments\n",
    "ot_marker = \"The Old Testament of the King James Version of the Bible\"\n",
    "nt_marker = \"The New Testament of the King James Bible\"\n",
    "\n",
    "# Regex to match verses\n",
    "verse_pattern = re.compile(r\"(\\d+):(\\d+)\\s+(.+?)$\", re.DOTALL)\n",
    "\n",
    "# Data containers\n",
    "ot_data = []\n",
    "nt_data = []\n",
    "current_testament = None\n",
    "current_book = None\n",
    "current_verse = None\n",
    "current_text = []\n",
    "\n",
    "# Step 1: Read the file line by line\n",
    "with open(input_kjv_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Detect Testament markers\n",
    "        if ot_marker in line:\n",
    "            current_testament = 'OT'\n",
    "            continue\n",
    "        elif nt_marker in line:\n",
    "            current_testament = 'NT'\n",
    "            continue\n",
    "\n",
    "        # Detect book names\n",
    "        if line.startswith(\"The \") and (\"Book\" in line or \"Epistle\" in line):\n",
    "            current_book = line.strip()\n",
    "            continue\n",
    "\n",
    "        # Check for a verse match\n",
    "        verse_match = verse_pattern.match(line)\n",
    "        if verse_match:\n",
    "            # Save the current verse if any\n",
    "            if current_verse and current_text:\n",
    "                full_text = \" \".join(current_text).strip()\n",
    "                if current_testament == 'OT':\n",
    "                    ot_data.append((current_book, current_verse[0], current_verse[1], full_text))\n",
    "                elif current_testament == 'NT':\n",
    "                    nt_data.append((current_book, current_verse[0], current_verse[1], full_text))\n",
    "\n",
    "            # Start a new verse\n",
    "            chapter, verse, text = verse_match.groups()\n",
    "            current_verse = (chapter, verse)\n",
    "            current_text = [text]\n",
    "        else:\n",
    "            # Accumulate text for the current verse\n",
    "            current_text.append(line)\n",
    "\n",
    "    # Save the last verse\n",
    "    if current_verse and current_text:\n",
    "        full_text = \" \".join(current_text).strip()\n",
    "        if current_testament == 'OT':\n",
    "            ot_data.append((current_book, current_verse[0], current_verse[1], full_text))\n",
    "        elif current_testament == 'NT':\n",
    "            nt_data.append((current_book, current_verse[0], current_verse[1], full_text))\n",
    "\n",
    "# Step 2: Write the output\n",
    "with open(output_ot_file, 'w', encoding='utf-8') as ot_file:\n",
    "    ot_file.write(\"Book\\tChapter\\tVerse\\tText\\n\")\n",
    "    for entry in ot_data:\n",
    "        ot_file.write(\"\\t\".join(entry) + \"\\n\")\n",
    "\n",
    "with open(output_nt_file, 'w', encoding='utf-8') as nt_file:\n",
    "    nt_file.write(\"Book\\tChapter\\tVerse\\tText\\n\")\n",
    "    for entry in nt_data:\n",
    "        nt_file.write(\"\\t\".join(entry) + \"\\n\")\n",
    "\n",
    "print(f\"Old Testament verses: {len(ot_data)}\")\n",
    "print(f\"New Testament verses: {len(nt_data)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRB_old_test_books = \"\"\"The Book of Genesis\n",
    " The Book of Exodus\n",
    " The Book of Leviticus\n",
    " The Book of Numbers\n",
    " The Book of Deuteronomy\n",
    " The Book of Josue\n",
    " The Book of Judges\n",
    " The Book of Ruth\n",
    " The First Book of Samuel, otherwise called the First Book of Kings\n",
    " The Second Book of Samuel, otherwise called the Second Book of Kings\n",
    " The Third Book of Kings\n",
    " The Fourth Book of Kings\n",
    " The First Book of Paralipomenon\n",
    " The Second Book of Paralipomenon\n",
    " The First Book of Esdras\n",
    " The Book of Nehemias, which is called the Second of Esdras\n",
    " The Book of Tobias\n",
    " The Book of Judith\n",
    " The Book of Esther\n",
    " The Book of Job\n",
    " The Book of Psalms\n",
    " The Book of Proverbs\n",
    " Ecclesiastes\n",
    " Solomon’s Canticle of Canticles\n",
    " The Book of Wisdom\n",
    " Ecclesiasticus\n",
    " The Prophecy of Isaias\n",
    " The Prophecy of Jeremias\n",
    " The Lamentations of Jeremias\n",
    " The Prophecy of Baruch\n",
    " The Prophecy of Ezechiel\n",
    " The Prophecy of Daniel\n",
    " The Prophecy of Osee\n",
    " The Prophecy of Joel\n",
    " The Prophecy of Amos\n",
    " The Prophecy of Abdias\n",
    " The Prophecy of Jonas\n",
    " The Prophecy of Micheas\n",
    " The Prophecy of Nahum\n",
    " The Prophecy of Habacuc\n",
    " The Prophecy of Sophonias\n",
    " The Prophecy of Aggeus\n",
    " The Prophecy of Zacharias\n",
    " The Prophecy of Malachias\n",
    " The First Book of Machabees\n",
    " The Second Book of Machabees\"\"\".split('\\n ')\n",
    "\n",
    "DRB_new_test_books = \"\"\"The Holy Gospel of Jesus Christ According to St. Matthew\n",
    " The Holy Gospel of Jesus Christ According to St. Mark\n",
    " The Holy Gospel of Jesus Christ According to St. Luke\n",
    " The Holy Gospel of Jesus Christ  According to St. John\n",
    " The Acts of the Apostles\n",
    " The Epistle of St. Paul the Apostle to the Romans\n",
    " The First Epistle of St. Paul to the Corinthians\n",
    " The Second Epistle of St. Paul to the Corinthians\n",
    " The Epistle of St. Paul to the Galatians\n",
    " The Epistle of St. Paul to the Ephesians\n",
    " The Epistle of St. Paul to the Philippians\n",
    " The Epistle of St. Paul to the Colossians\n",
    " The First Epistle of St. Paul to the Thessalonians\n",
    " The Second Epistle of St. Paul to the Thessalonians\n",
    " The First Epistle of St. Paul to Timothy\n",
    " The Second Epistle of St. Paul to Timothy\n",
    " The Epistle of St. Paul to Titus\n",
    " The Epistle of St. Paul to Philemon\n",
    " The Epistle of St. Paul to the Hebrews\n",
    " The Catholic Epistle of St. James the Apostle\n",
    " The First Epistle of St. Peter the Apostle\n",
    " The Second Epistle of St. Peter the Apostle\n",
    " The First Epistle of St. John the Apostle\n",
    " The Second Epistle of St. John the Apostle\n",
    " The Third Epistle of St. John the Apostle\n",
    " The Catholic Epistle of St. Jude the Apostle\n",
    " The Apocalypse of St. John the Apostle\"\"\".split('\\n ')\n",
    "print(DRB_old_test_books)\n",
    "print(DRB_new_test_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"The Book of Leviticus\"\n",
    "print(txt.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# List of book titles (assuming they are already in uppercase)\n",
    "titles = DRB_old_test_books + DRB_new_test_books\n",
    "titles = [element.upper() for element in titles]\n",
    "\n",
    "# Open the input Bible text file\n",
    "with open('DRV.txt', \"r\") as f:\n",
    "    line_counter = 0\n",
    "    book_name = None\n",
    "    book_content = \"\"  # Temporary storage for the current book's content\n",
    "\n",
    "    # Iterate through each line in the file\n",
    "    for line in f:\n",
    "        line_counter += 1\n",
    "        \n",
    "        # Only process lines between 145 and 140345\n",
    "        if line_counter < 145:\n",
    "            continue  # Skip lines before 145\n",
    "        if line_counter > 140345:\n",
    "            break  # Stop processing after line 140345\n",
    "\n",
    "        line = line.strip()  # Remove leading and trailing whitespace\n",
    "        \n",
    "        # Check if the line contains a book title\n",
    "        for title in titles:\n",
    "            if title in line:  # If a book title is found\n",
    "                if book_name:  # Process the previous book if it exists\n",
    "                    # Save the content of the previous book into a text file\n",
    "                    with open(f'DRB_{book_name}.txt', 'w') as book_file:\n",
    "                        book_file.write(book_content)\n",
    "                \n",
    "                # Set the new book name and reset content for the new book\n",
    "                book_name = title\n",
    "                book_content = \"\"  # Reset content for the next book\n",
    "                break  # Stop checking for other titles once the current one is found\n",
    "\n",
    "        # Append the current line to the book's content (if book_name is set)\n",
    "        if book_name:\n",
    "            book_content += line + \"\\n\"  # Add newline between lines of the book\n",
    "\n",
    "    # Save the last book's content after processing all lines\n",
    "    if book_name and book_content:\n",
    "        with open(f'DRB_{book_name}.txt', 'w') as book_file:\n",
    "            book_file.write(book_content)\n",
    "\n",
    "print(\"Books have been saved to separate text files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save unique book titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths for the input TSV files\n",
    "file_paths = {\n",
    "    \"DRB\": \"DRB_preprocessed_columns.tsv\",\n",
    "    \"KJV_NT\": \"KJV_NT.tsv\",\n",
    "    \"KJV_OT\": \"KJV_OT.tsv\",\n",
    "    \"OEB\": \"OEB_combined.tsv\",\n",
    "    \"WEB\": \"WEB_combined.tsv\"\n",
    "}\n",
    "\n",
    "# Dictionary to store book titles for each version\n",
    "titles_dict = {}\n",
    "\n",
    "# Extract unique book titles from each file and store in the dictionary\n",
    "for version, path in file_paths.items():\n",
    "    try:\n",
    "        # Load the TSV file\n",
    "        df = pd.read_csv(path, sep=\"\\t\")\n",
    "        \n",
    "        # Extract unique book titles and normalize\n",
    "        books = sorted(df[\"Book\"].str.strip().str.lower().unique())\n",
    "        \n",
    "        # Add to dictionary\n",
    "        titles_dict[version] = books\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {version}: {e}\")\n",
    "        titles_dict[version] = []\n",
    "\n",
    "# Create a DataFrame with unique book titles as rows and versions as columns\n",
    "unique_titles_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in titles_dict.items()]))\n",
    "\n",
    "# Save the table to a CSV file\n",
    "output_path = \"unique_book_titles_normalized.csv\"\n",
    "unique_titles_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Unique book titles have been normalized and saved to {output_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the two KJV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths for KJV Old and New Testament\n",
    "kjv_ot_path = \"KJV_OT.tsv\"\n",
    "kjv_nt_path = \"KJV_NT.tsv\"\n",
    "combined_kjv_path = \"KJV_combined.tsv\"\n",
    "\n",
    "# Load the Old and New Testament data\n",
    "kjv_ot = pd.read_csv(kjv_ot_path, sep=\"\\t\")\n",
    "kjv_nt = pd.read_csv(kjv_nt_path, sep=\"\\t\")\n",
    "\n",
    "# Combine the datasets\n",
    "kjv_combined = pd.concat([kjv_ot, kjv_nt]).drop_duplicates()\n",
    "\n",
    "# Save the combined dataset\n",
    "kjv_combined.to_csv(combined_kjv_path, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"Combined KJV saved to {combined_kjv_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths for the input TSV files\n",
    "file_paths = {\n",
    "    \"DRB\": \"DRB_preprocessed_columns.tsv\",\n",
    "    \"KJV_combined\": \"KJV_combined.tsv\",\n",
    "    \"OEB\": \"OEB_combined.tsv\",\n",
    "    \"WEB\": \"WEB_combined.tsv\"\n",
    "}\n",
    "\n",
    "# Function to extract only words (no numbers) from a title\n",
    "def extract_words(title):\n",
    "    tokens = title.lower().split()  # Split into tokens\n",
    "    return set(token for token in tokens if not token.isdigit())  # Exclude numbers\n",
    "\n",
    "# Load and normalize titles from all files\n",
    "titles_dict = {}\n",
    "for version, path in file_paths.items():\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=\"\\t\")\n",
    "        # Extract unique titles and normalize\n",
    "        titles_dict[version] = sorted(df[\"Book\"].str.strip().str.lower().unique())\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {version}: {e}\")\n",
    "        titles_dict[version] = []\n",
    "\n",
    "# Debug: Print all titles for inspection\n",
    "for version, titles in titles_dict.items():\n",
    "    print(f\"\\n{version} Titles ({len(titles)}):\")\n",
    "    print(titles)\n",
    "\n",
    "# Tokenize the OEB titles\n",
    "oeb_titles = titles_dict[\"OEB\"]\n",
    "oeb_words = {title: extract_words(title) for title in oeb_titles}\n",
    "\n",
    "# Debug: Print tokenized OEB titles\n",
    "print(\"\\nTokenized OEB Titles:\")\n",
    "print(oeb_words)\n",
    "\n",
    "# Compare OEB titles with all other versions for exact word matches\n",
    "results = []\n",
    "\n",
    "for oeb_title, oeb_words_set in oeb_words.items():\n",
    "    print(f\"\\nOEB Title: {oeb_title}\")\n",
    "    print(f\"Tokens: {oeb_words_set}\")\n",
    "    matched_versions = []\n",
    "    matched_titles = []\n",
    "    for version, titles in titles_dict.items():\n",
    "        if version == \"OEB\":\n",
    "            continue\n",
    "        for title in titles:\n",
    "            title_words = extract_words(title)\n",
    "            if oeb_words_set & title_words:  # Intersection of words\n",
    "                matched_versions.append(version)\n",
    "                matched_titles.append((version, title))\n",
    "                print(f\"Matched {version}: {title} (Tokens: {title_words})\")\n",
    "    # Filter titles matching all 4 versions\n",
    "    if len(set(matched_versions)) >= 3:\n",
    "        print(f\"OEB Title '{oeb_title}' matches all 4 versions.\")\n",
    "        for version, matched_title in matched_titles:\n",
    "            results.append({\n",
    "                \"OEB Title\": oeb_title,\n",
    "                \"Matched Version\": version,\n",
    "                \"Matched Title\": matched_title,\n",
    "                \"Matched Words\": \", \".join(oeb_words_set & extract_words(matched_title))  # Common words\n",
    "            })\n",
    "\n",
    "# Convert the results to a DataFrame for better readability\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "output_path = \"oeb_all_versions_matches_debugged.csv\"\n",
    "results_df.to_csv(output_path, index=False)\n",
    "\n",
    "results_df\n",
    "print(f\"\\nMatching titles across all 4 versions saved to {output_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths for the input TSV files\n",
    "file_paths = {\n",
    "    \"DRB\": \"DRB_preprocessed_columns.tsv\",\n",
    "    \"KJV\": \"KJV_combined.tsv\",\n",
    "    \"OEB\": \"OEB_combined.tsv\",\n",
    "    \"WEB\": \"WEB_combined.tsv\"\n",
    "}\n",
    "\n",
    "# Function to normalize a title (convert ordinals and clean)\n",
    "def normalize_title(title):\n",
    "    # Replace numbers with ordinals\n",
    "    title = re.sub(r'\\b1\\b', 'first', title, flags=re.IGNORECASE)\n",
    "    title = re.sub(r'\\b2\\b', 'second', title, flags=re.IGNORECASE)\n",
    "    title = re.sub(r'\\b3\\b', 'third', title, flags=re.IGNORECASE)\n",
    "    # Clean title (standardize phrases and lowercase)\n",
    "    title = re.sub(r'\\bthe\\b', '', title, flags=re.IGNORECASE)  # Remove \"the\"\n",
    "    title = re.sub(r'\\bepistle of\\b', 'epistle', title, flags=re.IGNORECASE)\n",
    "    title = title.lower().strip()\n",
    "    return title\n",
    "\n",
    "# Load and normalize titles for all versions\n",
    "aligned_titles_dict = {}\n",
    "for version, path in file_paths.items():\n",
    "    try:\n",
    "        # Load the file\n",
    "        df = pd.read_csv(path, sep=\"\\t\")\n",
    "        # Extract unique titles and normalize them\n",
    "        unique_titles = df[\"Book\"].unique()\n",
    "        normalized_titles = [normalize_title(title) for title in unique_titles]\n",
    "        aligned_titles_dict[version] = normalized_titles\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {version}: {e}\")\n",
    "        aligned_titles_dict[version] = []\n",
    "\n",
    "# Create a mapping of aligned titles\n",
    "aligned_mapping = []\n",
    "for oeb_title in set(aligned_titles_dict[\"OEB\"]):  # Use OEB as the base\n",
    "    oeb_tokens = set(oeb_title.split())  # Tokenize the OEB title\n",
    "    row = {\"OEB\": oeb_title}\n",
    "    for version, titles in aligned_titles_dict.items():\n",
    "        if version == \"OEB\":\n",
    "            continue\n",
    "        # Find matching titles that contain all OEB tokens\n",
    "        matches = [\n",
    "            title for title in titles\n",
    "            if oeb_tokens.issubset(set(title.split()))  # Check if all OEB tokens exist\n",
    "        ]\n",
    "        row[version] = matches[0] if matches else None  # Take the first match or None\n",
    "    aligned_mapping.append(row)\n",
    "\n",
    "# Convert the mapping into a DataFrame\n",
    "aligned_df = pd.DataFrame(aligned_mapping)\n",
    "\n",
    "# Save the aligned DataFrame to a CSV file for review\n",
    "aligned_df.to_csv(\"aligned_book_titles.csv\", index=False)\n",
    "\n",
    "# Print a sample of the DataFrame for verification\n",
    "print(\"Aligned Book Titles Mapping:\")\n",
    "print(aligned_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align all titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprehensive Aligned Book Titles with N/A (Sorted):\n",
      "           OEB                              DRB  \\\n",
      "46        acts                acts of  apostles   \n",
      "94        amos                 prophecy of amos   \n",
      "13  colossians  epistle st. paul to  colossians   \n",
      "78   ephesians   epistle st. paul to  ephesians   \n",
      "74      esther                   book of esther   \n",
      "\n",
      "                                     KJV         WEB  \n",
      "46                                   N/A        acts  \n",
      "94                                   N/A        amos  \n",
      "13  epistle paul  apostle to  colossians  colossians  \n",
      "78   epistle paul  apostle to  ephesians   ephesians  \n",
      "74                        book of esther      esther  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths for the input TSV files\n",
    "file_paths = {\n",
    "    \"DRB\": \"DRB_preprocessed_columns.tsv\",\n",
    "    \"KJV\": \"KJV_combined.tsv\",\n",
    "    \"OEB\": \"OEB_combined.tsv\",\n",
    "    \"WEB\": \"WEB_combined.tsv\"\n",
    "}\n",
    "\n",
    "# Function to normalize a title (convert ordinals and clean)\n",
    "def normalize_title(title):\n",
    "    # Replace numbers with ordinals\n",
    "    title = re.sub(r'\\b1\\b', 'first', title, flags=re.IGNORECASE)\n",
    "    title = re.sub(r'\\b2\\b', 'second', title, flags=re.IGNORECASE)\n",
    "    title = re.sub(r'\\b3\\b', 'third', title, flags=re.IGNORECASE)\n",
    "    # Clean title (standardize phrases and lowercase)\n",
    "    title = re.sub(r'\\bthe\\b', '', title, flags=re.IGNORECASE)  # Remove \"the\"\n",
    "    title = re.sub(r'\\bepistle of\\b', 'epistle', title, flags=re.IGNORECASE)\n",
    "    title = title.lower().strip()\n",
    "    return title\n",
    "\n",
    "# Load and normalize titles for all versions\n",
    "normalized_titles_dict = {}\n",
    "for version, path in file_paths.items():\n",
    "    try:\n",
    "        # Load the file\n",
    "        df = pd.read_csv(path, sep=\"\\t\")\n",
    "        # Extract unique titles and normalize them\n",
    "        unique_titles = df[\"Book\"].unique()\n",
    "        normalized_titles = [normalize_title(title) for title in unique_titles]\n",
    "        normalized_titles_dict[version] = normalized_titles\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {version}: {e}\")\n",
    "        normalized_titles_dict[version] = []\n",
    "\n",
    "# Combine all titles with OEB as the baseline\n",
    "all_titles = set(normalized_titles_dict[\"OEB\"])  # Start with OEB titles\n",
    "for version, titles in normalized_titles_dict.items():\n",
    "    if version != \"OEB\":\n",
    "        all_titles.update(titles)  # Add unique titles from other versions\n",
    "\n",
    "# Create a comprehensive alignment table\n",
    "aligned_mapping = []\n",
    "for title in all_titles:  # Loop through every unique title\n",
    "    row = {\"OEB\": \"N/A\", \"DRB\": \"N/A\", \"KJV\": \"N/A\", \"WEB\": \"N/A\"}  # Initialize with N/A\n",
    "    for version, titles in normalized_titles_dict.items():\n",
    "        # Check if the title exists in this version\n",
    "        matches = [t for t in titles if set(title.split()) <= set(t.split())]\n",
    "        row[version] = matches[0] if matches else \"N/A\"  # Add the matched title or N/A\n",
    "    aligned_mapping.append(row)\n",
    "\n",
    "# Convert the mapping into a DataFrame\n",
    "aligned_df = pd.DataFrame(aligned_mapping)\n",
    "\n",
    "# Add a helper column to explicitly handle N/A sorting\n",
    "aligned_df[\"OEB_Sort_Key\"] = aligned_df[\"OEB\"].apply(lambda x: \"zzz\" if x == \"N/A\" else x)\n",
    "\n",
    "# Sort the DataFrame alphabetically by OEB, placing N/A values last\n",
    "aligned_df = aligned_df.sort_values(by=\"OEB_Sort_Key\").drop(columns=[\"OEB_Sort_Key\"])\n",
    "\n",
    "# Save the aligned DataFrame to a CSV file\n",
    "aligned_df.to_csv(\"aligned_book_titles_with_na_sorted.csv\", index=False)\n",
    "\n",
    "# Print a sample of the sorted DataFrame for verification\n",
    "print(\"Comprehensive Aligned Book Titles with N/A (Sorted):\")\n",
    "print(aligned_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add index to kept book titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File with index added saved to aligned_book_titles_with_index.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path for the input file\n",
    "input_file = \"aligned_book_titles.csv\"\n",
    "output_file = \"aligned_book_titles_with_index.csv\"\n",
    "\n",
    "# Load the file\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Add an index column starting from 1\n",
    "df.index = range(1, len(df) + 1)\n",
    "df.index.name = \"Index\"  # Name the index column (optional)\n",
    "\n",
    "# Save the updated DataFrame\n",
    "df.to_csv(output_file, index=True)\n",
    "\n",
    "print(f\"File with index added saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter all versions based on common book titles separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data for OEB saved to OEB_filtered_with_index.tsv. Rows: 9171\n",
      "Filtered data for DRB saved to DRB_filtered_with_index.tsv. Rows: 9447\n",
      "Filtered data for KJV saved to KJV_filtered_with_index.tsv. Rows: 2681\n",
      "Filtered data for WEB saved to WEB_filtered_with_index.tsv. Rows: 9170\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "aligned_file = \"aligned_book_titles_with_index.csv\"\n",
    "file_paths = {\n",
    "    \"OEB\": \"OEB_combined.tsv\",\n",
    "    \"DRB\": \"DRB_preprocessed_columns.tsv\",\n",
    "    \"KJV\": \"KJV_combined.tsv\",\n",
    "    \"WEB\": \"WEB_combined.tsv\"\n",
    "}\n",
    "\n",
    "# Function to normalize titles (same as used for alignment)\n",
    "def normalize_title(title):\n",
    "    title = re.sub(r'\\b1\\b', 'first', title, flags=re.IGNORECASE)\n",
    "    title = re.sub(r'\\b2\\b', 'second', title, flags=re.IGNORECASE)\n",
    "    title = re.sub(r'\\b3\\b', 'third', title, flags=re.IGNORECASE)\n",
    "    title = re.sub(r'\\bthe\\b', '', title, flags=re.IGNORECASE)\n",
    "    title = re.sub(r'\\bepistle of\\b', 'epistle', title, flags=re.IGNORECASE)\n",
    "    return title.lower().strip()\n",
    "\n",
    "# Step 1: Load the aligned titles file\n",
    "aligned_df = pd.read_csv(aligned_file)\n",
    "\n",
    "# Step 2: Create a mapping of normalized titles to actual titles for each version\n",
    "title_mappings = {}\n",
    "for version, path in file_paths.items():\n",
    "    try:\n",
    "        # Load the TSV file\n",
    "        df = pd.read_csv(path, sep=\"\\t\")\n",
    "        # Normalize the titles and create a mapping\n",
    "        df[\"Normalized Book\"] = df[\"Book\"].apply(normalize_title)\n",
    "        title_mappings[version] = dict(zip(df[\"Normalized Book\"], df[\"Book\"]))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {version}: {e}\")\n",
    "        title_mappings[version] = {}\n",
    "\n",
    "# Step 3: Filter each version based on the aligned titles\n",
    "for version, path in file_paths.items():\n",
    "    try:\n",
    "        # Load the respective TSV file\n",
    "        version_df = pd.read_csv(path, sep=\"\\t\")\n",
    "        \n",
    "        # Prepare the filtered DataFrame\n",
    "        filtered_rows = []\n",
    "\n",
    "        # Iterate over each row in the alignment file\n",
    "        for _, row in aligned_df.iterrows():\n",
    "            index = row[\"Index\"]  # Get the index\n",
    "            aligned_title = row[version]  # Get the aligned title for this version\n",
    "            \n",
    "            # Find the corresponding actual title from the mapping\n",
    "            actual_title = title_mappings[version].get(normalize_title(aligned_title), None)\n",
    "            if actual_title:\n",
    "                # Filter rows matching the actual title\n",
    "                matched_rows = version_df[version_df[\"Book\"] == actual_title].copy()\n",
    "                matched_rows[\"Index\"] = index  # Add the index to the matched rows\n",
    "                filtered_rows.append(matched_rows)\n",
    "\n",
    "        # Combine all filtered rows\n",
    "        if filtered_rows:\n",
    "            filtered_version_df = pd.concat(filtered_rows, ignore_index=True)\n",
    "        else:\n",
    "            filtered_version_df = pd.DataFrame(columns=version_df.columns.tolist() + [\"Index\"])\n",
    "\n",
    "        # Save the filtered DataFrame\n",
    "        output_file = f\"{version}_filtered_with_index.tsv\"\n",
    "        filtered_version_df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "        print(f\"Filtered data for {version} saved to {output_file}. Rows: {len(filtered_version_df)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {version}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## align everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file grouped by Book saved to: aligned_bible_data.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths for the filtered TSV files\n",
    "file_paths = {\n",
    "    \"DRB\": \"DRB_filtered_with_index.tsv\",\n",
    "    \"KJV\": \"KJV_filtered_with_index.tsv\",\n",
    "    \"OEB\": \"OEB_filtered_with_index.tsv\",\n",
    "    \"WEB\": \"WEB_filtered_with_index.tsv\"\n",
    "}\n",
    "\n",
    "# List to hold DataFrames for each version\n",
    "dataframes = []\n",
    "\n",
    "# Add a \"Version\" column to each TSV file and load it\n",
    "for version, path in file_paths.items():\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=\"\\t\")\n",
    "        df[\"Version\"] = version  # Add version column\n",
    "        dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {version}: {e}\")\n",
    "\n",
    "# Combine all DataFrames into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Ensure Chapter and Verse columns are integers for proper sorting\n",
    "combined_df[\"Chapter\"] = combined_df[\"Chapter\"].astype(int)\n",
    "combined_df[\"Verse\"] = combined_df[\"Verse\"].astype(int)\n",
    "\n",
    "# Sort by Book, then by Version, Chapter, and Verse\n",
    "combined_df = combined_df.sort_values(by=[\"Book\", \"Version\", \"Chapter\", \"Verse\"])\n",
    "\n",
    "# Reorder columns for readability\n",
    "column_order = [\"Index\", \"Version\", \"Book\", \"Chapter\", \"Verse\", \"Text\"]\n",
    "combined_df = combined_df[column_order]\n",
    "\n",
    "# Save to a new TSV file\n",
    "output_file = \"aligned_bible_data.tsv\"\n",
    "combined_df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"Combined file grouped by Book saved to: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sorted alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted combined data saved to: sorted_aligned_bible_data.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the combined data\n",
    "combined_file = \"aligned_bible_data.tsv\"\n",
    "df = pd.read_csv(combined_file, sep=\"\\t\")\n",
    "\n",
    "# Ensure Chapter and Verse are treated as integers\n",
    "df[\"Chapter\"] = df[\"Chapter\"].astype(int, errors=\"ignore\")\n",
    "df[\"Verse\"] = df[\"Verse\"].astype(int, errors=\"ignore\")\n",
    "\n",
    "# Sort the DataFrame\n",
    "df = df.sort_values(\n",
    "    by=[\"Index\", \"Version\", \"Book\", \"Chapter\", \"Verse\"],  # Sorting keys\n",
    ")\n",
    "\n",
    "# Save to a new file\n",
    "output_file = \"sorted_aligned_bible_data.tsv\"\n",
    "df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"Sorted combined data saved to: {output_file}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
