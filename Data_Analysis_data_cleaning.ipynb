{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of KJV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "input_file = \"KJV_reformatted.txt\"\n",
    "output_file = \"KJV_extracted.tsv\"\n",
    "\n",
    "# List of books\n",
    "books = [\n",
    "    \"The First Book of Moses: Called Genesis\", \"The Second Book of Moses: Called Exodus\",\n",
    "    \"The Third Book of Moses: Called Leviticus\", \"The Fourth Book of Moses: Called Numbers\",\n",
    "    \"The Fifth Book of Moses: Called Deuteronomy\", \"The Book of Joshua\", \"The Book of Judges\",\n",
    "    \"The Book of Ruth\", \"The First Book of Samuel\", \"The Second Book of Samuel\",\n",
    "    \"The First Book of the Kings\", \"The Second Book of the Kings\", \"The First Book of the Chronicles\",\n",
    "    \"The Second Book of the Chronicles\", \"Ezra\", \"The Book of Nehemiah\", \"The Book of Esther\",\n",
    "    \"The Book of Job\", \"The Book of Psalms\", \"The Proverbs\", \"Ecclesiastes\",\n",
    "    \"The Song of Solomon\", \"The Book of the Prophet Isaiah\", \"The Book of the Prophet Jeremiah\",\n",
    "    \"The Lamentations of Jeremiah\", \"The Book of the Prophet Ezekiel\", \"The Book of Daniel\",\n",
    "    \"Hosea\", \"Joel\", \"Amos\", \"Obadiah\", \"Jonah\", \"Micah\", \"Nahum\", \"Habakkuk\", \"Zephaniah\",\n",
    "    \"Haggai\", \"Zechariah\", \"Malachi\", \"The Gospel According to Saint Matthew\",\n",
    "    \"The Gospel According to Saint Mark\", \"The Gospel According to Saint Luke\",\n",
    "    \"The Gospel According to Saint John\", \"The Acts of the Apostles\",\n",
    "    \"The Epistle of Paul the Apostle to the Romans\", \"The First Epistle of Paul the Apostle to the Corinthians\",\n",
    "    \"The Second Epistle of Paul the Apostle to the Corinthians\",\n",
    "    \"The Epistle of Paul the Apostle to the Galatians\", \"The Epistle of Paul the Apostle to the Ephesians\",\n",
    "    \"The Epistle of Paul the Apostle to the Philippians\", \"The Epistle of Paul the Apostle to the Colossians\",\n",
    "    \"The First Epistle of Paul the Apostle to the Thessalonians\",\n",
    "    \"The Second Epistle of Paul the Apostle to the Thessalonians\",\n",
    "    \"The First Epistle of Paul the Apostle to Timothy\", \"The Second Epistle of Paul the Apostle to Timothy\",\n",
    "    \"The Epistle of Paul the Apostle to Titus\", \"The Epistle of Paul the Apostle to Philemon\",\n",
    "    \"The Epistle of Paul the Apostle to the Hebrews\", \"The General Epistle of James\",\n",
    "    \"The First Epistle General of Peter\", \"The Second General Epistle of Peter\",\n",
    "    \"The First Epistle General of John\", \"The Second Epistle General of John\",\n",
    "    \"The Third Epistle General of John\", \"The General Epistle of Jude\",\n",
    "    \"The Revelation of Saint John the Divine\"\n",
    "]\n",
    "\n",
    "# Regular expression patterns\n",
    "book_pattern = re.compile(r\"^(\" + \"|\".join(re.escape(book) for book in books) + r\")$\")\n",
    "verse_pattern = re.compile(r\"^(\\d+):(\\d+)\\s(.+)$\")\n",
    "\n",
    "# Initialize variables\n",
    "current_book = None\n",
    "current_chapter = None\n",
    "current_verse = None\n",
    "current_text = []\n",
    "data = []\n",
    "\n",
    "# Read the file and process\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Check for book title\n",
    "        if book_pattern.match(line):\n",
    "            # If there is ongoing text, save it before switching books\n",
    "            if current_book and current_text:\n",
    "                data.append([current_book, current_chapter, current_verse, \" \".join(current_text)])\n",
    "            current_book = line\n",
    "            current_chapter = None\n",
    "            current_verse = None\n",
    "            current_text = []\n",
    "            continue\n",
    "\n",
    "        # Check for chapter and verse\n",
    "        match = verse_pattern.match(line)\n",
    "        if match:\n",
    "            # If there is ongoing text, save it before starting a new verse\n",
    "            if current_book and current_text:\n",
    "                data.append([current_book, current_chapter, current_verse, \" \".join(current_text)])\n",
    "            current_chapter, current_verse, text = match.groups()\n",
    "            current_chapter = int(current_chapter)\n",
    "            current_verse = int(current_verse)\n",
    "            current_text = [text]\n",
    "        elif current_text:\n",
    "            # Append additional lines to the current verse\n",
    "            current_text.append(line)\n",
    "\n",
    "# Save the last ongoing verse\n",
    "if current_book and current_text:\n",
    "    data.append([current_book, current_chapter, current_verse, \" \".join(current_text)])\n",
    "\n",
    "# Create DataFrame and save\n",
    "df = pd.DataFrame(data, columns=[\"Book\", \"Chapter\", \"Verse\", \"Text\"])\n",
    "df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "print(f\"Extraction complete. Data saved to {output_file}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the extracted file\n",
    "df = pd.read_csv(\"KJV_extracted.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Count unique books\n",
    "unique_books = df[\"Book\"].nunique()\n",
    "print(f\"Unique books extracted: {unique_books}\")\n",
    "\n",
    "# Compare with expected list\n",
    "missing_books = [book for book in books if book not in df[\"Book\"].unique()]\n",
    "print(f\"Missing books: {missing_books}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OEB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# File paths\n",
    "input_file = 'OEB.txt'\n",
    "output_tsv_file = 'OEB_combined.tsv'\n",
    "\n",
    "# Book names extracted from the table of contents\n",
    "book_names = [\n",
    "    \"Ruth\", \"Esther\", \"Psalms\", \"Hosea\", \"Joel\", \"Amos\", \"Obadiah\", \"Jonah\", \n",
    "    \"Micah\", \"Nahum\", \"Habakkuk\", \"Zephaniah\", \"Haggai\", \"Zechariah\", \"Malachi\",\n",
    "    \"Matthew\", \"Mark\", \"Luke\", \"John\", \"Acts\", \"Romans\", \"1 Corinthians\", \n",
    "    \"2 Corinthians\", \"Galatians\", \"Ephesians\", \"Philippians\", \"Colossians\", \n",
    "    \"1 Thessalonians\", \"2 Thessalonians\", \"1 Timothy\", \"2 Timothy\", \"Titus\", \n",
    "    \"Philemon\", \"Hebrews\", \"James\", \"1 Peter\", \"2 Peter\", \"1 John\", \"2 John\", \n",
    "    \"3 John\", \"Jude\", \"Revelation\"\n",
    "]\n",
    "\n",
    "# Regex pattern to detect verses\n",
    "verse_pattern = re.compile(r\"\\[(\\d+:\\d+)\\]\\s+(.+?)(?=\\[\\d+:\\d+\\]|$)\")\n",
    "\n",
    "# Data collection\n",
    "combined_data = []\n",
    "current_book = None\n",
    "\n",
    "# Use the order of books from the table of contents to infer context\n",
    "book_index = 0\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Detect verses\n",
    "        for verse_match in verse_pattern.finditer(line):\n",
    "            verse_id, verse_text = verse_match.groups()\n",
    "            chapter_num, verse_num = verse_id.split(\":\")\n",
    "\n",
    "            # Assign current book based on the detected order in the text\n",
    "            if current_book is None or len(combined_data) > 0 and int(chapter_num) == 1 and int(verse_num) == 1:\n",
    "                current_book = book_names[book_index]\n",
    "                book_index += 1\n",
    "\n",
    "            # Collect data\n",
    "            combined_data.append((current_book, chapter_num, verse_num, verse_text.strip()))\n",
    "\n",
    "# Write the data to the TSV file\n",
    "with open(output_tsv_file, 'w', encoding='utf-8') as out_file:\n",
    "    out_file.write(\"Book\\tChapter\\tVerse\\tText\\n\")\n",
    "    for entry in combined_data:\n",
    "        out_file.write(\"\\t\".join(entry) + \"\\n\")\n",
    "\n",
    "print(f\"Extraction complete! {len(combined_data)} verses written to {output_tsv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# File paths for input and output\n",
    "input_web_file = \"WEB.txt\"\n",
    "output_web_tsv_file = \"WEB_combined.tsv\"\n",
    "\n",
    "# Regex patterns for books, chapters, and verses\n",
    "book_pattern = re.compile(r\"^Book \\d+\\s+(.+)$\")  # Matches lines like \"Book 01 Genesis\"\n",
    "verse_pattern = re.compile(r\"^(\\d{3}):(\\d{3})\\s+(.+)$\")  # Matches lines like \"001:001 Text\"\n",
    "\n",
    "# Function to normalize chapter and verse numbers\n",
    "def normalize_number(value):\n",
    "    return str(int(value))  # Remove leading zeros by converting to integer and back to string\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[\\\"“”]', '', text).strip()  # Remove quotation marks and clean whitespace\n",
    "\n",
    "# Data collection\n",
    "web_data = []\n",
    "current_book = None\n",
    "current_verse = None\n",
    "current_text = []\n",
    "\n",
    "with open(input_web_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Detect book titles\n",
    "        book_match = book_pattern.match(line)\n",
    "        if book_match:\n",
    "            current_book = book_match.group(1).strip()\n",
    "            continue\n",
    "\n",
    "        # Detect verses\n",
    "        verse_match = verse_pattern.match(line)\n",
    "        if verse_match and current_book:\n",
    "            # Save the previous verse if it exists\n",
    "            if current_verse and current_text:\n",
    "                full_text = \" \".join(current_text).strip()\n",
    "                full_text = clean_text(full_text)  # Clean the text\n",
    "                web_data.append((current_book, *current_verse, full_text))\n",
    "\n",
    "            # Start a new verse\n",
    "            chapter_num, verse_num, text = verse_match.groups()\n",
    "            chapter_num = normalize_number(chapter_num)  # Normalize chapter number\n",
    "            verse_num = normalize_number(verse_num)      # Normalize verse number\n",
    "            current_verse = (chapter_num, verse_num)\n",
    "            current_text = [text]\n",
    "        else:\n",
    "            # Accumulate lines for the current verse\n",
    "            current_text.append(line)\n",
    "\n",
    "    # Save the last verse\n",
    "    if current_verse and current_text:\n",
    "        full_text = \" \".join(current_text).strip()\n",
    "        full_text = clean_text(full_text)  # Clean the text\n",
    "        web_data.append((current_book, *current_verse, full_text))\n",
    "\n",
    "# Write data to a single TSV file\n",
    "with open(output_web_tsv_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    out_file.write(\"Book\\tChapter\\tVerse\\tText\\n\")\n",
    "    for entry in web_data:\n",
    "        out_file.write(\"\\t\".join(entry) + \"\\n\")\n",
    "\n",
    "print(f\"Processed {len(web_data)} entries. Cleaned WEB file saved as {output_web_tsv_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRB_old_test_books = \"\"\"The Book of Genesis\n",
    " The Book of Exodus\n",
    " The Book of Leviticus\n",
    " The Book of Numbers\n",
    " The Book of Deuteronomy\n",
    " The Book of Josue\n",
    " The Book of Judges\n",
    " The Book of Ruth\n",
    " The First Book of Samuel, otherwise called the First Book of Kings\n",
    " The Second Book of Samuel, otherwise called the Second Book of Kings\n",
    " The Third Book of Kings\n",
    " The Fourth Book of Kings\n",
    " The First Book of Paralipomenon\n",
    " The Second Book of Paralipomenon\n",
    " The First Book of Esdras\n",
    " The Book of Nehemias, which is called the Second of Esdras\n",
    " The Book of Tobias\n",
    " The Book of Judith\n",
    " The Book of Esther\n",
    " The Book of Job\n",
    " The Book of Psalms\n",
    " The Book of Proverbs\n",
    " Ecclesiastes\n",
    " Solomon’s Canticle of Canticles\n",
    " The Book of Wisdom\n",
    " Ecclesiasticus\n",
    " The Prophecy of Isaias\n",
    " The Prophecy of Jeremias\n",
    " The Lamentations of Jeremias\n",
    " The Prophecy of Baruch\n",
    " The Prophecy of Ezechiel\n",
    " The Prophecy of Daniel\n",
    " The Prophecy of Osee\n",
    " The Prophecy of Joel\n",
    " The Prophecy of Amos\n",
    " The Prophecy of Abdias\n",
    " The Prophecy of Jonas\n",
    " The Prophecy of Micheas\n",
    " The Prophecy of Nahum\n",
    " The Prophecy of Habacuc\n",
    " The Prophecy of Sophonias\n",
    " The Prophecy of Aggeus\n",
    " The Prophecy of Zacharias\n",
    " The Prophecy of Malachias\n",
    " The First Book of Machabees\n",
    " The Second Book of Machabees\"\"\".split('\\n ')\n",
    "\n",
    "DRB_new_test_books = \"\"\"The Holy Gospel of Jesus Christ According to St. Matthew\n",
    " The Holy Gospel of Jesus Christ According to St. Mark\n",
    " The Holy Gospel of Jesus Christ According to St. Luke\n",
    " The Holy Gospel of Jesus Christ  According to St. John\n",
    " The Acts of the Apostles\n",
    " The Epistle of St. Paul the Apostle to the Romans\n",
    " The First Epistle of St. Paul to the Corinthians\n",
    " The Second Epistle of St. Paul to the Corinthians\n",
    " The Epistle of St. Paul to the Galatians\n",
    " The Epistle of St. Paul to the Ephesians\n",
    " The Epistle of St. Paul to the Philippians\n",
    " The Epistle of St. Paul to the Colossians\n",
    " The First Epistle of St. Paul to the Thessalonians\n",
    " The Second Epistle of St. Paul to the Thessalonians\n",
    " The First Epistle of St. Paul to Timothy\n",
    " The Second Epistle of St. Paul to Timothy\n",
    " The Epistle of St. Paul to Titus\n",
    " The Epistle of St. Paul to Philemon\n",
    " The Epistle of St. Paul to the Hebrews\n",
    " The Catholic Epistle of St. James the Apostle\n",
    " The First Epistle of St. Peter the Apostle\n",
    " The Second Epistle of St. Peter the Apostle\n",
    " The First Epistle of St. John the Apostle\n",
    " The Second Epistle of St. John the Apostle\n",
    " The Third Epistle of St. John the Apostle\n",
    " The Catholic Epistle of St. Jude the Apostle\n",
    " The Apocalypse of St. John the Apostle\"\"\".split('\\n ')\n",
    "print(DRB_old_test_books)\n",
    "print(DRB_new_test_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"The Book of Leviticus\"\n",
    "print(txt.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# List of book titles (assuming they are already in uppercase)\n",
    "titles = DRB_old_test_books + DRB_new_test_books\n",
    "titles = [element.upper() for element in titles]\n",
    "\n",
    "# Open the input Bible text file\n",
    "with open('DRV.txt', \"r\") as f:\n",
    "    line_counter = 0\n",
    "    book_name = None\n",
    "    book_content = \"\"  # Temporary storage for the current book's content\n",
    "\n",
    "    # Iterate through each line in the file\n",
    "    for line in f:\n",
    "        line_counter += 1\n",
    "        \n",
    "        # Only process lines between 145 and 140345\n",
    "        if line_counter < 145:\n",
    "            continue  # Skip lines before 145\n",
    "        if line_counter > 140345:\n",
    "            break  # Stop processing after line 140345\n",
    "\n",
    "        line = line.strip()  # Remove leading and trailing whitespace\n",
    "        \n",
    "        # Check if the line contains a book title\n",
    "        for title in titles:\n",
    "            if title in line:  # If a book title is found\n",
    "                if book_name:  # Process the previous book if it exists\n",
    "                    # Save the content of the previous book into a text file\n",
    "                    with open(f'DRB_{book_name}.txt', 'w') as book_file:\n",
    "                        book_file.write(book_content)\n",
    "                \n",
    "                # Set the new book name and reset content for the new book\n",
    "                book_name = title\n",
    "                book_content = \"\"  # Reset content for the next book\n",
    "                break  # Stop checking for other titles once the current one is found\n",
    "\n",
    "        # Append the current line to the book's content (if book_name is set)\n",
    "        if book_name:\n",
    "            book_content += line + \"\\n\"  # Add newline between lines of the book\n",
    "\n",
    "    # Save the last book's content after processing all lines\n",
    "    if book_name and book_content:\n",
    "        with open(f'DRB_{book_name}.txt', 'w') as book_file:\n",
    "            book_file.write(book_content)\n",
    "\n",
    "print(\"Books have been saved to separate text files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file grouped by Book saved to: aligned_bible_final.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths for the filtered TSV files\n",
    "file_paths = {\n",
    "    \"DRB\": \"DRB_filtered_with_index.tsv\",\n",
    "    \"KJV\": \"KJV_filtered_with_index.tsv\",\n",
    "    \"OEB\": \"OEB_filtered_with_index.tsv\",\n",
    "    \"WEB\": \"WEB_filtered_with_index.tsv\"\n",
    "}\n",
    "\n",
    "# List to hold DataFrames for each version\n",
    "dataframes = []\n",
    "\n",
    "# Add a \"Version\" column to each TSV file and load it\n",
    "for version, path in file_paths.items():\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=\"\\t\")\n",
    "        df[\"Version\"] = version  # Add version column\n",
    "        dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {version}: {e}\")\n",
    "\n",
    "# Combine all DataFrames into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Ensure Chapter and Verse columns are integers for proper sorting\n",
    "combined_df[\"Chapter\"] = combined_df[\"Chapter\"].astype(int)\n",
    "combined_df[\"Verse\"] = combined_df[\"Verse\"].astype(int)\n",
    "\n",
    "# Sort by Book, then by Version, Chapter, and Verse\n",
    "combined_df = combined_df.sort_values(by=[\"Book\", \"Version\", \"Chapter\", \"Verse\"])\n",
    "\n",
    "# Reorder columns for readability\n",
    "column_order = [\"Index\", \"Version\", \"Book\", \"Chapter\", \"Verse\", \"Text\"]\n",
    "combined_df = combined_df[column_order]\n",
    "\n",
    "# Save to a new TSV file\n",
    "output_file = \"aligned_bible_final.tsv\"\n",
    "combined_df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"Combined file grouped by Book saved to: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sorted alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted combined data saved to: sorted_aligned_bible_final.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the combined data\n",
    "combined_file = \"aligned_bible_final.tsv\"\n",
    "df = pd.read_csv(combined_file, sep=\"\\t\")\n",
    "\n",
    "# Ensure Chapter and Verse are treated as integers\n",
    "df[\"Chapter\"] = df[\"Chapter\"].astype(int, errors=\"ignore\")\n",
    "df[\"Verse\"] = df[\"Verse\"].astype(int, errors=\"ignore\")\n",
    "\n",
    "# Sort the DataFrame\n",
    "df = df.sort_values(\n",
    "    by=[\"Index\", \"Version\", \"Book\", \"Chapter\", \"Verse\"],  # Sorting keys\n",
    ")\n",
    "\n",
    "# Save to a new file\n",
    "output_file = \"sorted_aligned_bible_final.tsv\"\n",
    "df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"Sorted combined data saved to: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "input_file = \"DRB_preprocessed_columns.tsv\"\n",
    "output_file = \"DRB_normalized_titles.tsv\"\n",
    "\n",
    "# List of canonical book titles\n",
    "canonical_titles = DRB_old_test_books + DRB_new_test_books\n",
    "\n",
    "# Function to normalize a title\n",
    "def normalize_title(title):\n",
    "    # Convert title to lowercase and strip whitespace\n",
    "    title = title.strip().lower()\n",
    "    # Match with canonical titles (case insensitive)\n",
    "    for canonical in canonical_titles:\n",
    "        if re.fullmatch(canonical.strip().lower(), title):\n",
    "            return canonical  # Return the canonical title if it matches\n",
    "    return title  # Return original title if no match is found\n",
    "\n",
    "# Read the TSV file\n",
    "df = pd.read_csv(input_file, sep=\"\\t\")\n",
    "\n",
    "# Normalize the \"Book\" column\n",
    "df[\"Book\"] = df[\"Book\"].apply(normalize_title)\n",
    "\n",
    "# Save the updated DataFrame to a new TSV file\n",
    "df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"Book titles in '{input_file}' have been normalized and saved to '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fix alignment file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "aligned_file = \"def_titles_aligned.csv\"  # Path to aligned titles file\n",
    "file_paths = {\n",
    "    \"OEB\": \"OEB_combined.tsv\",\n",
    "    \"DRB\": \"DRB_normalized_titles.tsv\",\n",
    "    \"KJV\": \"KJV_final.tsv\",\n",
    "    \"WEB\": \"WEB_combined.tsv\"\n",
    "}\n",
    "\n",
    "# Function to normalize titles\n",
    "def normalize_title(title):\n",
    "    if not isinstance(title, str):\n",
    "        return title\n",
    "    title = re.sub(r'\\b1\\b', 'first', title, flags=re.IGNORECASE)\n",
    "    title = re.sub(r'\\b2\\b', 'second', title, flags=re.IGNORECASE)\n",
    "    title = re.sub(r'\\b3\\b', 'third', title, flags=re.IGNORECASE)\n",
    "    title = re.sub(r'\\bthe\\b', '', title, flags=re.IGNORECASE)\n",
    "    title = re.sub(r'\\bepistle of\\b', 'epistle', title, flags=re.IGNORECASE)\n",
    "    return title.lower().strip()\n",
    "\n",
    "# Step 1: Load the aligned titles file\n",
    "aligned_df = pd.read_csv(aligned_file)\n",
    "\n",
    "# Add an index to the aligned titles for sorting purposes\n",
    "aligned_df[\"Index\"] = aligned_df.index + 1  # Start index from 1 for readability\n",
    "\n",
    "# Step 2: Create a mapping of normalized titles to actual titles for each version\n",
    "title_mappings = {}\n",
    "unmatched_titles = {version: [] for version in file_paths.keys()}  # Log unmatched titles\n",
    "\n",
    "for version, path in file_paths.items():\n",
    "    try:\n",
    "        # Load the TSV file\n",
    "        df = pd.read_csv(path, sep=\"\\t\")\n",
    "        # Normalize the titles and create a mapping\n",
    "        df[\"Normalized Book\"] = df[\"Book\"].apply(normalize_title)\n",
    "        title_mappings[version] = dict(zip(df[\"Normalized Book\"], df[\"Book\"]))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {version}: {e}\")\n",
    "        title_mappings[version] = {}\n",
    "\n",
    "# Step 3: Filter each version based on the aligned titles\n",
    "for version, path in file_paths.items():\n",
    "    try:\n",
    "        # Load the respective TSV file\n",
    "        version_df = pd.read_csv(path, sep=\"\\t\")\n",
    "        \n",
    "        # Prepare the filtered DataFrame\n",
    "        filtered_rows = []\n",
    "\n",
    "        # Iterate over each row in the alignment file\n",
    "        for _, row in aligned_df.iterrows():\n",
    "            aligned_title = row[version]  # Get the aligned title for this version\n",
    "            \n",
    "            # Find the corresponding actual title from the mapping\n",
    "            actual_title = title_mappings[version].get(normalize_title(aligned_title), None)\n",
    "            if actual_title:\n",
    "                # Filter rows matching the actual title\n",
    "                matched_rows = version_df[version_df[\"Book\"] == actual_title].copy()\n",
    "                # Add Index and Version columns\n",
    "                matched_rows[\"Index\"] = row[\"Index\"]\n",
    "                matched_rows[\"Version\"] = version  \n",
    "                filtered_rows.append(matched_rows)\n",
    "\n",
    "        # Combine all filtered rows\n",
    "        if filtered_rows:\n",
    "            filtered_version_df = pd.concat(filtered_rows, ignore_index=True)\n",
    "        else:\n",
    "            filtered_version_df = pd.DataFrame(columns=version_df.columns.tolist())\n",
    "\n",
    "        # Save the filtered DataFrame\n",
    "        output_file = f\"{version}_filtered_with_index.tsv\"\n",
    "        filtered_version_df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {version}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final sorted and reordered file saved to: sorted_aligned_bible_final.tsv\n"
     ]
    }
   ],
   "source": [
    "# Combine all filtered versions\n",
    "filtered_files = [\"OEB_filtered_with_index.tsv\", \"DRB_filtered_with_index.tsv\", \"KJV_filtered_with_index.tsv\", \"WEB_filtered_with_index.tsv\"]\n",
    "combined_df = pd.concat([pd.read_csv(file, sep=\"\\t\") for file in filtered_files], ignore_index=True)\n",
    "\n",
    "# Ensure Chapter and Verse columns are integers for sorting\n",
    "combined_df[\"Chapter\"] = combined_df[\"Chapter\"].astype(int, errors=\"ignore\")\n",
    "combined_df[\"Verse\"] = combined_df[\"Verse\"].astype(int, errors=\"ignore\")\n",
    "\n",
    "# Sort by Index, Version, Chapter, and Verse\n",
    "combined_df = combined_df.sort_values(by=[\"Index\", \"Version\", \"Chapter\", \"Verse\"])\n",
    "\n",
    "# Reorder columns\n",
    "reorder_columns = [\"Index\", \"Version\", \"Book\", \"Chapter\", \"Verse\", \"Text\"]\n",
    "combined_df = combined_df[reorder_columns]  # Reorder columns\n",
    "\n",
    "# Save the final sorted and reordered file\n",
    "output_file = \"sorted_aligned_bible_final.tsv\"\n",
    "combined_df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"Final sorted and reordered file saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing or Empty Version Entries:\n",
      "Empty DataFrame\n",
      "Columns: [Index, Version, Book, Chapter, Verse, Text]\n",
      "Index: []\n",
      "Unique Versions in File:\n",
      "['DRB' 'KJV' 'OEB' 'WEB']\n",
      "Missing KJV Books:\n",
      "Empty DataFrame\n",
      "Columns: [Index, Version, Book, Chapter, Verse, Text]\n",
      "Index: []\n",
      "Total Rows: 25041\n",
      "Rows with Missing Version: 0\n",
      "Total KJV Rows: 5895\n",
      "KJV Rows with Missing Books: 0\n"
     ]
    }
   ],
   "source": [
    "# inspect missing KJV books\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the final sorted file\n",
    "df = pd.read_csv(\"sorted_aligned_bible_final.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Check for missing or empty entries in the Version column\n",
    "print(\"Missing or Empty Version Entries:\")\n",
    "print(df[df[\"Version\"].isnull() | (df[\"Version\"] == \"\")].head())\n",
    "\n",
    "# Check unique versions present in the file\n",
    "print(\"Unique Versions in File:\")\n",
    "print(df[\"Version\"].unique())\n",
    "\n",
    "# Filter for KJV-related rows and check for missing books\n",
    "kjv_data = df[df[\"Version\"] == \"KJV\"]\n",
    "missing_kjv_books = kjv_data[kjv_data[\"Book\"].isnull() | (kjv_data[\"Book\"] == \"\")]\n",
    "print(\"Missing KJV Books:\")\n",
    "print(missing_kjv_books.head())\n",
    "\n",
    "# Summarize insights\n",
    "print(f\"Total Rows: {len(df)}\")\n",
    "print(f\"Rows with Missing Version: {len(df[df['Version'].isnull() | (df['Version'] == '')])}\")\n",
    "print(f\"Total KJV Rows: {len(kjv_data)}\")\n",
    "print(f\"KJV Rows with Missing Books: {len(missing_kjv_books)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OEB_filtered_with_index.tsv': 25, 'DRB_filtered_with_index.tsv': 24, 'KJV_filtered_with_index.tsv': 24, 'WEB_filtered_with_index.tsv': 24}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of filtered file paths\n",
    "filtered_files = [\n",
    "    \"OEB_filtered_with_index.tsv\",\n",
    "    \"DRB_filtered_with_index.tsv\",\n",
    "    \"KJV_filtered_with_index.tsv\",\n",
    "    \"WEB_filtered_with_index.tsv\"\n",
    "]\n",
    "\n",
    "# Function to count unique book titles in a file\n",
    "def count_unique_titles(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=\"\\t\")  # Load the filtered file\n",
    "        unique_titles = df[\"Book\"].nunique()  # Count unique titles\n",
    "        return unique_titles\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Count unique titles in each file\n",
    "unique_title_counts = {file: count_unique_titles(file) for file in filtered_files}\n",
    "\n",
    "# Display the results\n",
    "print(unique_title_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique books extracted: 70\n"
     ]
    }
   ],
   "source": [
    "# Load the extracted file\n",
    "df = pd.read_csv(\"sorted_aligned_bible_final.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Count unique books\n",
    "unique_books = df[\"Book\"].nunique()\n",
    "print(f\"Unique books extracted: {unique_books}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
